# Introduction {#sec-introduction}

Artificial intelligence (AI) has become a headline topic in today's news, capturing the attention of individuals across various fields eager to leverage its capabilities to enhance their work. Historically, computers were introduced to take on repetitive and mundane tasks, leading to significant advancements in processing speed, communication infrastructures, and storage capacities. However, with the advent of sophisticated AI algorithms, we are now faced with a new level of challenges and solutions.

In the past, problem-solving involved identifying the issue and explicitly programming the computer to transform input data into desired outputs. Software specialists had to meticulously define every step of the problem-solving process to achieve the correct outcome. This traditional method required a deep understanding of the problem and the logic needed to solve it.

The emergence of AI, however, has revolutionized this approach. Instead of explicitly programming each step, we now provide the system with vast amounts of data, allowing it to "learn" and adjust itself to achieve the desired results. This learning process enables AI to tackle problems that were previously unsolvable using classical methods. By training models with large datasets, AI systems develop the capability to make predictions, recognize patterns, and generate insights without human intervention in the learning process.

This shift from explicit programming to AI opens up possibilities for addressing complex problems across diverse domains. The challenge now lies in selecting the appropriate type of learning—supervised, unsupervised, or reinforcement learning—based on the nature of the problem and the available data. Understanding the strengths and limitations of each learning type is essential for effectively harnessing AI to solve real-world problems. From art and entertainment to industry, manufacturing, and healthcare, AI's transformative impact is evident. These developments highlight the need for individuals and organizations to embrace AI, as leveraging its potential can drive innovation, efficiency, and improvements across various domains. As AI continues to evolve, integrating it into diverse fields becomes increasingly crucial. Therefore, understanding and utilizing AI is essential to remain competitive and progressive.

## Problem Statement

This thesis stems from a research project that explored using AI solutions to improve structural analysis, specifically by optimizing design parameters in Structural simulations. The goal is to enhance the efficiency and accuracy of engineering analyses. The thesis will further detail how AI - or more specifically machine learning - can be integrated into these workflows, aiming to identify effective models, reduce computational costs, and improve decision-making in engineering design. Individuals involved in this process are highly skilled experts, and their involvement represents a significant investment for the industry. Given the complexity and precision required in these tasks, even small improvements in efficiency can lead to substantial cost savings, increased profitability, and significant time savings. Streamlining their workflow, whether through automation or enhanced tools, has the potential not only to reduce expenses but also to accelerate project timelines, allowing companies to bring products to market faster and gain a competitive edge.

### ML\@Karoprod Project

The BMBF project titled "Machine Learning for the Prediction of Process Parameters and Component Quality in Automotive Body Production (ML\@Karoprod)" focuses on optimizing process parameters in a product chain for deforming a metal plate through a series of operations. In this project, conducted in collaboration with TU Chemnitz, IWU Fraunhofer Dresden, and SCALE GmbH, the aim was to leverage the machine learning methods to ensure high-quality products. Traditionally, an expert conducts process simulations, adjusts parameters, and evaluates the results to achieve the desired shape. This iterative process, which takes about 20 minutes per cycle, is repeated until the desired accuracy is reached, with the expert manually making the necessary adjustments and running further simulations.

![Process in action - IWU Fraunhofer Dresden](img/Chp1/karo_process.png){#fig-karo-proc fig-align="center" width="50%"}

The process begins with deep drawing, a sheet metal forming technique where a metal blank is radially drawn into a forming die by a punch, with the goal of producing a wrinkle-free and crack-free product. Our simulated deep drawing data consists of a number of valid experiments, each characterized by various process parameters. In collaboration with our partners, the relevant set of features that are necessary and sufficient to fully determine the effects of a simulation were identified. The features were divided into numerical and categorical types. That is, numerical features represented continuous values (the sheet thickness (from 0.99 to 1.48 mm) the blank-holder force (from 10 to 500 kN) and the insertion position (-5 to +5 mm)), while categorical features represented distinct classes or groups (the drawing depth (30, 50, or 70 mm) the drawing gap (1.6 or 2.4 mm)). To represent the different materials and stamps, IDs were assigned to uniquely identify each type of feature.

![from blank metal sheets to the product](img/Chp1/pikaso_enhance__none_2K_Standard_r_c_.jpeg){#fig-karo-hq fig-align="center" width="50%"}

![For each set of parameters in a row, one mesh were provided.](img/Chp1/table_params.png){#table-params fig-align="center" width="50%"}

Approximately 1000 simulation files in mesh format were generated by the project partner by varying process parameters. After data cleaning, about 880 of these files were usable. The meshes had three different geometries (with three different depths: 30, 50, and 70 shown in @fig-zt), and parameters such as force and material varied, resulting in different final mesh outcomes. The ultimate goal in this section was to train a neural network to predict the dependency between changes in input parameters and the generated mesh. With sufficient training, the model would be able to accurately predict the shape of the final mesh even with new inputs that were not seen during training. This allows the specialist to quickly and accurately observe the final simulation results by manipulating parameters, in a much shorter time compared to traditional finite element methods.

![a view of three different meshes with different drawing depths : 30,50,70.](img/Chp1/z70-50-30.png){#fig-zt fig-align="center" width="50%"}


|                                                                                                                                 |                                                                                             |
|------------------------------------|------------------------------------|
| ![The reference mesh for extracting face centers (Drawing depth Zt=70).](img/Chp1/banana_1.png){#fig-z70 fig-align="center" width="50%"} | ![The same mesh in a closer view](img/Chp1/banana_zoom.png){#fig-z70-zoom fig-align="center" width="50%"} |

## Finite Element Method

The Finite Element Method (FEM) [@liu2022eighty] is a robust computational technique widely used in engineering and physical sciences to solve complex problems related to structures, fluids, and thermal dynamics. While the exact origin of the finite element method is hard to define, its creation was driven by the necessity to solve complex problems in elasticity and structural analysis. The method’s development traces back to early contributions by Alexander Hrennikoff [@hrennikoff1941solution] and Richard Courant [@courant1994variational] in the 1940s. The essence of FEM lies in its ability to break down a large, complicated problem into smaller, more manageable parts known as finite elements. By systematically solving these elements, FEM provides an approximate solution that represents the behavior of the entire system.

### Mathematical Foundations

FEM is essentially a numerical approximation method that provides a solution for the continuous field $u$ of any partial differential equation (PDE) defined on a specific domain $\Omega$. The governing PDE can be expressed generally as:

$$
\mathcal{L}(u) = 0 \quad \text{on } \Omega
$$ {#eq-fem-1}

with boundary conditions:

$$
u = u_d \quad \text{on } \Gamma_D
$${#eq-fem-2}

$$
\frac{\partial u}{\partial x} = g \quad \text{on } \Gamma_N
$$ {#eq-fem-3}

In these equations, $L(u)$ represents a differential operator applied to the field $u$. This operator encapsulates the specific physical laws governing the problem, such as heat conduction, elasticity, fluid flow, etc. The nature of $L(u)$ depends on the type of PDE being solved:

For thermal problems: $L$ could represent the Laplace operator $\nabla^2 u$ , describing how heat diffuses through a material.

For structural problems: $L(u)$ might include terms representing the balance of forces, such as in the elasticity equations where $L(u)$ could involve the divergence of stress tensors.

For fluid dynamics: $L(u)$ could include the Navier-Stokes equations, which describe the motion of fluid substances.

In general, $L(u) = 0$ defines a PDE that the field $u$ must satisfy over the domain $\Omega$.

### Discretization and System of Equations

To solve this PDE using FEM, the domain $\Omega$ is discretized into $m$ finite elements with $n$ nodes. This discretization, along with the boundary conditions, leads to a system of linear or non-linear equations. For a linear operator $L$, this system can be represented as:

$$
\begin{pmatrix}
k_{1,1} & k_{1,2} & \cdots & k_{1,n} \\
k_{2,1} & k_{2,2} & \cdots & k_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
k_{n,1} & k_{n,2} & \cdots & k_{n,n}
\end{pmatrix}
\begin{pmatrix}
u_1 \\
u_2 \\
\vdots \\
u_n
\end{pmatrix} =
\begin{pmatrix}
F_1 \\
F_2 \\
\vdots \\
F_n
\end{pmatrix}
$${#eq-Kmatrix}

In this equation, ${K}(u)$ is the non-linear left hand side matrix, also called the stiffness matrix. ${u}$ represents the unknowns (i.e., the solution at each node), and ${F}$ is the load vector or the right-hand side of the equation.

### Solving the System: Iterative Techniques

The solution $u^h$ is obtained by solving this system of equations iteratively. A common approach is to use the Newton–Raphson method, which involves linearizing the residual of the system:

$$
r(u^h) = K(u^h)u^h - F
$${#eq-fem-it}

The iterations continue until the norm of the residual $r(u^h)$ meets the desired tolerance level. For linear problems, the system converges in one iteration, but for non-linear systems, multiple iterations are generally required. The most computationally intensive step in FEM is often the solution of this linear system, particularly when dealing with a large number of elements and nodes. Given that the mesh and its structure play a critical role in the success or failure of FEM, we will address this topic in detail in the next section.

### Mesh structure and Its Impact on FEM Analysis

A mesh is a specialized type of graph, consisting of vertices (points in 3D space) and edges (connections between these vertices), which collectively form the faces that define the surface of a 3D object. For any given geometry, it is possible to generate an infinite number of meshes, making them highly versatile for representing 3D shapes. Meshes are particularly effective and popular due to their ability to accurately portray complex geometries. By increasing the mesh resolution, the quality and precision of the 3D representation are enhanced, allowing for more detailed and realistic visualizations. Dense meshes, in particular, excel at accurately representing intricate details such as corners and curves, making them essential for rendering complex 3D objects. However, higher resolution meshes come with significant trade-offs, as they require more computational power and advanced graphics cards to manage the increased data load.

![three different meshes to represent one 3D object](img/Chp1/mesh-geom.png){#fig-mesh-geom width="40%" fig-align="center"}

Another aspect of the mesh is its topology. Mesh topology refers to the arrangement and connectivity of elements, such as triangles, quadrilaterals, or tetrahedra, within a mesh. It defines how these elements are connected through their vertices and edges, focusing on the structure and relationships between elements rather than their specific geometric positions. Topology is concerned with the mesh's internal structure without considering the actual spatial coordinates of the points.

On the other hand, geometry deals with the actual spatial placement and shape of the elements in the mesh. Geometry provides the coordinates and dimensions of each element, defining the shape and size of the mesh within the physical domain. While topology describes the connectivity, geometry specifies where the vertices are positioned and how the mesh represents the physical space.

The relationship between topology and geometry is crucial in mesh generation. A mesh with the same topology can have different geometries based on the positioning of the vertices. Good topology is essential for accurately representing the geometry of the model, especially in complex regions. Poor topology, such as elements with bad aspect ratios or improper connectivity, can lead to inaccuracies in representing the geometry, negatively affecting the quality and convergence of the finite element analysis. The comparison between topology and mesh geometry is shown in Figure @fig-topo-geom.

![mesh topology vs. geometry](img/Chp1/mesh-geo-topo.png){#fig-topo-geom width="40%" fig-align="center"}

A mesh in the context of FEM is defined as a network of interconnected elements that subdivides a complex geometry into smaller, simpler parts, known as elements or cells. These elements can take various shapes, such as triangles or quadrilaterals in 2D, and tetrahedra or hexahedra in 3D. In this context, the vertices of the mesh are referred to as nodes, and the faces are known as elements. The connections between nodes, which form the edges, are typically called edges or connections in FEM. The mesh serves as the framework over which the mathematical equations governing the physical behavior of the system are solved. The quality and resolution of the mesh are crucial, as they directly influence the accuracy, convergence, and computational efficiency of FEM analyses. A finer mesh improves accuracy but requires more computational resources, while a coarser mesh reduces computational demand but lowers precision. Understanding the structure and characteristics of the mesh is essential for optimizing FEM simulations and achieving reliable results in complex engineering problems. In Figure @fig-fem-mesh-quality, you can see an example of a suitable mesh for FEM and an unsuitable one.

![a bad mesh (left) vs a good mesh (right) for FEM](img/Chp1/fem-mesh-opt.png){#fig-fem-mesh-quality width="40%" fig-align="center"}

In general, suitable meshes for FEM should exhibit several essential characteristics: they must be sufficiently refined to accurately capture the geometry and stress gradients of the structure, with a higher mesh density in regions of high stress or complex geometry. The mesh elements should maintain an appropriate aspect ratio, avoiding excessively elongated or distorted shapes, to ensure numerical accuracy and stability. Additionally, the mesh should be well-aligned with the boundaries and features of the model to minimize interpolation errors and enhance the precision of the analysis. It is always important to remember that the significance of the mesh is so crucial that even with a poorly constructed mesh, we can obtain results that are vastly different and full of errors.

FEM is widely used in various industries to simulate physical phenomena and optimize designs, significantly reducing production costs by allowing for virtual testing and refinement before physical prototypes are created. The method gained prominence in the 1960s and has since undergone significant advancements, driven by improvements in computer technology and numerical algorithms.Today, FEM is integral to fields such as aerospace, automotive, civil engineering, and biomedical engineering, offering highly accurate predictions and contributing to innovation and efficiency in product development. In this thesis however, our focus on FEM is specifically related to structural analysis and deformation problems. Since this thesis aims to explore the potential of using AI methods for solving FEM simulations, a brief introduction to neural networks and their functions is necessary.

## Introduction to Deep Neural Networks

Deep Neural Networks (DeepNN) [@lecun2015deep] represent a significant evolution in the field of artificial intelligence, particularly in machine learning and pattern recognition. Building on the foundation of traditional neural networks, DeepNNs consist of multiple layers of interconnected neurons, allowing them to model complex patterns and relationships in data more effectively than shallow networks. The advent of DeepNNs has revolutionized numerous fields, including computer vision, natural language processing, and reinforcement learning, marking a pivotal shift in the capabilities of AI systems. The diagram in @fig-ai-diag presents how Artificial Intelligence, Machine Learning, Neural Networks, and Deep Learning are related.

![A hierarchical diagram showing the relationship between Artificial Intelligence, Machine Learning, Neural Networks, and Deep Learning.](img/Chp1/AI_diagram.png){#fig-ai-diag fig-align="center" width="30%"}

### Perceptron

The history of the perceptron begins in 1943 with McCulloch and Pitts [@mcculloch1943logical], who developed the first theoretical model of a neuron. In 1958, Frank Rosenblatt [@Rosenblatt_1957_6098] expanded on their work and introduced the perceptron as a learning algorithm for binary classification. His work laid the foundation for neural networks and machine learning as we know them today.

It simulates the way a single neuron in the human brain works by taking multiple input signals, assigning each a weight, summing them up, and then passing the result through an activation function to produce an output. This simple model can classify data into two categories by finding a linear decision boundary, making it foundational for understanding more complex neural network structures. Despite its limitations, such as being unable to solve problems that aren't linearly separable, the perceptron laid the groundwork for modern neural networks and deep learning.

![Perceptron](img/Chp1/perceptron.png){#fig-perceptron fig-align="center" width="50%"}

### Forward Pass

In a neural network, the input data is fed forward through the network to generate an output. This involves calculating the output of each neuron in each layer by applying a weighted sum of inputs followed by an activation function.

For a given neuron, the output can be represented as: 
$$
z_j = \sum_{i} w_{ji}*x_i + b_j
$${#eq-prec1}

where:

$z_j$ is the weighted input to neuron $j$, 
$w_{ji}$ is the weight connecting input $i$ to neuron $j$, 
$x_i$ is the input from the previous layer, $b_j$ is the bias term.

The output $a_j$ of neuron $j$ is then calculated using an activation function $\phi$ :\
$$
a_j = \phi(z_j)
$${#eq-prec2}

### Backpropagation

The backpropagation algorithm, which has its roots in the 1960s, became widely known after Rumelhart, Hinton, and Williams published their breakthrough paper in 1986 [@rumelhart1986learning]. Before this, training deep neural networks was difficult due to inefficient methods for adjusting weights in multiple layers. Backpropagation solved this problem by calculating the error at the output layer and propagating it backward through the network, adjusting weights layer by layer to minimize the error. Once the forward pass generates an output, the backward pass applies backpropagation to determine how much each weight contributed to the error, significantly improving the training process and leading to the rise of deep learning in later years. The partial derivative of the output with respect to the weight $w_{ji}$ is computed using the chain rule of calculus. For a neuron in the output layer, the error gradient with respect to a weight is given by: 
$$
\frac{\partial E}{\partial w_{ji}} = \delta_j \cdot a_i
$${#eq-bp-1} 
where:

$E$ is the error associated with the output, 
$\delta_j$ is the error term for neuron $j$, 
$a_i$ is the output of the neuron in the previous layer.

The error term $\delta_j$ for each neuron in the output layer is: 
$$
\delta_j = \phi'(z_j) \cdot (a_j - y_j)
$${#eq-bp-2}

where $y_j$ is the target output and $\phi'(z_j)$ is the derivative of the activation function at neuron $j$'s input.

For neurons in the hidden layers, $\delta_j$ is computed as: 
$$
\delta_j = \phi'(z_j) \sum_k \delta_k w_{kj}
$${#eq-bp-3}

where the sum is over all neurons $k$ in the subsequent layer that receive input from neuron $j$.

### Optimizers

Optimizers are algorithms used to update the weights in a neural network to minimize the loss function and improve performance. Common optimizers include Gradient Descent, Adam and RMSProp. Optimizers determine how quickly or slowly a model learns and converge toward an optimal solution. There are various types of optimizers, each with its strengths and suited to different types of tasks and data distributions.

#### Gradient Descent

Gradient descent is the optimization technique used to adjust the network weights. In its simplest form, the weights are updated as follows: 
$$
w_{ji}^{new} = w_{ji}^{old} - \eta \frac{\partial E}{\partial w_{ji}}
$${#eq-gd}

where:

$\eta$ is the learning rate, a small positive number that controls the step size of the update,

$\frac{\partial E}{\partial w_{ji}}$ is the gradient of the error with respect to the weight.

This process is repeated iteratively across multiple epochs, gradually adjusting the weights to reduce the overall error of the network.

The error mentioned here is formally known as the loss function $L(\theta)$, which quantifies the difference between the predicted outputs and the actual target values. Here, $\theta$ represents the network's parameters (weights and biases).

$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)
$${#eq-gd-loss}

where $\eta$ is the learning rate, and $\nabla_\theta L(\theta_t)$ is the gradient of the loss function with respect to the parameters. The gradient $\nabla_\theta L(\theta_t)$ indicates the direction and rate of the steepest increase in the loss. By moving in the opposite direction, gradient descent reduces the loss, thereby improving the performance of the neural network. However, the success of gradient descent in deep networks relies heavily on factors such as effective weight initialization, the choice of activation functions, and strategies to mitigate challenges like overfitting and the vanishing gradient problem. The introduction of ReLU activation functions, expressed as $Relu(x)=\max(0,x)$, was instrumental in overcoming the vanishing gradient problem by ensuring that gradients remain significant in deeper layers, thus allowing for better learning and performance.

While gradient descent is the foundation of most optimization processes in deep learning, modern optimizers like Adam, RMSProp, and Adagrad improve on basic gradient descent by dynamically adjusting the learning rates and using past information (e.g., momentum or adaptive learning rates). These improvements help accelerate convergence and make training more efficient, especially in cases where the loss landscape is complex and difficult to navigate.

#### Stochastic Gradient Descent

Stochastic Gradient Descent (SGD) is a classic optimization algorithm that updates the model's parameters based on a small random subset (or "batch") of the training data, instead of using the entire dataset like standard gradient descent. This makes the process faster and less computationally expensive. However, because SGD uses a random subset, its updates tend to be noisier, which can cause fluctuations in the loss function. Adding momentum to SGD can help smooth out these noisy updates, leading to more stable convergence. SGD updates the parameters by moving in the direction of the negative gradient of the loss function with respect to the parameters. The update rule for SGD is: 
$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)
$${#eq-sgd} 

Where:

$\theta_t$ is the parameter at time step $t$,

$\eta$ is the learning rate,

$\nabla_\theta L(\theta_t)$ is the gradient of the loss function with respect to $\theta_t$.

#### Root Mean Square Propagation

Root Mean Square Propagation (RMSProp) is an adaptive learning rate method that scales the learning rate of each parameter by dividing the gradient by a moving average of its recent squared values. This helps RMSProp to handle non-stationary objectives (where the data distribution changes over time) and improves convergence. It is particularly effective when dealing with mini-batch gradient descent, as it adjusts the learning rate based on how large or small the gradients are, preventing large parameter updates that can hinder training.The update rule is:

1.  Compute the gradients: 
    $$
    g_t = \nabla_\theta L(\theta_t)
    $${#eq-rms-1}

2.  Update the moving average of squared gradients: 
    $$
    v_t = \beta v_{t-1} + (1 - \beta) g_t^2
    $${#eq-rms-2}

3.  Update the parameters: 
    $$
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} g_t
    $${{#eq-rms-3}}

Where:

$\eta$ is the learning rate, 
$\beta$ is the decay rate for the moving average (commonly $\beta = 0.9$), 
$\epsilon$ is a small constant to prevent division by zero (usually $10^{-8}$), 
and $v_t$ is the moving average of the squared gradients.

#### Momentum

Momentum is a technique used in optimization algorithms to accelerate convergence and reduce oscillations, especially in cases where gradients change direction frequently. It works by maintaining an exponentially decaying moving average of past gradients, which allows the optimizer to continue moving in the direction of consistent gradients, thus dampening oscillations and speeding up convergence. Momentum introduces an additional term to the update rule that accumulates the gradients from previous steps. This accumulated gradient is used to "push" the parameters more in the direction of the overall trend of the loss surface, making the updates smoother and more efficient.

The update rule for momentum is:

1.  Update the velocity (accumulated gradients): $$
    v_t = \beta v_{t-1} + (1 - \beta) \nabla_\theta L(\theta_t)
    $$

2.  Update the parameters using the velocity: $$
    \theta_{t+1} = \theta_t - \eta v_t
    $$

Where:

$\eta$ is the learning rate,

$\beta$ is the momentum coefficient (commonly set to 0.9),

$v_t$ is the velocity (the exponentially weighted moving average of the gradients),

$\nabla_\theta L(\theta_t)$ is the gradient of the loss function with respect to $\theta_t$.

#### Adaptive Moment Estimation

Adaptive Moment Estimation (Adam) is an adaptive learning rate optimization algorithm that combines the advantages of two popular methods: momentum and RMSProp. It computes individual learning rates for each parameter based on estimates of the first and second moments (mean and variance) of the gradients. This helps the optimizer adjust the learning rate dynamically based on the gradients' behavior. Adam generally works well across a wide range of tasks, making it a popular choice for deep learning applications. It also incorporates bias correction to ensure unbiased estimates in the early stages of training. Adam uses moving averages of the gradients and squared gradients for adaptive learning rates. The update rule is as follows:

1.  Compute the gradients: 
    $$
    g_t = \nabla_\theta L(\theta_t)
    $${#eq-adam-1}
2.  Update biased first moment estimate (mean of gradients): 
    $$
    m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
    $${#eq-adam-2}
3.  Update biased second moment estimate (variance of gradients): 
    $$
    v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
    $${#eq-adam-3}
4.  Bias correction for the first and second moments: 
    $$
    \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
    $${#eq-adam-4}
5.  Update the parameters: 
    $$
    \theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
    $${#eq-adam-5}

Where:

$\eta$ is the learning rate,

$\beta_1$ and $\beta_2$ are decay rates for the first and second moment estimates (default values $\beta_1 = 0.9$, $\beta_2 = 0.999$,

$\epsilon$ is a small constant to prevent division by zero (usually $10^{-8}$),

$m_t$ is the exponentially weighted average of the gradients (first moment estimate),

$v_t$ is the exponentially weighted average of the squared gradients (second moment estimate).

Usually, starting with Adam is a common approach, as it is generally effective across many tasks. Experimenting with different optimizers may be necessary if better performance is needed. Fine-tuning the optimizer's specific hyperparameters (such as $\beta_1$ and $\beta_2$ in Adam) can also lead to improved results.

### Activation Function

An activation function in neural networks is a mathematical function applied to the output of a neuron that introduces non-linearity into the model. These functions help the network learn complex patterns by allowing it to capture non-linear relationships between inputs and outputs. Activation functions determine whether a neuron should be "activated" or "fired" based on its input, controlling the flow of information in the network. Without activation functions, the model would behave like a simple linear regression and would not be able to solve more complex tasks. Below are some of the most important activation functions along with their formulas:

#### Linear Activation Function

The linear (or identity) activation function simply passes the input as it is without any transformation.

$$
f(x) = x
$${#eq-af-lin}

-   **Range**: (-$\infty$, $\infty$)
-   **Common use case**: Output layer in regression tasks for predicting continuous values.

#### Sigmoid Activation Function

The sigmoid function maps the input to a range between 0 and 1. It’s widely used in binary classification problems.

$$
f(x) = \frac{1}{1 + e^{-x}}
$${#eq-af-sig}

-   **Range**: (0, 1)
-   **Common use case**: Output layer in binary classification.

#### Hyperbolic Tangent (Tanh) Activation Function

The tanh function maps the input to a range between -1 and 1, offering a zero-centered output.

$$
\tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}
$${#eq-af-tanh}

-   **Range**: (-1, 1)
-   **Common use case**: Hidden layers in many neural networks.

#### ReLU (Rectified Linear Unit) Activation Function

ReLU is one of the most commonly used activation functions in modern deep learning networks. It outputs the input directly if positive, otherwise, it outputs zero.

$$
f(x) = \max(0, x)
$${#eq-af-relu}

-   **Range**: \[0, $\infty$)
-   **Common use case**: Hidden layers in convolutional and fully connected networks.

#### Leaky ReLU Activation Function

Leaky ReLU is a variation of ReLU that allows a small, non-zero gradient when the input is negative, helping mitigate the "dying ReLU" problem.

$$
f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}
$${#eq-af-lrelu}

Where $\alpha$ is a small constant (typically 0.01).

-   **Range**: (-$\infty$, $\infty$)
-   **Common use case**: Hidden layers to avoid dead neurons.

#### Sine Activation Function

The sine activation function is less commonly used but can be beneficial in certain special cases or scientific models.

$$
f(x) = \sin(x)
$${#eq-af-sin}

-   **Range**: (-1, 1)
-   **Common use case**: Experimental or specific tasks requiring periodic behavior.

These activation functions play critical roles in determining the performance of neural networks, with each function having its strengths and weaknesses depending on the task at hand.

### Loss Function

Loss functions serve as a measure of how well or poorly a model's predictions align with the actual target values. By quantifying the error or "loss" between the predicted outputs and the true labels, the loss function guides the adjustment of the model's weights during training. Essentially, the model seeks to minimize the loss function, and by doing so, it iteratively updates the weights to improve accuracy and performance.

In many cases, standard loss functions like Mean Squared Error (MSE) for regression tasks or Cross-Entropy Loss for classification tasks are sufficient. However, there are situations where a custom loss function is necessary to address specific challenges or nuances of the problem at hand. For instance, in imbalanced datasets, where certain classes are underrepresented, a custom loss function might be designed to assign higher penalties to misclassifications of the minority class, thereby ensuring the model learns more effectively. Crafting a custom loss function often requires deep domain knowledge and experience, as it needs to balance the specific objectives of the task with the overall training process. This expertise helps in fine-tuning the loss function to achieve optimal performance for the given problem.

#### Mean Absolute Error

L1 loss, also known as Mean Absolute Error (MAE), is a loss function commonly used in NN models, especially for regression tasks. It measures the average of the absolute differences between predicted values and actual values. The L1 loss is robust to outliers since it penalizes errors linearly rather than quadratically like L2 loss. Minimizing L1 loss encourages sparsity in the model's parameters, making it useful in contexts such as feature selection and sparse models. The formula for L1 loss is given as:

$$
MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y_i}|
$${#eq-mae-1}

Where $y$ represents the actual values, $\hat{y}$ the predicted values, and $n$ is the total number of data points. The absolute value function ensures that the magnitude of the error is considered regardless of its direction.

#### Mean Squared Error

MSE loss or L2 loss, is one of the most commonly used loss functions in regression problems. It measures the average squared difference between the predicted values and the actual target values. MSE is particularly sensitive to large errors due to the squaring of the differences, meaning that models producing large errors are penalized more heavily. 
$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$${#eq-mse-1}

#### Combined Loss

Integrating different loss functions into deep neural networks allows for balancing various aspects of model performance. A single loss function might not capture all the essential characteristics for optimal learning. For example, one loss function $Loss_A$ might focus on robustness to outliers, while another loss function $Loss_B$ could emphasize minimizing large errors or encouraging smooth predictions. By combining these loss functions, we can leverage the strengths of both. The combined loss can be written as:

$$
Combined Loss = \alpha \cdot {Loss_A} + \beta \cdot {Loss_B}
$${#eq-loss-combi}

Here, $\alpha$ and $\beta$ are non-negative hyperparameters that determine how much each loss contributes to the final combined loss. This approach allows fine-tuning to achieve a desirable trade-off between different properties, such as robustness and precision, depending on the specific task.

### Challenges in Neural Networks

Neural networks, while powerful, face several key challenges during training and application. One of the most common issues is underfitting, which occurs when the model is too simple to capture the underlying patterns in the data. This often happens when the neural network has too few neurons or layers, or if the model is not trained for long enough. Underfitting results in poor performance on both the training and test sets, as the network fails to learn the complexities of the task.

On the other hand, overfitting is the opposite problem. Overfitting happens when a neural network becomes too tailored to the training data, capturing noise and irrelevant details, which hinders its ability to generalize to new, unseen data. As a result, the model performs well on the training set but struggles with generalization, leading to poor performance on real-world tasks.Techniques such as regularization (L1/L2), dropout, and data augmentation are commonly used to prevent overfitting.

Another major challenge is convergence during training. Neural networks are trained using gradient-based optimization techniques like gradient descent, and the learning process can sometimes get stuck in local minima or saddle points. Choosing appropriate learning rates and initialization methods is crucial for ensuring smooth convergence. If the learning rate is too high, the model may oscillate or fail to converge, while a too-low learning rate can slow down training.

A particularly difficult issue, especially in deep networks, is the vanishing gradient problem. This occurs when gradients in the backpropagation process become extremely small, especially in the earlier layers of deep networks. When this happens, the weights in these layers receive little to no update, stalling learning. This problem is more prevalent with certain activation functions like sigmoid and tanh. To mitigate this, modern architectures often use activation functions like ReLU and its variants, which help maintain gradient flow. Techniques like batch normalization and proper weight initialization also help combat the vanishing gradient problem.

### Network Hyperparameters

In the field of neural networks, hyperparameters are settings that define the architecture and training process of the model. Unlike the model's parameters (such as weights and biases), hyperparameters are set before the learning process begins and are not learned from the data. Proper tuning of these hyperparameters is crucial for improving the performance of a neural network. Below are some common hyperparameters and the general approach to tuning them.

#### Learning Rate

The learning rate controls how much to adjust the model's weights with respect to the loss gradient during each update. A small learning rate may lead to slow convergence, whereas a large learning rate could cause the model to overshoot optimal weights, preventing convergence or leading to suboptimal results. Learning rate schedules or adaptive learning rate methods like Adam or RMSProp can help dynamically adjust this parameter during training.

For the tuning, a common approach is to begin with a standard value, such as 0.001 for Adam or 0.01 for stochastic gradient descent (SGD). Methods like grid search or random search are typically used to explore different values. Additionally, learning rate schedules (e.g., exponential decay, step decay) are often applied to improve training outcomes.

#### Batch Size

Batch size determines how many samples from the training data are used to calculate the gradient before updating the model's parameters. Smaller batch sizes often provide noisier updates but require less memory, making them suitable for limited hardware resources. Larger batch sizes lead to smoother gradient estimates but require more memory.

A common approach is to use batch sizes ranging from 16 to 512, with experimentation often conducted using powers of 2 (e.g., 16, 32, 64, etc.). Smaller batch sizes tend to aid generalization, while larger batches can accelerate the training process.

#### Number of Epochs

The number of epochs defines how many times the entire dataset is passed through the neural network during training. Too few epochs can result in underfitting, where the model fails to learn meaningful patterns from the data. Too many epochs can lead to overfitting, where the model memorizes the training data but fails to generalize to unseen data.

A common approach involves applying early stopping, which halts the training process when the performance on a validation set begins to degrade, thereby preventing overfitting. This method starts with a relatively large number of epochs and continuously monitors the model's performance on the validation set to determine the optimal point to stop training.

#### Dropout Rate

Dropout is a regularization technique that randomly drops neurons during training to prevent overfitting. The dropout rate controls the percentage of neurons to drop during each forward pass. High dropout rates can prevent the network from learning effectively, while low dropout rates might not provide enough regularization.

Dropout rates typically range from 0.2 to 0.5. Starting with a dropout rate of 0.3 and adjusting based on the model’s performance on the validation set is a common practice.

#### L2 Regularization

L2 regularization, also known as weight decay, is a technique used to prevent overfitting by discouraging large weights in a neural network. It works by adding a penalty term to the loss function, which is proportional to the sum of the squared weights of the network. This penalty term forces the optimization process to prefer smaller weights, making the model less complex and more generalizable to unseen data.

The L2 regularization term is typically scaled by a factor called the regularization parameter ($\lambda$), which controls the strength of the regularization. A larger value of $\lambda$ increases the penalty, encouraging smaller weights but potentially underfitting the model. Conversely, a smaller value of $\lambda$ reduces the penalty, allowing the network to learn more complex patterns but with the risk of overfitting.

L2 regularization is commonly used alongside techniques like dropout and is often incorporated in hyperparameter tuning to find the right balance between model complexity and generalization.

#### Number of Layers and Neurons

The architecture of a neural network is another important aspect of hyperparameter tuning. The number of layers (depth) and units per layer (width) determine the capacity of the model to learn from data. More layers and units increase the network’s capacity but also make it more prone to overfitting and computationally expensive to train.

It's common to start with a simple architecture and then gradually increase the depth and width of the network. Techniques like grid search or random search are typically used to explore different combinations, and regularization methods such as dropout or L2 regularization are often applied to prevent overfitting.

#### Weight Initialization

Proper weight initialization helps the network converge faster and avoid problems like vanishing or exploding gradients. Popular initialization methods include Xavier (Glorot) initialization for sigmoid or tanh activations, and He initialization for ReLU activations.

Usually, the choice of initialization depends on the activation function used. For example, He initialization is effective for layers with ReLU activations, while Xavier initialization works well with sigmoid or tanh. Fine-tuning may not be necessary unless convergence issues are encountered.

### Hyperparameter Tuning

Hyperparameter tuning is an essential step in developing neural networks that perform well on specific tasks. While no universal set of hyperparameters guarantees success, systematic experimentation and techniques like random search, grid search, or Bayesian optimization can greatly assist in finding the optimal configuration for a given task.

1.  **Grid Search**: It involves manually specifying a range of values for each hyperparameter and training a model for each combination. While thorough, it can be computationally expensive.

2.  **Random Search**: Instead of trying every combination, random search selects random combinations of hyperparameters. It often performs comparably to grid search but is computationally more efficient.

3.  **Bayesian Optimization**: This method builds a probabilistic model of the objective function and uses it to select hyperparameters in a way that balances exploration and exploitation. It is more sophisticated than grid or random search and can lead to better performance with fewer trials.

4.  **Automated Hyperparameter Tuning (e.g., Hyperopt, Optuna)**: Automated libraries can simplify the tuning process by intelligently searching for the optimal hyperparameters. These libraries often use strategies like Bayesian optimization, tree-structured Parzen estimators, or evolutionary algorithms to find optimal settings with minimal manual intervention.

## Neural Network Architectures

Neural networks are remarkably flexible, allowing various architectures and units to be connected and combined to create more complex models tailored to specific problems. For instance, by increasing or decreasing the number of layers and the number of neurons in each layer, the network's performance can be improved. Another crucial factor is the choice of activation functions, which play a significant role in the learning process. Additionally, specialized layers such as convolutional layers and recurrent layers can be added or removed depending on the data type, the complexity of the problem, and the specific requirements. These adjustments are made to achieve the most suitable architecture for the task at hand.

For instance, a Convolutional Neural Network (CNN) can be enhanced with conditional layers, forming a Conditional Convolutional Neural Network (CCNN) that generates different outputs based on additional context. This modularity adds significant power to the neural network, but it also introduces a considerable level of complexity. The goal is to design models that are as simple as possible while still providing adequate performance to solve the targeted problems effectively. A number of widely adopted neural network architectures will be covered in the following sections.

### Multi-Layer Perceptron

Multi-Layer Perceptron (MLP) is a type of feedforward artificial neural network that consists of multiple layers of neurons, where each layer is fully connected to the next one. The basic unit of an MLP is the perceptron, which only computes a weighted sum of its input features and passes the result through an activation function. By adding multiple layers to a perceptron, an MLP becomes capable of solving more complex problems. MLPs typically consist of an input layer, one or more hidden layers, and an output layer. The input layer receives the initial data, the hidden layers process the data through multiple transformations, and the output layer produces the final prediction or classification. MLPs are capable of approximating complex functions and are commonly used in tasks such as classification, regression, and pattern recognition. For an MLP with a single hidden layer:

$$
z^{(1)} = W^{(1)} {x} + b^{(1)}
$${#eq-mlp-1}

$$
a^{(1)} = \phi(z^{(1)})
$${#eq-mlp-2}

$$
z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}
$${#eq-mlp-3}

$$
\hat{y} = \phi(z^{(2)})
$${#eq-mlp-4}

Where:

$x$ is the input vector.

$W^{(1)}$ and $W^{(2)}$ are the weight matrices for the first and second layers, respectively.

$b^{(1)}$ and $b^{(2)}$ are the bias vectors for the first and second layers, respectively.

$\phi$ is the activation function (e.g., sigmoid, ReLU, etc.).

and $\hat{y}$ is the final output (prediction) of the MLP.

![An MLP with 2 hidden layers](img/Chp1/mlp.png){#fig-mlp fig-align="center" width="50%"}

### Autoencoders

An autoencoder is a type of neural network designed to learn a compressed, efficient representation of input data. It consists of two main components: an encoder and a decoder. The encoder compresses the input data into a lower-dimensional representation, often referred to as the latent space or bottleneck. The decoder then attempts to reconstruct the original input from this compressed representation. The goal is to minimize the difference between the input and the reconstructed output.

![an Autoencoder Architecture](img/Chp1/ae.png){#fig-ae fig-align="center" width="50%"}

Autoencoders, used for tasks like dimensionality reduction or feature learning, can also be improved by incorporating elements such as convolutional layers, recurrent units, or Generative Adversarial Networks (GANs). This integration allows autoencoders to learn more complex data representations, enhancing their effectiveness in applications like anomaly detection or data compression.

Mathematically, the process can be represented as: 
$$
z = f_\theta(x)
$${#eq-ae-1}

$$
\hat{x} = g_\phi({z})
$${#eq-ae-2}

Here, $z$ is the latent representation of the input $x$, and $\hat{x}$ is the reconstructed output. The objective of the autoencoder is to minimize the reconstruction loss, commonly measured as the mean squared error (MSE).

Autoencoders are particularly useful in areas such as dimensionality reduction and reconstruction, where they compress high-dimensional data into a lower-dimensional latent space and then accurately reconstruct the original data. They also excel in denoising by removing noise from corrupted data, and in anomaly detection by identifying outliers based on reconstruction errors. Additionally, autoencoders are valuable in generative modeling, contributing to the creation of new, similar data samples, which is especially beneficial in image synthesis and data augmentation.

### Variational Autoencoders

A Variational Autoencoder (VAE) is a generative model that extends the concept of autoencoders by incorporating probabilistic elements. Unlike traditional autoencoders, which learn deterministic mappings from input data to a latent space, VAEs aim to model the latent space as a probability distribution. This enables VAEs to not only reconstruct input data but also generate new samples from the learned distribution.

In a VAE, the encoder maps the input data to a distribution over the latent space, typically a Gaussian distribution with mean $\mu$ and variance $\sigma^2$. Instead of directly using the encoder's output, a latent vector $z$ is sampled from this distribution. The decoder then reconstructs the input data from the sampled latent vector. This sampling introduces variability and allows VAEs to generate new data by sampling from the latent space.

The VAE training objective consists of two components: the reconstruction loss (similar to a traditional autoencoder) and a regularization term that enforces the learned latent space to follow a prior distribution (commonly a standard normal distribution). This regularization is achieved through the Kullback-Leibler (KL) divergence, which measures how much the learned distribution deviates from the prior.

The process is mathematically represented as:

$$
z \sim q_\phi(z|x) = \mathcal{N}(z|\mu(x), \sigma(x)^2)
$${#eq-vae-1}

$$
\hat{x} = g_\theta(z)
$${#eq-vae-2}

Here, $q_\phi(z|x)$ is the approximate posterior distribution generated by the encoder, and $\hat{x}$ is the reconstructed output. The objective of the VAE is to minimize the total loss, which consists of the reconstruction loss and the KL divergence:

$$
\mathcal{L}_{\text{VAE}} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{\text{KL}}(q_\phi(z|x) \| p(z))
$${#eq-vae-3}

- The first term is the reconstruction loss, encouraging accurate reconstruction of input data.
- The second term is the KL divergence, ensuring that the learned latent space distribution stays close to the prior distribution $p(z)$.

Variational Autoencoders are widely used in generative tasks such as image synthesis, data augmentation, and anomaly detection. By sampling from the learned latent space, VAEs can generate new data points that resemble the original dataset, making them a powerful tool for creating realistic data in various domains.

### Convolutional Neural Networks

Convolutional Neural Networks (CNNs) are a class of deep neural networks specifically designed to process data with a grid-like structure, such as images. Unlike fully connected networks, where each neuron is connected to every neuron in the previous layer, CNNs take advantage of the spatial structure of the data by using convolutional layers. These layers apply convolutional filters to local regions of the input, which can be mathematically represented as:

$$
y = f * x + b
$${#eq-cnn-1}

where $y$ is the output feature map, $f$ is the filter, $*$ denotes the convolution operation, $x$ is the input, and $b$ is the bias term. This operation allows the network to detect patterns such as edges, textures, and shapes in the input data.

![Convolution operation](img/Chp1/conv.png){#fig-conv fig-align="center" width="30%"}

A typical CNN architecture consists of multiple layers, including convolutional layers, pooling layers, and fully connected layers. Convolutional layers apply filters to the input to extract features, while pooling layers perform down-sampling, reducing the spatial dimensions of the data. A common pooling operation is max pooling, defined as:

$$
y_{i,j} = \max_{m,n} (x_{i+m,j+n})
$${#eq-max}

where $y_{i,j}$ represents the output after pooling, and $x_{i+m,j+n}$ is the region of the input over which the pooling operation is applied. The final fully connected layers combine the features extracted by the convolutional and pooling layers to make predictions, typically using a softmax function for classification:

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$${#eq-softmax}

This equation represents the softmax function, where:

$z_i$ is the input to the softmax function for the $i$-th class.
$K$ is the total number of classes.

The output of the softmax function is the probability distribution over $K$ classes, with each value $z_i$ transformed into a probability between 0 and 1.This ensures that the sum of all output probabilities equals 1, making it useful for multi-class classification.

![CNN Architecture for image classification](img/Chp1/CNN.png){#fig-cnn-layers fig-align="center" width="50%"}

CNNs are widely used in various applications, including image classification, object detection, and image segmentation. Their ability to automatically learn and extract features from raw pixel data has led to significant advancements in computer vision. The modularity of CNNs allows them to be easily adapted to different tasks by adjusting the architecture, such as changing the number of layers or the size of filters. This flexibility, combined with their high accuracy and efficiency, makes CNNs a cornerstone of modern deep learning applications.

### Conditional Neural Networks

Conditional Neural Networks are a specialized type of neural network where the output is conditioned not only on the input data but also on an additional context or condition. This conditioning can be represented as an additional input that influences the network's behavior, enabling it to generate different outputs depending on the given condition.

Consider a standard neural network where the input data is $x$ and the output is $y$. The network typically learns a function $f_\theta(x) = y$, where $\theta$ represents the parameters of the network. In a Conditional Neural Network, an additional condition $c$ is introduced, modifying the function to:

$$
f_{\theta}({x}, {c}) = {y}
$${#eq-cond}

Here, $c$ is the condition that influences the network's output. This condition could be a class label, a set of parameters, or any other contextual information relevant to the task.

If a neural network is designed to perform a specific task on a dataset, and we want to generalize that task to different categories of data, we can introduce a condition or context to the network. This allows the network to adapt its behavior based on the category or condition, making it more versatile. Moreover, this conditioning is not tied to a specific architecture; for example, a condition can be added to a convolutional network or any other architecture. For instance, in a conditional image generation task, $x$ might represent a latent vector, and $c$ could be a label corresponding to the class of the image to be generated. The network would then generate an image $y$ conditioned on both $x$ and $c$:

$$
{y} = G_{\theta}({x}, {c})
$${#eq-gen}

Where $G_\theta$ is the generator function of the Conditional Neural Network.

The loss function in a Conditional Neural Network often takes the condition into account as well. For example, in a supervised learning scenario, the loss function $L$ could be defined as:

$$
L_\theta = \frac{1}{N} \sum_{i=1}^{N} \ell\left(f_{\theta}({x}_i, {c}_i), {y}_i\right)
$${#eq-cond-gen}

Where $\ell$ is a suitable loss function (e.g., cross-entropy or mean squared error), $N$ is the number of training examples, and $y_i$ is the true output corresponding to input $x_i$ under condition $c_i$.

![Conditional Variational Autoencoder](img/Chp1/cvae.png){#fig-cvae fig-align="center" width="50%"}

Conditional Neural Networks are widely used in various applications, such as conditional image generation, style transfer, and structured prediction tasks. Their ability to incorporate context or conditions makes them highly versatile and effective for problems where the output must be tailored based on specific criteria. This flexibility allows conditions to be added to various architectures, whether it's a convolutional network or any other type, broadening their applicability.

## Research Questions
So far, the research topic has been discussed, and preliminary discussions have been made regarding familiarization with FEM and the existing challenges. Additionally, we provided a brief overview of neural networks and their components, as well as commonly used algorithms and key considerations. This research has been conducted to answer the following questions: 

-   combine NN and 3D data for modeling the object deformation
-   investigating the best match for modeling the 3D data and neural networks to develop techniques to balance neural network size, accuracy, and computational efficiency
-   Investigating how implicit representations can replace traditional mesh processing methods in 3D models
-   examining the potential benefits of continuous model representations
-   Identifying methods to reduce simulation times for complex 3D models using implicit representations
-   Deriving and optimizing an accurate set of process parameters from neural networks for handling the dynamics of 3D shapes, particularly for real-time applications.
  
In the next chapter, the work done in the area of the Finite Element Method using AI techniques will be reviewed. The next chapter will address data, covering various forms of 3D representation and datasets, as well as the steps taken in generating them. Additionally, the chapter will detail the preparation of the data for training the neural network. In the following chapter, single-step deformation will be examined, focusing on both shell meshes and solid meshes. Multi-step deformation will then be explored in the subsequent chapter, incorporating reinforcement learning to enhance the process using mesh data, image data and 3D implicit fields. Finally, the research will conclude with a comprehensive analysis of the results and insights derived from the study.