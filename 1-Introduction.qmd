# Introduction {#sec-introduction}

Artificial intelligence (AI) has become a headline topic in today's news, capturing the attention of individuals across various fields eager to leverage its capabilities to enhance their work. Historically, computers were introduced to take on repetitive and mundane tasks, leading to significant advancements in processing speed, communication infrastructures, and storage capacities. However, with the advent of sophisticated AI algorithms, we are now faced with a new level of challenges and solutions.

In the past, problem-solving involved identifying the issue and explicitly programming the computer to transform input data into desired outputs. Software specialists had to meticulously define every step of the problem-solving process to achieve the correct outcome. This traditional method required a deep understanding of the problem and the logic needed to solve it.

The emergence of AI, however, has revolutionized this approach. Instead of explicitly programming each step, we now provide the system with vast amounts of data, allowing it to "learn" and adjust itself to achieve the desired results. This learning process enables AI to tackle problems that were previously unsolvable using classical methods. By training models with large datasets, AI systems develop the capability to make predictions, recognize patterns, and generate insights without human intervention in the learning process.

This shift from explicit programming to AI opens up possibilities for addressing complex problems across diverse domains. The challenge now lies in selecting the appropriate type of learning—supervised, unsupervised, or reinforcement learning—based on the nature of the problem and the available data. Understanding the strengths and limitations of each learning type is essential for effectively harnessing AI to solve real-world problems. From art and entertainment to industry, manufacturing, and healthcare, AI's transformative impact is evident. These developments highlight the need for individuals and organizations to embrace AI, as leveraging its potential can drive innovation, efficiency, and improvements across various domains. As AI continues to evolve, integrating it into diverse fields becomes increasingly crucial. Therefore, understanding and utilizing AI is essential to remain competitive and progressive.

## Problem Statement

This thesis stems from a research project that explored using AI solutions to improve structural analysis, specifically by optimizing design parameters in Structural simulations. The goal is to enhance the efficiency and accuracy of engineering analyses. The thesis will further detail how AI - or more specifically machine learning - can be integrated into these workflows, aiming to identify effective models, reduce computational costs, and improve decision-making in engineering design. Individuals involved in this process are highly skilled experts, and their involvement represents a significant investment for the industry. Given the complexity and precision required in these tasks, even small improvements in efficiency can lead to substantial cost savings, increased profitability, and significant time savings. Streamlining their workflow, whether through automation or enhanced tools, has the potential not only to reduce expenses but also to accelerate project timelines, allowing companies to bring products to market faster and gain a competitive edge.

### ML\@Karoprod Project

The BMBF project titled "Machine Learning for the Prediction of Process Parameters and Component Quality in Automotive Body Production (ML\@Karoprod)" focuses on optimizing process parameters in a product chain for deforming a metal plate through a series of operations. In this project, conducted in collaboration with TU Chemnitz, IWU Fraunhofer Dresden, and SCALE GmbH, the aim was to leverage the machine learning methods to ensure high-quality products. Traditionally, an expert conducts process simulations, adjusts parameters, and evaluates the results to achieve the desired shape. This iterative process, which takes about 20 minutes per cycle, is repeated until the desired accuracy is reached, with the expert manually making the necessary adjustments and running further simulations.

![Process in action - IWU Fraunhofer Dresden](img/Chp1/karo_process.png){#fig-karo-proc fig-align="center" width="50%"}

The process begins with deep drawing, a sheet metal forming technique where a metal blank is radially drawn into a forming die by a punch, with the goal of producing a wrinkle-free and crack-free product. Our simulated deep drawing data consists of a number of valid experiments, each characterized by various process parameters. In collaboration with our partners, the relevant set of features that are necessary and sufficient to fully determine the effects of a simulation were identified. The features were divided into numerical and categorical types. That is, numerical features represented continuous values (the sheet thickness (from 0.99 to 1.48 mm) the blank-holder force (from 10 to 500 kN) and the insertion position (-5 to +5 mm)), while categorical features represented distinct classes or groups (the drawing depth (30, 50, or 70 mm) the drawing gap (1.6 or 2.4 mm)). To represent the different materials and stamps, IDs were assigned to uniquely identify each type of feature.

![from blank metal sheets to the product](img/Chp1/pikaso_enhance__none_2K_Standard_r_c_.jpeg){#fig-karo-hq fig-align="center" width="40%"}

Approximately 1000 simulation files in mesh format were generated by the project partner by varying process parameters. After data cleaning, about 880 of these files were usable. The meshes had three different geometries (with three different depths: 30, 50, and 70 shown in @fig-zt), and parameters such as force and material varied, resulting in different final mesh outcomes. The ultimate goal in this section was to train a neural network to predict the dependency between changes in input parameters and the generated mesh. With sufficient training, the model would be able to accurately predict the shape of the final mesh even with new inputs that were not seen during training. This allows the specialist to quickly and accurately observe the final simulation results by manipulating parameters, in a much shorter time compared to traditional finite element methods.

![a view of three different meshes with different drawing depths : 30,50,70.](img/Chp1/z70-50-30.png){#fig-zt fig-align="center" width="50%"}

### Metal Deformation Process

Deformation refers to the change in shape, size, or position of a material or structure when subjected to external forces, such as tension, compression, shear, bending, or torsion. This alteration occurs due to the displacement of particles within the material, resulting in either elastic deformation, where the material returns to its original shape after the removal of the force, or plastic deformation, where the change is permanent. Single-step deformation (SSD) in our context refers to the process of applying a force to a structure or material in a single instance, resulting in a one-time deformation.

In practice, what engineers do is resort to simulations to reduce costs and increase accuracy and speed, using tools like Finite Element Methods (FEM) for this purpose. An expert needs to design the geometry in the software, perform meshing, and then define the fixed and force constraints. After that, they must select the material and choose an appropriate solver, depending on the complexity of the geometry and the problem, as well as the computational power available, and then wait for the simulation results. This process can even take several days, and in the end, the expert evaluates the parameters based on the simulation results. It is usually necessary for the expert to adjust some parameters (based on their experience and knowledge) and rerun the simulation. As it might seem, this can be a challenging process for experts. Moreover, in industry, this can be quite costly because, alongside powerful equipment, a significant amount of time and energy from an expert is required to address a single problem. Therefore, any improvement in this process can be very beneficial and profitable.

The focus of this thesis is on a specific type of deformation that occurs when pressure or force is applied to the surface of a metal piece, causing it to change shape. While this type of deformation is relatively simple, it has a wide range of applications.

## Challenges of 3D Deformation Modeling

When applying neural networks to deformation problems, several challenges emerge. One of the most significant issues was the lack of access to a standardized dataset for this purpose. Another key challenge involved how to present geometry to the neural network in a way that would preserve the accuracy of geometric computations. Additionally, the network needed to operate effectively with a relatively small dataset and an appropriate model size, which would lead to both faster learning and inference processes.

### Standard Dataset for Training and Benchmarking

Data is fundamental to the success of neural network modeling, as the quality and quantity of data directly influence model performance. Unlike standard datasets available for images, videos, and text, there is no publicly accessible deformation dataset for this specific case, which limits the ability to compare and test models. Since a large amount of data was not available at the beginning, and data generation for the project partner was time-consuming, it was preferred to first create a simpler dataset with basic geometry for training the network. The model would then be expanded for the new data.

### The Dataset Size and Data Units

In most existing works involving 3D data, the entire 3D object is fed into the network at once, meaning that the input unit is the mesh, and the network learns based on this input in each iteration. As a result, large datasets with meshes containing a high number of vertices significantly increase the size of the network, particularly in the input layer and subsequently in the following layers. This leads to a substantial increase in computational complexity and the number of parameters in the network. Consequently, it becomes necessary to increase both the number of training samples and the number of training epochs. However, generating a large volume of data for the network is often impractical.

In our specific case, we have around 1,000 meshes, each ranging between 13,000 and 16,000 vertices in size. If 3D meshes with around 13,000 to 16,000 vertices are fed directly into a neural network, the input layer size will be proportional to the number of vertices. For instance, using 3D coordinates (x, y, z), the input layer would have 3 neurons per vertex. Therefore, the input size for each mesh would be calculated as follows: For 13,000 vertices: 13,000×3=39,000 input neurons. In most neural networks, the input layer is followed by fully connected layers. If the first hidden layer contains 1,000 neurons, the number of parameters (weights) between the input layer and the first hidden layer would be: For 13,000 vertices: 39,000×1,000=39,000,000 parameters.

This number can increase rapidly as additional hidden layers or larger layers are added, and it does not yet account for biases or further layer connections. Even this simplified model would have tens of millions of parameters. To ensure generalization and prevent overfitting, a sufficient number of data samples is required. A common rule of thumb in machine learning suggests having at least 10 times more training examples than parameters, though this varies depending on the data and model complexity. For 39,000,000 parameters (based on the 13,000 vertex case), at least 390 million data points would be needed for effective training. Given the large number of parameters, having only 1,000 meshes with 13,000–16,000 vertices may be insufficient, posing a risk of overfitting. Ideally, 10,000–50,000 samples would be necessary, or techniques such as downsampling or using specialized architectures should be considered to reduce input size.

### Combining 3D Data and Neural Networks - 3D Data Formats

Another challenge would be the selection of an appropriate 3D data input format. RGBD, Voxel, Point Cloud, and Mesh are well-known and widely-used formats for 3D representation, each with its own characteristics, advantages, and limitations. RGBD provides depth information alongside color, Voxel offers a volumetric approach, Point Cloud represents objects through discrete points, and Mesh captures surface geometry through vertices and edges. These formats are explained in detail in chapter @sec-dnn-3d-reps.

![3D Stanford Bunny in 4 explicit representations : RGBD, Voxel, Point Cloud and Mesh](/img/Chp1/rabbit_3d.png){#fig-3d-rep fig-align="center"}

Neural networks are typically designed to process structured inputs of fixed size. For example, in image processing, the input is a regular grid of pixels with a consistent and specific order. However, 3D data does not usually follow this regular grid structure, making it difficult to directly feed into traditional architectures like convolutional neural networks (CNNs). 3D data can vary in resolution, topology, and structure, complicating the task of maintaining a fixed input size. Furthermore, the inherent complexity of 3D data presents additional challenges in preserving spatial relationships and geometric features, both of which are essential for accurately modeling deformation. Overcoming these obstacles is crucial for successfully leveraging neural networks in 3D deformation tasks. Here is why these discrete representations are not ideal for describing deformation with neural networks:

#### Mesh

Meshes are commonly used in representing 3D objects, especially in finite element analysis and computer graphics. However, for neural networks, especially traditional architectures like convolutional networks, meshes present a challenge due to their irregular structure. Meshes consist of vertices and edges that form triangular or polygonal faces, and the number of vertices can vary across different objects. This lack of uniformity in structure makes it difficult to apply neural networks that require fixed input sizes. Additionally, the connectivity between vertices (topology) adds complexity, making it harder for neural networks to learn deformation patterns consistently across different meshes. The irregularity and large size of meshes also lead to increased computational demands and parameter counts, which can make training inefficient.

#### Point Cloud

Point clouds represent 3D data as a collection of discrete points in space, which describe the surface of an object without explicitly encoding the connectivity between points. While point clouds provide a simple and direct way to capture 3D shapes, they lack inherent structure and order, making it challenging for neural networks to process them effectively. The unordered nature of point clouds means that traditional neural network architectures like CNNs, which rely on spatial relationships, cannot be directly applied. Additionally, point clouds may vary in density and resolution, which can further complicate the learning of deformation, as small deformations might not be accurately captured depending on the point sampling. Handling the lack of connectivity information also makes learning the geometry of deformations more difficult.

#### Voxel

Voxels represent 3D space as a grid of volumetric pixels, similar to 2D pixels but in three dimensions. Although this regular grid structure can be directly fed into convolutional neural networks, voxel representations suffer from a significant drawback: memory inefficiency. Voxel grids require vast amounts of memory to represent high-resolution 3D objects, especially when capturing fine details of deformation. The resolution must be very high to model small deformations accurately, which dramatically increases the computational cost. In addition, the sparse nature of 3D objects (with most voxels being empty) leads to inefficient usage of both memory and computational resources, making voxel-based representations impractical for many deformation tasks.

#### Depth Image

Depth images capture the distance between the camera and the surface of objects, representing 3D structures in a 2D format. While they offer the advantage of being easily processed by traditional CNNs due to their image-like structure, depth images inherently lose information about the object’s geometry, especially in areas not visible from the camera's perspective. This loss of data makes it difficult to fully capture the shape of an object, particularly in deformation scenarios where understanding the entire 3D structure is crucial. Additionally, since depth images are viewpoint-dependent, multiple images from different angles would be required to capture the full deformation, which introduces further complexity in the data preprocessing and network design.

Each of these data representations poses unique challenges when used to describe deformations with neural networks. Their limitations in terms of structure, memory efficiency, and ability to capture complex geometric relationships make them less than ideal for modeling 3D deformation tasks. These challenges necessitate the development of specialized architectures or preprocessing techniques to overcome their inherent shortcomings when applied to neural networks.

## Key Solution: Insights from Implicit Representations

So far, it has been shown that discrete data representations pose significant challenges when applied to deformation using neural networks. As the research progressed, a more suitable combination of neural networks with 3D data was identified, which, with certain adjustments, appeared well-suited for representing deformation. The initial inspiration came from the paper on DeepSDF [@park2019deepsdf]. In this work, neural networks were used as embedding functions for 3D objects, resulting in a relatively small network capable of embedding high-resolution meshes. Additionally, the approach is independent of topology, providing an accurate representation of geometry, which is crucial for deformation tasks. Despite introducing a novel representation method and training a neural network accordingly, this paper did not explicitly address deformation. It appeared, however, that the method could be adapted to deformation-related tasks after some modifications.

### Implicit Representation : Signed Distance Fields

Imagine a 3D object, such as a sphere, with a field around it where every point in space is at the same distance from the surface of the object. For instance, all points exactly 1 cm away from the surface form a layer at that constant distance. This concept can be extended to envision infinitely many such layers, both outside and inside the object, each representing different distances from the surface. The fundamental idea behind the Signed Distance Field (SDF) is that every point in space is assigned a value corresponding to its distance from the object's surface, with positive values outside, negative values inside, and zero on the surface itself. The concept provides a method to describe an object's shape, not by its precise boundaries, but by the distance of any point in space from the object's surface.

![Signed Distance Fields around an Object : the points with the same colors, have the same distance from the object surface](/img/Chp1/sdField.jpeg){#fig-field fig-align="center" width="20%"}

This concept is represented by what is known as a signed distance function, which evaluates any point in space and provides two pieces of information:

1.  How far the point is from the surface of the object.

2.  Whether the point is inside or outside the object (indicated by a negative or positive distance).

In DeepSDF paper, it was demonstrated that a neural network is capable of encoding a 3D object as a function. In this scenario, the trained neural network can be queried to determine the distance of any arbitrary point from the surface. By querying the network for a sufficient number of points, we can generate a point cloud, where the distances of these points from the surface are estimated by the neural network. This point cloud allows us to determine the surface boundary of the desired object without having to explicitly model the edges.

To generalize across a larger set of objects, a condition was introduced to the neural network. This condition, which is essentially a code, is defined for each object, enabling the SDF function to return the corresponding distance for each point based on this condition. \[more information in section X regarding this coding and autodecoder process\] This adjustment allowed DeepSDF to encode a wider range of objects. In our case, the focus is not on different classes of objects but rather on the deformations of a single object. Therefore, the condition-defining code can be the force vector that causes the deformation. With this setup, we will have a neural network that, conditioned on the force, can predict the deformations of a single object.

![DeepSDF uses the codes for each object to predict the correct distance accordingly](/img/Chp1/code_sdf.png){#fig-code-sdf fig-align="center" width="70%"}

This image showcases the codes for two distinct objects—a sofa and a chair. These codes, in their compressed form, capture the underlying structure of each object, as encoded by an autodecoder. By interpolating between these codes, values were blended, resulting in the reconstruction of intermediate shapes. These shapes exhibited a smooth transition from the sofa to the chair, demonstrating the capability of this method to represent different objects within a continuous space.

![query the points (inside, outside and on the surace) to the Network to get the distance](/img/Chp1/sdf_dist.png){#fig-sdf-dist fig-align="center" width="70%"}

## Direct Deformation

Direct or Single-step deformation refers to a process where a material or object undergoes deformation in one direct operation, without intermediate stages. This approach is commonly used in manufacturing processes like stamping, forging, or molding, where the material is shaped into its final form in a single action. Its main advantage lies in its efficiency, reducing production time and minimizing the need for multiple machine setups or adjustments. In production, single-step deformation is often applied in mass manufacturing of parts where speed and uniformity are critical, such as in automotive or aerospace industries.

### Modeling the Deformation : Solid Mesh

To model deformation and train the neural network, data samples were needed that depicted various states of a 3D object along with the force vectors applied to it. At that time, limited data was available from the project partner—around 40 samples, which did not seem sufficient for training the network. In the absence of a standard dataset for object deformation representation to train our network, a custom dataset was designed, named DefBeam @sec-dataset-defbeam descried in Chapter 3. In the dataset, the initial shape was kept constant across all data samples, and the intended deformation was achieved by applying a force in a single step. so there would be one code (force vector) available for each deformed shape to distinguish it from other variants. In other words, rather than applying the algorithm to different object categories, the approach was adapted to capture deformations of a single object. In this adapted method, the use of encoded object representations was replaced with force vectors. These vectors are mathematical representations of forces that can be applied to an object, and by adjusting their magnitude (the strength of the force) and direction, different deformations of the object can be achieved. This approach allows for a more intuitive way to control the deformation process, as the object undergoes a unique transformation based on how the force vector is modified.

At this stage, a neural network model was trained on DefBeam dataset, with the aim of estimating the corresponding "signed distance" for any given point in space. Ultimately, by querying a large number of points, a set of SDF values was obtained, which were then converted into a mesh using discretization methods such as Marching Cubes, providing an estimate of the final deformed shape. 

![deformation modeling of the watertight mesh from our DefBeam dataset](/img/Chp1/solid_sdf.png){#fig-solid-sdf fig-align="center" width="70%"}

### Modeling the Deformation : Shell Mesh

During the time the model was being trained on our custom synthetic dataset, the project partner generated additional meshes, approximately 1,000 samples. However, another issue arose: the generated meshes were open shell meshes rather than watertight ones (Figure @fig-banana). Open shell meshes refer to meshes that lack a well-defined volume, consisting only of surface geometry without internal structure. In contrast, DeepSDF is defined on watertight meshes, which have distinct inside and outside regions, with the surface serving as the boundary separating these two spaces. 

![The shell mesh object provided for the project](/img/Chp1/banana_1.png){#fig-banana fig-align="center" width="40%"}

For each deformed shell mesh, the process parameters such as force, thickness, and others were provided, along with the deformed mesh geometry, deviation from the target mesh, and the thickness and thinning of each element. As a result, a neural network was designed to create a mapping between the process parameters and these values.

![For each set of parameters in a row, one mesh were provided.](img/Chp1/table_params.png){#table-params fig-align="center" width="50%"}

Initially, by cross-sectioning the shell mesh, we transformed our problem into a one-dimensional problem to solve the desired values in a 1D space. To achieve this, a cut was made in the middle of the mesh, and points were sampled at equal intervals to create a mapping between the process parameters and the thickness corresponding to the sampled points using a neural network. The results after training appeared satisfactory, and the same approach was applied to thinning and deviation. Additionally, it was possible to estimate all three parameters in 1D space effectively within a single network with three outputs.

![Shell mesh - 1D cut](/img/Chp1/karo_1D_cut.png){#fig-karo-1d fig-align="center" width="80%"}

Next, we extended the problem to a two-dimensional space, using the UV-map image of the product's surface. The advantage of this method was the ability to leverage image-based neural network processing techniques, such as convolutional structures, to improve training. Here too, the neural network successfully estimated all three parameters—thickness, thinning, and deviation—based on the input image structure.

![Shell mesh - 2D image - deviation and thickness, ground truth vs. network prediction](/img/Chp1/karo_2D.png){#fig-karo-2d fig-align="center" width="80%"}

In the subsequent step, we moved towards generalizing these two models in 3D space. In 3D, it is evident that we can achieve the best understanding of the deformation of the part and provide the most comprehensive estimation to accurately capture the object's deformation. Since we had the target mesh and the desired values for each element, we were able to map the corresponding value for each point using a neural network. Both single-output and three-output models were employed, and both yielded satisfactory results. As a result, instead of predicting SDF values, the network predicts corresponding features for each element, such as deviation, thickness, and thinning. Interestingly, despite having only 1,000 mesh samples—of which only 880 were usable for training—the implicit model simulation, combined with using mesh elements as individual input units (unlike explicit methods, which require the entire geometry to be input at once), allowed for accurate and reliable prediction of the values.

![deviation prediction for two samples of the shell mesh](/img/Chp1/karo_3d.png){#fig-shell-sdf fig-align="center" width="60%"}

Meanwhile, the same process in FEM simulation with a mesh of this size typically takes around 20 minutes, and once the results are obtained, an expert tunes the parameters and re-execute the simulation to assess any improvements. In contrast, after training, our network performs inference in just 1-2 seconds, providing predictions almost instantly. This allows the user to interact with a real-time slider, observing the effects of parameter changes—even for unseen parameters—immediately. A detailed explanation of these methods and their outcomes will be presented in the results chapter.

## Sequential Deformation

Contrary to direct deformation, sequential multi-step deformation (MSD) takes place over multiple stages, where a structure or material progressively changes under successive loads or forces (@fig-seq-def). Each stage adds new stresses or strains, resulting in cumulative impacts on the material’s shape or integrity. In production, sequential deformation is typically seen in processes such as incremental forming, deep drawing, or multi-stage forging, where materials are gradually shaped to form complex geometries while ensuring proper strain distribution and reducing defects. To model this type of process, it is necessary to first gather data to train the network on a series of deformation steps. For this purpose, a dataset was designed in which an object is deformed over several stages under different forces. To better capture the deformation changes and cover more stages, a cuboid fixed at the bottom with force applied to its top surface and restricted to 1DOF (one degree of freedom) was used. This means the deformation is limited to a single direction, primarily vertical, as the cuboid responds to the applied force. In this setup, the deformation mainly occurs on the top surface. The modeling approach must be adapted to account for these multiple steps, allowing the network to predict deformations over time.

![Sequential Mesh Deformation in 5 Steps](/img/Chp3/MSD.png){#fig-seq-def fig-align="center" width="60%"}

The previous method, designed for cases where different forces act on the initial object to generate deformations, is unsuitable for modeling multi-stage deformations. In multi-stage deformation, the sequence and interaction of applied forces are interdependent. To address this challenge, a Deep Reinforcement Learning (DeepRL) approach was employed to model the sequential deformation process. DeepRL operates by framing the problem as a series of decisions, where the system learns to take optimal actions based on the current state of the object. In this case, the object's deformation is treated as a state, and the forces applied at each stage act as actions. The goal of DeepRL is to maximize the reward, which is defined by how closely the deformed shape matches the desired outcome at each step. Through repeated training, the network learns to predict and apply the correct sequence of forces, ensuring that the object deforms in a way that aligns with the target. This approach allows for the modeling of complex, multi-stage deformation processes with higher accuracy compared to traditional methods.

Due to the extensive computational demands of handling numerous state-action pairs, some approaches employ compressed state representations to reduce the computational load and improve efficiency. In the context of Deep Reinforcement Learning (DRL), a world model can be used to further enhance efficiency. A world model serves as an internal representation of the environment, allowing the agent to predict future states without interacting directly with the real world at each step. This model simulates the behavior of the environment, enabling the DRL agent to make predictions about how actions will affect the system’s state. By training on these simulated environments, the agent can learn faster and more efficiently, significantly reducing the computational resources needed to evaluate state-action pairs in real time. This is particularly useful in multi-stage deformation, where the complexity of interactions between forces and material response requires sophisticated modeling to capture the cumulative effects of sequential deformations.


To achieve this, several approaches for compressing the states were explored:

- Treating the deformed mesh surface as a 2D depth image (Image-Based MSD): 

- Select the vertex coordinates on top to represent the deformation (Vertex based MSD)

- Compressing the mesh using a mesh autoencoder (Mesh-Based MSD)

- Neural-encoded object representation to model multi-step deformations (Implicit MSD)

### Image-Based MSD
Since the cuboid is subjected to force only from the top, the deformation primarily affects the top surface. As a result, the entire shape can be effectively represented by focusing on the top surface alone. To achieve this, the vertices at the top of the mesh were selected, and interpolation techniques were applied to estimate the Z-values (height or deformation) across the surface. This method produced a smooth and continuous surface, accurately capturing the spatial trends of the deformation. Such interpolation is particularly valuable for filling in the gaps between measured points, ensuring a comprehensive representation of the data, whether in 2D or 3D. The specific steps for preparing the data and interpolating Z-values are detailed in Chapter X.

### Vertex based MSD
In the vertex-based approach, the 3D coordinates of the vertices on the top surface are used directly. The input to the neural network consists of the coordinates (x, y, z) of the top vertices, resulting in a smaller network compared to image-based methods. It is essential that the topology of the mesh remains fixed to ensure consistency in the network inputs, so the vertices were manually indexed and ordered. A simple multi-layer perceptron (MLP) was trained to take the xyz coordinates of the top vertices, along with the applied action, to predict the next positions of the vertices.

This approach provides a low-cost and straightforward solution, particularly when working with datasets where the topology is identical across all samples, which is uncommon in real-world scenarios. As with the image-based approach, all data (in this case, the vertex coordinates) are fed into the network at once, offering a global view of the current state. Due to the relatively small data size, the vertex positions can be directly incorporated into the DRL loop, allowing for efficient processing without significant computational demands.

### Mesh-Based MSD 
A mesh autoencoder, such as CoMA, is employed to encode a high-dimensional mesh into a lower-dimensional latent space. This process reduces the complexity of the mesh while preserving its geometric integrity, allowing the deep reinforcement learning (DRL) model to work with a more compact and efficient representation of the object's surface during deformation. By compressing the data, the autoencoder enables faster and more efficient processing without sacrificing essential geometric details. 

As with other mesh-based methods, this approach requires that the topology of the mesh remains fixed. Ensuring that the vertex order and connectivity are consistent across all samples is essential for maintaining the accuracy and coherence of the encoded representations. This fixed topology is a critical factor in effectively applying mesh autoencoders like CoMA to deformation modeling.

### Implicit MSD
In this approach to modeling multi-step deformation, a neural network architecture is used with two types of modules: Single Step Deformation (SSD) and Multi-Step Deformation (MSD). The process starts by taking a query point in the form of (x, y, z) coordinates. The SSD module applies an initial action (force or transformation) and computes the Signed Distance Function (SDF) to represent the deformation at that step. The SDF output is then passed through multiple MSD modules, each representing an additional deformation step with further actions applied, progressively refining the SDF and capturing the continuous deformation process. 

In this approach, the MSD (Multi-Step Deformation) module shares the same structure as the SSD (Single Step Deformation) module, with one key difference: it has an additional input, $SDF_{t-1}$, which represents the previous deformation state. This input allows the new state to be directly linked to the previous one, ensuring that each deformation step builds on the one before it. The $SDF_{t-1}$ acts as a bridge between consecutive deformation steps, allowing the model to capture the cumulative effects of multiple actions. This connection between states ensures that the model can handle complex, sequential deformations more effectively by incorporating past deformations into the prediction of future ones.

To ensure consistency across the deformation steps, a fixed grid is employed throughout the dataset. The query points are selected from this grid, which remains constant for the entire model. Importantly, the grid itself is a random sample, preventing any overfitting or loss of generalization in the method. This approach allows the network to maintain consistency across sequential deformations while still being flexible enough to generalize to new, unseen deformations.

## Research Questions

In this chapter, the problem statement was introduced, the existing challenges were discussed, and the proposed solution, including the use of implicit representation for deformation and its modeling with neural networks, was presented. Furthermore, methods for addressing both single-step and sequential deformation were briefly outlined. In essence, this research was carried out to explore the following questions:


-   Investigating the best match for modeling the 3D data and neural networks to develop techniques to balance neural network size, accuracy, and computational efficiency
-   Examining the potential benefits of continuous model representations
-   Investigating how implicit representations can replace traditional mesh processing methods in 3D models
-   Combine NN and 3D data for modeling the object deformation for watertight and non-watertight meshes
-   Identifying methods to reduce simulation times for complex 3D models using implicit representations
-   Deriving and optimizing an accurate set of process parameters from neural networks for handling the dynamics of 3D shapes, particularly for real-time applications

In the following, preliminary discussions will be made regarding familiarization with Finite Element Methods as the powerfull approach in physical simulations and the existing challenges, as well as the work done in the area of the FEM using AI techniques. Additionally, a brief overview of neural networks and their components will be provided, along with commonly used algorithms and key considerations. The third chapter will address datasets, as well as the steps taken in generating them and detail the preparation of the data for training the neural network. In the following chapter, single-step deformation will be examined, focusing on both shell meshes and solid meshes. Multi-step deformation will then be explored in the subsequent chapter, incorporating reinforcement learning to enhance the process using image data, mesh data and 3D implicit fields. Finally, the research will conclude with a comprehensive analysis of the results and insights derived from the study.

## list of publications
to be added