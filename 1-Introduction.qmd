# Introduction {#sec-introduction}

Artificial intelligence (AI) has become a headline topic in today's news, capturing the attention of individuals across various fields eager to leverage its capabilities to enhance their work. Historically, computers were introduced to take on repetitive and mundane tasks, leading to significant advancements in processing speed, communication infrastructures, and storage capacities. However, with the advent of sophisticated AI algorithms, we are now faced with a new level of challenges and solutions.

In the past, problem-solving involved identifying the issue and explicitly programming the computer to transform input data into desired outputs. Software specialists had to meticulously define every step of the problem-solving process to achieve the correct outcome. This traditional method required a deep understanding of the problem and the logic needed to solve it.

The emergence of AI, however, has revolutionized this approach. Instead of explicitly programming each step, we now provide the system with vast amounts of data, allowing it to "learn" and adjust itself to achieve the desired results. This learning process enables AI to tackle problems that were previously unsolvable using classical methods. By training models with large datasets, AI systems develop the capability to make predictions, recognize patterns, and generate insights without human intervention in the learning process.

This shift from explicit programming to machine learning opens up possibilities for addressing complex problems across diverse domains. The challenge now lies in selecting the appropriate type of learning—supervised, unsupervised, or reinforcement learning—based on the nature of the problem and the available data. Understanding the strengths and limitations of each learning type is essential for effectively harnessing AI to solve real-world problems. From art and entertainment to industry, manufacturing, and healthcare, AI's transformative impact is evident. These developments highlight the need for individuals and organizations to embrace AI, as leveraging its potential can drive innovation, efficiency, and improvements across various domains. As AI continues to evolve, integrating it into diverse fields becomes increasingly crucial. Therefore, understanding and utilizing AI is essential to remain competitive and progressive.

## Problem Statement

This thesis stems from a research project that explored using machine learning to improve structural analysis, specifically by optimizing design parameters in FEM simulations. The goal is to enhance the efficiency and accuracy of engineering analyses. The thesis will further detail how machine learning can be integrated into these workflows, aiming to identify effective models, reduce computational costs, and improve decision-making in engineering design. Individuals involved in this process are highly skilled experts, and their involvement represents a significant investment for the industry. Given the complexity and precision required in these tasks, even small improvements in efficiency can lead to substantial cost savings, increased profitability, and significant time savings. Streamlining their workflow, whether through automation or enhanced tools, has the potential not only to reduce expenses but also to accelerate project timelines, allowing companies to bring products to market faster and gain a competitive edge.

### ML\@Karoprod Project

The BMBF project titled "Machine Learning for the Prediction of Process Parameters and Component Quality in Automotive Body Production (ML\@Karoprod)" focuses on optimizing process parameters in a product chain for deforming a metal plate through a series of operations. From a machine learning perspective, the goal is to streamline this process. Traditionally, an expert conducts FEM simulations, adjusts parameters, and evaluates the results to achieve the desired shape. This iterative process, which takes about 20 minutes per cycle, is repeated until the desired accuracy is reached, with the expert manually making the necessary adjustments and running further simulations.

![caption](img/Chp1/karo_process.png){#fig-karo-proc fig-align="center" width="50%"}

The process begins with deep drawing, a sheet metal forming technique where a metal blank is radially drawn into a forming die by a punch, with the goal of producing a wrinkle-free and crack-free product. Our simulated deep drawing data consists of a number of valid experiments, each characterized by various process parameters such as blank holder force, insertion position, material ID, and punch ID. In collaboration with our partners, we have identified the relevant set of features that are necessary and sufficient to fully determine the effects of a simulation.

![caption](img/Chp1/karoprod.png){#fig-karo-proc2 fig-align="center" width="50%"} ![caption](img/Chp1/pikaso_enhance__none_2K_Standard_r_c_.jpeg){fig-align="center" width="50%"}

|                                                            |                  |
|------------------------------------------------|-----------------------|
| Input parameters                                           | From - to        |
| die diameter                                               | 7.5 .. 12.5mm    |
| die depth in relation to total sheet thickness             | 0.2  .. 0.6      |
| die diameter bottom in relation to punch diameter          | 0.8 .. 1.2       |
| angle die bottom                                           | 15 .. 45 deg     |
| angle die side face                                        | 0 .. 5 deg       |
| punch diameter in relation to die diameter                 | 0.5 .. 0.75      |
| radius punch                                               | 0.25 .. 0.5mm    |
| angle punch face                                           | 0 .. 2 deg       |
| angle punch side face                                      | 0 .. 4 deg       |
| thickness upper sheet in relation to total sheet thickness | 0.2 .. 0.8       |
| bottom thickness                                           | 0.589 .. 5.035mm |

Approximately 1000 simulation files in mesh format were generated by the project partner by varying process parameters. After data cleaning, about 880 of these files were usable. The meshes had three different geometries (with three different depths: 30, 50, and 70), and parameters such as force and material varied, resulting in different final mesh outcomes. The ultimate goal in this section was to train a neural network to predict the dependency between changes in input parameters and the generated mesh. With sufficient training, the model would be able to accurately predict the shape of the final mesh even with new inputs that were not seen during training. This allows the specialist to quickly and accurately observe the final simulation results by manipulating parameters, in a much shorter time compared to traditional finite element methods.

|                                                                                                                                 |                                                                                             |
|------------------------------------|------------------------------------|
| ![The reference mesh for extracting face centers (Drawing depth Zt=70).](img/Chp1/banana_1.png){fig-align="center" width="50%"} | ![The same mesh in a closer view](img/Chp1/banana_zoom.png){fig-align="center" width="50%"} |

|                                                       |                                                                                        |
|------------------------------------|------------------------------------|
| ![caption](img/Chp1/ban_sen1.png){fig-align="center"} | ![caption](img/Chp1/ban_sen_2.png){#fig-karo-proc-2 fig-align="center"} |

## FEM Simulations and applications

The Finite Element Method (FEM) is a powerful computational technique used for solving complex structural, fluid, and thermal problems in engineering and physical sciences. It works by breaking down a large problem into smaller, simpler parts known as finite elements, and then systematically solving these elements to understand the behavior of the entire system. FEM is a numerical approximation of the continuous solution field $u$ of any partial differential equation (PDE) given by Eq. (1) on a given domain $\Omega$ can be performed by various methods. Some of the widely used techniques include finite element method \[ref1\], finite volume method \[ref2\], particle methods \[ref3\], and finite cell method \[ref4\]. In this contribution, we restrict the discussion to Galerkin-based finite element methods.

$$
\mathcal{L}(u) = 0 \quad \text{on } \Omega
$$

$$
u = u_d \quad \text{on } \Gamma_D
$$

$$
\frac{\partial u}{\partial x} = g \quad \text{on } \Gamma_N
$$

Consider the PDE in Eq. (1) defined on a domain $\Omega$ together with the boundary conditions given by Eqs. 2 and 3. Here, $u_d$ and $g$ are the Dirichlet and Neumann boundary conditions on the respective boundaries. A finite element formulation of Eq. (1) on a discretization of the domain with $m$ elements and $n$ nodes, together with boundary conditions, will result in the system of equations shown by Eq. (4). We assume all the necessary conditions on the test and trial spaces \[ref1\] are fulfilled.

$$
\begin{pmatrix}
k_{1,1} & k_{1,2} & \cdots & k_{1,n} \\
k_{2,1} & k_{2,2} & \cdots & k_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
k_{n,1} & k_{n,2} & \cdots & k_{n,n}
\end{pmatrix}
\begin{pmatrix}
u_1 \\
u_2 \\
\vdots \\
u_n
\end{pmatrix} =
\begin{pmatrix}
F_1 \\
F_2 \\
\vdots \\
F_n
\end{pmatrix}
$$

In Eq. (4), ${K}(u)$ is the non-linear left hand side matrix, also called the stiffness matrix. ${u}$ is the discrete solution field, and ${F}$ is the right hand side vector. The residual of the system of equations in Eq. (4) can be written as

$$
r(u^h) = K(u^h)u^h - F
$$

To obtain the solution $u^h$, a Newton–Raphson iteration technique can be employed using the linearization of $r(u^h)$ and its tangent matrix. This requires the solution of a linear system of equations in every iteration. These iterations are carried out until the residual norm $\|r(u^h)\|$ meets the tolerance requirements. For a detailed discussion of the methodology, readers are referred to \[ref2\]. For this residual-based formulation, in case of a linear operator $K$, it takes only one iteration to converge. For a large number of elements and nodes, among different steps of the finite element methodology, the most computationally demanding step is the solution of the linear system of equations. In an application where computational efficiency is critical, like real time simulations \[ref3\] and digital twins \[ref4\], it is imperative that this step be avoided. Techniques suitable for such applications, like model order reduction \[ref5, ref6\], construct a surrogate model of Eq. (4) to reduce this cost significantly. Techniques involving neural-networks can completely avoid this cost, but will require a significant amount of training and test data, which is typically generated by simulating the underlying finite element problem. In “**Finite element method-enhanced neural network for forward problems**” section, we discuss an algorithm that combines residual information from a numerical method to train a neural network for linear PDEs. In this case the residual $r(u^h)$ becomes

$$
r(u^h) = Ku^h - F
$$

FEM is widely used in various industries to simulate physical phenomena and optimize designs, significantly reducing production costs by allowing for virtual testing and refinement before physical prototypes are created. The method gained prominence in the 1960s and has since undergone significant advancements, driven by improvements in computer technology and numerical algorithms. Today, FEM is integral to fields such as aerospace, automotive, civil engineering, and biomedical engineering, offering highly accurate predictions and contributing to innovation and efficiency in product development. In this thesis however, our focus on FEM is specifically related to structural analysis and deformation problems. Since this thesis aims to explore the potential of using AI methods for solving FEM simulations, a brief introduction to neural networks and their functions is necessary. However, before we discuss neural networks, the structure of the mesh and its importance in FEM problems will be briefly discussed.

## Mesh structure and Its Impact on FEM Analysis

A mesh is a specialized type of graph, consisting of vertices (points in 3D space) and edges (connections between these vertices), which collectively form the faces that define the surface of a 3D object. For any given geometry, it is possible to generate an infinite number of meshes, making them highly versatile for representing 3D shapes. Meshes are particularly effective and popular due to their ability to accurately portray complex geometries. By increasing the mesh resolution, the quality and precision of the 3D representation are enhanced, allowing for more detailed and realistic visualizations. Dense meshes, in particular, excel at accurately representing intricate details such as corners and curves, making them essential for rendering complex 3D objects. However, higher resolution meshes come with significant trade-offs, as they require more computational power and advanced graphics cards to manage the increased data load.

![image mesh , vertex, same geometry](img/Chp1/mesh-geom.png){#fig-mesh-geom width="40%" fig-align="center"}

![image mesh , vertex, same geometry](img/Chp1/mesh-geo-topo.png){width="40%" fig-align="center"}

A mesh in the context of Finite Element Method (FEM) is defined as a network of interconnected elements that subdivides a complex geometry into smaller, simpler parts, known as elements or cells. These elements can take various shapes, such as triangles or quadrilaterals in 2D, and tetrahedra or hexahedra in 3D. In this context, the vertices of the mesh are referred to as nodes, and the faces are known as elements. The connections between nodes, which form the edges, are typically called edges or connections in FEM. The mesh serves as the framework over which the mathematical equations governing the physical behavior of the system are solved. The quality and resolution of the mesh are crucial, as they directly influence the accuracy, convergence, and computational efficiency of FEM analyses. A finer mesh, with more elements, generally results in higher accuracy but at the cost of increased computational resources. Conversely, a coarser mesh reduces computational demand but may compromise the precision of the simulation. Understanding the structure and characteristics of the mesh is essential for optimizing FEM simulations and achieving reliable results in complex engineering problems.

\[image examples of good and bad mesh\]

## Introduction to Deep Neural Networks

Deep Neural Networks (DNN) represent a significant evolution in the field of artificial intelligence, particularly in machine learning and pattern recognition. Building on the foundation of traditional neural networks, DeepNNs consist of multiple layers of interconnected neurons, allowing them to model complex patterns and relationships in data more effectively than shallow networks. The advent of DeepNNs has revolutionized numerous fields, including computer vision, natural language processing, and reinforcement learning, marking a pivotal shift in the capabilities of AI systems.

![caption](img/Chp1/AI_diagram.png){#fig-ai-diag fig-align="center" width="30%"}

The concept of neural networks dates back to the mid-20th century, with the introduction of the perceptron by Frank Rosenblatt in 1958. However, the journey towards DeepNNs gained momentum only in the 1980s, with the development of the backpropagation algorithm, a critical breakthrough enabling the training of multi-layered networks. Backpropagation, introduced by Rumelhart, Hinton, and Williams in 1986, made it feasible to adjust the weights of neural networks through gradient descent, efficiently minimizing the error between predicted and actual outcomes. This algorithm remains at the heart of training deep networks, enabling them to learn complex functions from data.

Despite these early advances, DeepNNs struggled to gain traction due to computational limitations and the challenge of vanishing gradients, a problem where gradients used to update the weights become increasingly small in deeper layers, hindering effective learning. This issue was addressed by the introduction of more advanced activation functions like ReLU (Rectified Linear Unit), which helped maintain more consistent gradients, and by innovations such as batch normalization and more sophisticated weight initialization techniques.

Key Concepts and Advancements One of the core principles behind the success of DeepNNs is gradient descent, an optimization algorithm used to minimize the loss function. The loss function $L(\theta)$ quantifies the error in the network's predictions, where $\theta$ represents the network's parameters (weights and biases). Gradient descent iteratively adjusts $\theta$ in the direction opposite to the gradient of the loss function, formally expressed as:

$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)
$$

where $\eta$ is the learning rate, and $\nabla_\theta L(\theta_t)$ is the gradient of the loss function with respect to the parameters. However, the success of gradient descent in deep networks relies heavily on effective weight initialization, appropriate activation functions, and strategies to mitigate overfitting and vanishing gradients. The introduction of ReLU activation functions, expressed as $f(x)=max(0,x)$, was instrumental in overcoming the vanishing gradient problem by ensuring that gradients remain significant in deeper layers, thus allowing for better learning and performance.

### Learning from Data

A crucial aspect of neural networks is the dataset. Typically, the model is trained on a large amount of training data, validated on a separate validation set, and then tested on unseen test data to evaluate its performance. The selection and preprocessing of data are of particular importance in this process. Neural networks often operate best when the data is scaled within a specific range, usually between 0 and 1. Therefore, considerable effort is made to ensure that the data is normalized to fall within this optimal range, which significantly contributes to the effectiveness of the model.

The success of these architectures has been further amplified by large-scale datasets and the availability of massive computational resources, leading to groundbreaking achievements such as Google's AlphaGo, OpenAI's GPT models, and various state-of-the-art systems in image and speech recognition.

### NN Modularity and Structures

Neural networks are remarkably flexible, allowing various architectures and units to be connected and combined to create more complex models tailored to specific problems. For instance, by increasing or decreasing the number of layers and the number of neurons in each layer, the network's performance can be improved. Another crucial factor is the choice of activation functions, which play a significant role in the learning process. Additionally, specialized layers such as convolutional layers and recurrent layers can be added or removed depending on the data type, the complexity of the problem, and the specific requirements. These adjustments are made to achieve the most suitable architecture for the task at hand.

For instance, a Convolutional Neural Network (CNN) can be enhanced with conditional layers, forming a Conditional Convolutional Neural Network (CCNN) that generates different outputs based on additional context. This modularity adds significant power to the neural network, but it also introduces a considerable level of complexity. The goal is to design models that are as simple as possible while still providing adequate performance to solve the targeted problems effectively.

### Loss Function

Loss functions play a crucial role in the learning process of neural networks. They serve as a measure of how well or poorly a model's predictions align with the actual target values. By quantifying the error or "loss" between the predicted outputs and the true labels, the loss function guides the adjustment of the model's weights during training. Essentially, the model seeks to minimize the loss function, and by doing so, it iteratively updates the weights to improve accuracy and performance.

In many cases, standard loss functions like Mean Squared Error (MSE) for regression tasks or Cross-Entropy Loss for classification tasks are sufficient. However, there are situations where a custom loss function is necessary to address specific challenges or nuances of the problem at hand. For instance, in imbalanced datasets, where certain classes are underrepresented, a custom loss function might be designed to assign higher penalties to misclassifications of the minority class, thereby ensuring the model learns more effectively. Crafting a custom loss function often requires deep domain knowledge and experience, as it needs to balance the specific objectives of the task with the overall training process. This expertise helps in fine-tuning the loss function to achieve optimal performance for the given problem.

**Mean Squared Error (MSE) Loss**

MSE is one of the most commonly used loss functions in regression problems. It measures the average squared difference between the predicted values and the actual target values. MSE is particularly sensitive to large errors due to the squaring of the differences, meaning that models producing large errors are penalized more heavily. 
$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

**Cross-Entropy Loss (for binary classification tasks):** 
$$
\text{Cross-Entropy Loss} = - \frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$

**Combined Loss:** 
$$
\text{Combined Loss} = \alpha \cdot \text{Loss1} + \beta \cdot \text{Loss2}
$$

## Multi-Layer Perceptron (MLP)

MLP is a type of feedforward artificial neural network that consists of multiple layers of neurons, where each layer is fully connected to the next one. The basic unit of an MLP is the perceptron, which computes a weighted sum of its input features and passes the result through an activation function. MLPs typically consist of an input layer, one or more hidden layers, and an output layer. The input layer receives the initial data, the hidden layers process the data through multiple transformations, and the output layer produces the final prediction or classification. MLPs are capable of approximating complex functions and are commonly used in tasks such as classification, regression, and pattern recognition.

The mathematical operation of a single perceptron in an MLP can be expressed as: 
$$
z^{(l)} = {W}^{(l)} {a}^{(l-1)} + {b}^{(l)}
$$

Here, ${W}^{(l)}$ represents the weight matrix, ${a}^{(l-1)}$ is the activation from the previous layer, and ${b}^{(l)}$ is the bias vector. The activation for the current layer is then obtained by applying an activation function $\sigma$ to $z^{(l)}$:

$$
{a}^{(l)} = \sigma(z^{(l)})
$$

## Autoencoders

An autoencoder is a type of neural network designed to learn a compressed, efficient representation of input data. It consists of two main components: an encoder and a decoder. The encoder compresses the input data into a lower-dimensional representation, often referred to as the latent space or bottleneck. The decoder then attempts to reconstruct the original input from this compressed representation. The goal is to minimize the difference between the input and the reconstructed output.

Autoencoders, used for tasks like dimensionality reduction or feature learning, can also be improved by incorporating elements such as convolutional layers, recurrent units, or Generative Adversarial Networks (GANs). This integration allows autoencoders to learn more complex data representations, enhancing their effectiveness in applications like anomaly detection or data compression

Mathematically, the process can be represented as: $$
z = f_\theta(x)
$$

$$
\hat{x} = g_\phi({z})
$$

Here, $z$ is the latent representation of the input $x$, and $\hat{x}$ is the reconstructed output. The objective of the autoencoder is to minimize the reconstruction loss, commonly measured as the mean squared error (MSE):

$$
L({x}, \hat{{x}}) = \| {x} - \hat{{x}} \|^2_2
$$

Autoencoders are particularly useful in areas such as dimensionality reduction and reconstruction, where they compress high-dimensional data into a lower-dimensional latent space and then accurately reconstruct the original data. They also excel in denoising by removing noise from corrupted data, and in anomaly detection by identifying outliers based on reconstruction errors. Additionally, autoencoders are valuable in generative modeling, contributing to the creation of new, similar data samples, which is especially beneficial in image synthesis and data augmentation.

## Convolutional Neural Networks

Convolutional Neural Networks (CNNs) are a class of deep neural networks specifically designed to process data with a grid-like structure, such as images. Unlike fully connected networks, where each neuron is connected to every neuron in the previous layer, CNNs take advantage of the spatial structure of the data by using convolutional layers. These layers apply convolutional filters to local regions of the input, which can be mathematically represented as:

$$
y = f * x + b
$$

where $y$ is the output feature map, $f$ is the filter, $*$ denotes the convolution operation, $x$ is the input, and $b$ is the bias term. This operation allows the network to detect patterns such as edges, textures, and shapes in the input data.

![caption](img/Chp1/conv.png){#fig-conv fig-align="center" width="30%"}

A typical CNN architecture consists of multiple layers, including convolutional layers, pooling layers, and fully connected layers. Convolutional layers apply filters to the input to extract features, while pooling layers perform down-sampling, reducing the spatial dimensions of the data. A common pooling operation is max pooling, defined as:

$$
y_{i,j} = \max_{m,n} (x_{i+m,j+n})
$$

where $y_{i,j}$ represents the output after pooling, and $x_{i+m,j+n}$ is the region of the input over which the pooling operation is applied. The final fully connected layers combine the features extracted by the convolutional and pooling layers to make predictions, typically using a softmax function for classification:

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

where $z_i$ is the input to the softmax function, and $K$ is the number of classes.

![caption](img/Chp1/CNN.png){#fig-cnn-layers fig-align="center" width="50%"}

CNNs are widely used in various applications, including image classification, object detection, and image segmentation. Their ability to automatically learn and extract features from raw pixel data has led to significant advancements in computer vision. The modularity of CNNs allows them to be easily adapted to different tasks by adjusting the architecture, such as changing the number of layers or the size of filters. This flexibility, combined with their high accuracy and efficiency, makes CNNs a cornerstone of modern deep learning applications.

## Conditional Neural Networks

Conditional Neural Networks are a specialized type of neural network where the output is conditioned not only on the input data but also on an additional context or condition. This conditioning can be represented as an additional input that influences the network's behavior, enabling it to generate different outputs depending on the given condition.

Consider a standard neural network where the input data is $x$ and the output is $y$. The network typically learns a function $f_\theta(x) = y$, where $\theta$ represents the parameters of the network. In a Conditional Neural Network, an additional condition $c$ is introduced, modifying the function to:

$$
f_{\theta}({x}, {c}) = {y}
$$

Here, $c$ is the condition that influences the network's output. This condition could be a class label, a set of parameters, or any other contextual information relevant to the task.

If a neural network is designed to perform a specific task on a dataset, and we want to generalize that task to different categories of data, we can introduce a condition or context to the network. This allows the network to adapt its behavior based on the category or condition, making it more versatile. Moreover, this conditioning is not tied to a specific architecture; for example, a condition can be added to a convolutional network or any other architecture. For instance, in a conditional image generation task, $x$ might represent a latent vector, and $c$ could be a label corresponding to the class of the image to be generated. The network would then generate an image $y$ conditioned on both $x$ and $c$:

$$
{y} = G_{\theta}({x}, {c})
$$

Where $G_\theta$ is the generator function of the Conditional Neural Network.

The loss function in a Conditional Neural Network often takes the condition into account as well. For example, in a supervised learning scenario, the loss function $L$ could be defined as:

$$
L_\theta = \frac{1}{N} \sum_{i=1}^{N} \ell\left(f_{\theta}({x}_i, {c}_i), {y}_i\right)
$$

Where $\ell$ is a suitable loss function (e.g., cross-entropy or mean squared error), $N$ is the number of training examples, and $y_i$ is the true output corresponding to input $x_i$ under condition $c_i$.

Conditional Neural Networks are widely used in various applications, such as conditional image generation, style transfer, and structured prediction tasks. Their ability to incorporate context or conditions makes them highly versatile and effective for problems where the output must be tailored based on specific criteria. This flexibility allows conditions to be added to various architectures, whether it's a convolutional network or any other type, broadening their applicability.

## Research Questions

-   replacing mesh processing with implicit representations to have a continuous representation of models

-   continuous fields

-   infinite resolution

-   we need to deal with 3D data and NNs , so we need to find the best representation to keep the net size as small as possible even for more complicated geometries, good accuracy and most importantly be able to work with dynamics of a 3d shape

-   reduce the time of simulations

-   provide accurate set of process parameters