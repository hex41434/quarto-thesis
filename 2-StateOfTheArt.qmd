# State of the art {#sec-sota}
As the research topic is multidisciplinary, a broad understanding of various fields is required. In this chapter, the focus will first be placed on the Finite Element Method (FEM), exploring its history and its role as a fundamental tool in structural analysis. Then, the mesh structure will be reviewed as the primary data structure for 3D processing in FEM. Following this, an introduction to neural networks and their key characteristics will be provided. The chapter will conclude with an overview of several well-known neural network architectures, examining their features and applications.

## Finite Element Method

The Finite Element Method (FEM) [@liu2022eighty] is a robust computational technique widely used in engineering and physical sciences to solve complex problems related to structures, fluids, and thermal dynamics. While the exact origin of the finite element method is hard to define, its creation was driven by the necessity to solve complex problems in elasticity and structural analysis. The method’s development traces back to early contributions by Alexander Hrennikoff [@hrennikoff1941solution] and Richard Courant [@courant1994variational] in the 1940s. The essence of FEM lies in its ability to break down a large, complicated problem into smaller, more manageable parts known as finite elements. By systematically solving these elements, FEM provides an approximate solution that represents the behavior of the entire system.

### Mathematical Foundations

FEM is essentially a numerical approximation method that provides a solution for the continuous field $u$ of any partial differential equation (PDE) defined on a specific domain $\Omega$. The governing PDE can be expressed generally as:

$$
\mathcal{L}(u) = 0 \quad \text{on } \Omega
$$ {#eq-fem-1}

with boundary conditions:

$$
u = u_d \quad \text{on } \Gamma_D
$$ {#eq-fem-2}

$$
\frac{\partial u}{\partial x} = g \quad \text{on } \Gamma_N
$$ {#eq-fem-3}

In these equations, $L(u)$ represents a differential operator applied to the field $u$. This operator encapsulates the specific physical laws governing the problem, such as heat conduction, elasticity, fluid flow, etc. The nature of $L(u)$ depends on the type of PDE being solved:

For thermal problems: $L$ could represent the Laplace operator $\nabla^2 u$ , describing how heat diffuses through a material.

For structural problems: $L(u)$ might include terms representing the balance of forces, such as in the elasticity equations where $L(u)$ could involve the divergence of stress tensors.

For fluid dynamics: $L(u)$ could include the Navier-Stokes equations, which describe the motion of fluid substances.

In general, $L(u) = 0$ defines a PDE that the field $u$ must satisfy over the domain $\Omega$.

### Discretization and System of Equations

To solve this PDE using FEM, the domain $\Omega$ is discretized into $m$ finite elements with $n$ nodes. This discretization, along with the boundary conditions, leads to a system of linear or non-linear equations. For a linear operator $L$, this system can be represented as:

$$
\begin{pmatrix}
k_{1,1} & k_{1,2} & \cdots & k_{1,n} \\
k_{2,1} & k_{2,2} & \cdots & k_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
k_{n,1} & k_{n,2} & \cdots & k_{n,n}
\end{pmatrix}
\begin{pmatrix}
u_1 \\
u_2 \\
\vdots \\
u_n
\end{pmatrix} =
\begin{pmatrix}
F_1 \\
F_2 \\
\vdots \\
F_n
\end{pmatrix}
$$ {#eq-Kmatrix}

In this equation, ${K}(u)$ is the non-linear left hand side matrix, also called the stiffness matrix. ${u}$ represents the unknowns (i.e., the solution at each node), and ${F}$ is the load vector or the right-hand side of the equation.

### Solving the System: Iterative Techniques

The solution $u^h$ is obtained by solving this system of equations iteratively. A common approach is to use the Newton–Raphson method, which involves linearizing the residual of the system:

$$
r(u^h) = K(u^h)u^h - F
$$ {#eq-fem-it}

The iterations continue until the norm of the residual $r(u^h)$ meets the desired tolerance level. For linear problems, the system converges in one iteration, but for non-linear systems, multiple iterations are generally required. The most computationally intensive step in FEM is often the solution of this linear system, particularly when dealing with a large number of elements and nodes. Given that the mesh and its structure play a critical role in the success or failure of FEM, we will address this topic in detail in the next section.

## Mesh structure and Its Impact on FEM Analysis

A mesh is a specialized type of graph, consisting of vertices (points in 3D space) and edges (connections between these vertices), which collectively form the faces that define the surface of a 3D object. For any given geometry, it is possible to generate an infinite number of meshes, making them highly versatile for representing 3D shapes. Meshes are particularly effective and popular due to their ability to accurately portray complex geometries. By increasing the mesh resolution, the quality and precision of the 3D representation are enhanced, allowing for more detailed and realistic visualizations. Dense meshes, in particular, excel at accurately representing intricate details such as corners and curves, making them essential for rendering complex 3D objects. However, higher resolution meshes come with significant trade-offs, as they require more computational power and advanced graphics cards to manage the increased data load.

![three different meshes to represent one 3D object](img/Chp1/mesh-geom.png){#fig-mesh-geom width="40%" fig-align="center"}

Another aspect of the mesh is its topology. Mesh topology refers to the arrangement and connectivity of elements, such as triangles, quadrilaterals, or tetrahedra, within a mesh. It defines how these elements are connected through their vertices and edges, focusing on the structure and relationships between elements rather than their specific geometric positions. Topology is concerned with the mesh's internal structure without considering the actual spatial coordinates of the points.

On the other hand, geometry deals with the actual spatial placement and shape of the elements in the mesh. Geometry provides the coordinates and dimensions of each element, defining the shape and size of the mesh within the physical domain. While topology describes the connectivity, geometry specifies where the vertices are positioned and how the mesh represents the physical space.

The relationship between topology and geometry is crucial in mesh generation. A mesh with the same topology can have different geometries based on the positioning of the vertices. Good topology is essential for accurately representing the geometry of the model, especially in complex regions. Poor topology, such as elements with bad aspect ratios or improper connectivity, can lead to inaccuracies in representing the geometry, negatively affecting the quality and convergence of the finite element analysis. The comparison between topology and mesh geometry is shown in Figure @fig-topo-geom.

![mesh topology vs. geometry](img/Chp1/mesh-geo-topo.png){#fig-topo-geom width="40%" fig-align="center"}

A mesh in the context of FEM is defined as a network of interconnected elements that subdivides a complex geometry into smaller, simpler parts, known as elements or cells. These elements can take various shapes, such as triangles or quadrilaterals in 2D, and tetrahedra or hexahedra in 3D. In this context, the vertices of the mesh are referred to as nodes, and the faces are known as elements. The connections between nodes, which form the edges, are typically called edges or connections in FEM. The mesh serves as the framework over which the mathematical equations governing the physical behavior of the system are solved. The quality and resolution of the mesh are crucial, as they directly influence the accuracy, convergence, and computational efficiency of FEM analyses. A finer mesh improves accuracy but requires more computational resources, while a coarser mesh reduces computational demand but lowers precision. Understanding the structure and characteristics of the mesh is essential for optimizing FEM simulations and achieving reliable results in complex engineering problems. In Figure @fig-fem-mesh-quality, an example of a suitable mesh for FEM and an unsuitable one can be seen.

![a bad mesh (left) vs a good mesh (right) for FEM](img/Chp1/fem-mesh-opt.png){#fig-fem-mesh-quality width="40%" fig-align="center"}

In general, suitable meshes for FEM should exhibit several essential characteristics: they must be sufficiently refined to accurately capture the geometry and stress gradients of the structure, with a higher mesh density in regions of high stress or complex geometry. The mesh elements should maintain an appropriate aspect ratio, avoiding excessively elongated or distorted shapes, to ensure numerical accuracy and stability. Additionally, the mesh should be well-aligned with the boundaries and features of the model to minimize interpolation errors and enhance the precision of the analysis. It is always important to remember that the significance of the mesh is so crucial that even with a poorly constructed mesh, we can obtain results that are vastly different and full of errors.

## FEM : Current Advancements and Challenges

Despite its significant power and advantages, FEM's primary focus on mesh construction and solving numerous complex partial differential equations (PDEs) can lead to slow performance when dealing with highly intricate problems. This method demands substantial computational power, and each parameter change necessitates a complete re-execution, demonstrating limited flexibility in adapting to changes. Consequently, scientists are actively seeking to improve and potentially replace FEM with more efficient methods.

Due to the challenges associated with traditional FEM, mesh-free methods have consistently garnered attention. These approaches aim to address FEM's limitations by offering greater flexibility, faster computations, and the ability to handle complex scenarios more efficiently. However, it is important to note that FEM remains comprehensive and applicable for a wide range of problems, including solid and fluid mechanics, among others. Alternative methods are often tailored to specific types of problems and applications, meaning that there is currently no complete replacement for FEM. Instead, these alternative methods are being developed and refined to provide better and more suitable solutions for specific issues​. for instance in @zhang2024state Mesh-free methods for crack problems have been reviewed.

Among the proposed approaches, the Smoothed Particle Hydrodynamics (SPH) \[\] , the Element-Free Galerkin Method (EFGM) \[\] and the Material Point Method (MPM) \[\] have been notable efforts. SPH employs particles to simulate fluid elements and interactions, proving particularly effective for complex fluid dynamics problems. EFGM, on the other hand, utilizes nodes and shape functions to approximate solutions, providing significant flexibility and accuracy for intricate geometries and boundary conditions. Additionally, MPM represents materials as moving points through a computational grid, making it especially suitable for scenarios involving large deformations and complex material behaviors. These mesh-free approaches offer enhanced adaptability and computational efficiency, addressing some of the core limitations of FEM​​.


## AI innovations in the FEM domain

In recent years, numerous alternative solutions have been proposed to replace traditional FEM in various problems and subfields. For example, techniques such as \[mention specific alternatives\] and \[mention another alternative\] have been suggested. Specifically, within the domain of structural analysis, which is the focus of this thesis, approaches like \[mention relevant methods\] and \[mention another relevant method\] have been explored.

A significant category of these advancements involves solving partial differential equations (PDEs), with Physics-Informed Neural Networks (PINNs) being a notable example. Another set of solutions simplifies the simulation by converting the simulation shell into a 2D image and applying conventional image processing techniques, such as convolutional neural networks (CNNs).

This thesis will review these approaches, discussing the advantages and disadvantages of each.

------------------------------------------------------------------------

Similar to other fields, the application of AI in FEM has a relatively long history. While a comprehensive solution to completely replace FEM is yet to be found, AI's advancements in this area are promising.

in @zhang2024state the Mesh-free methods for crack problems have been reviewed.

with examples such as neural networks for stress analysis, which have been proposed for specific applications\[\].

For deformation problems, solutions often involve simplifying the problem to 2D images and employing image processing techniques\[\].

Advancements in replacing the Finite Element Method (FEM) are not confined to a few specific techniques. Instead, various solutions have been proposed depending on the nature of the problem, the type of data, and the complexities involved. These diverse approaches reflect the need for tailored solutions to effectively address the unique challenges presented by different FEM applications.

Physics-Informed Neural Networks (PINNs) are another innovative approach, integrating physical laws into the learning process to solve PDEs. In the following sections, we will review some of the most significant works related to our problem:

### PDE Solving using NNs

Physics-Informed Neural Networks (PINNs) @raissi2019physics are a class of neural networks that integrate physical laws described by partial differential equations (PDEs) into the learning process. They leverage the universal approximation capability of neural networks to solve forward and inverse problems governed by PDEs. The core idea of PINNs is to minimize a loss function that includes both the data-driven error and the residuals of the PDEs, thereby ensuring that the learned solution satisfies the underlying physical laws.

In the context of a PDE, such as $\mathcal{N}(u(x)) = 0$, where $\mathcal{N}$ is a differential operator and $u(x)$ is the solution, the loss function $\mathcal{L}$ for a PINN can be expressed as:

$$
\mathcal{L} = \mathcal{L}_{\text{data}} + \mathcal{L}_{\text{PDE}}
$$

Here, $\mathcal{L}_{\text{data}}$ represents the mean squared error (MSE) between the neural network's predictions $u_{\theta}(x)$ and the observed data points $u_{\text{obs}}(x)$:

$$
\mathcal{L}_{\text{data}} = \frac{1}{N} \sum_{i=1}^{N} \left( u_{\theta}(x_i) - u_{\text{obs}}(x_i) \right)^2
$$

The term $\mathcal{L}_{\text{PDE}}$ enforces the PDE constraints by computing the MSE of the residuals at collocation points $x_c$:

$$
\mathcal{L}_{\text{PDE}} = \frac{1}{M} \sum_{j=1}^{M} \left( \mathcal{N}(u_{\theta}(x_{c_j})) \right)^2
$$

By optimizing the combined loss $\mathcal{L}$, the neural network is trained to produce a solution that fits the observed data while also satisfying the physical constraints imposed by the PDE.

PINNs have been successfully applied to various challenging problems, including fluid dynamics, structural mechanics, and electromagnetic simulations. Their ability to incorporate prior physical knowledge directly into the learning process makes them a powerful tool for modeling complex systems where data is scarce or noisy. Additionally, PINNs can be used for solving inverse problems, where the goal is to infer unknown parameters or functions within the PDEs, by including terms in the loss function that account for the discrepancies between the predicted and observed data, as well as the governing physical laws. This versatility highlights the potential of PINNs in enhancing the accuracy and robustness of simulations in scientific and engineering applications.

### Image based fems ...





Despite significant advancements in various fields, a comprehensive model specifically designed to address the problem of 3D shape deformation using AI has not yet been developed (as of the time of writing this thesis). Many existing methods are frequently confined to 2D spaces, with fewer efforts made to extend these solutions to 3D problems. In real-world applications, interacting with 3D data is preferable as it more closely resembles actual conditions, enhancing the realism and accuracy of simulations. Expanding AI applications to 3D FEM simulations can significantly improve their applicability and fidelity in real-world scenarios.

On the other hand, various AI techniques for working with 3D data have been actively pursued in domains such as computer graphics, 3D reconstruction, 3D object classification etc. It is important to note that this is a multidisciplinary issue, requiring collaboration across different scientific fields. Interaction among specialists from various domains is crucial to finding a common ground and proposing more effective solutions. This interdisciplinary cooperation is essential for aligning different areas of expertise to develop more robust and effective methods for 3D shape deformation.

Therefore, in this section, we will explore computer science methods that are similar to our problem and work with 3D data using AI tools. The work presented here is relatively specialized, and as of now, no existing AI method has been found that directly addresses our specific problem. However, we can expect to see an increase in intelligent models tackling this issue in the near future. The continued development of AI in this area holds great promise for improving the accuracy and efficiency of simulations involving 3D shape deformation.

FEM is widely used in various industries to simulate physical phenomena and optimize designs, significantly reducing production costs by allowing for virtual testing and refinement before physical prototypes are created. The method gained prominence since its inception and has since undergone significant advancements, driven by improvements in computer technology and numerical algorithms. Today, FEM is integral to fields such as aerospace, automotive, civil engineering, and biomedical engineering, offering highly accurate predictions and contributing to innovation and efficiency in product development. In this thesis however, our focus on FEM is specifically related to structural analysis and deformation problems. Since this thesis aims to explore the potential of using AI methods for solving FEM simulations, a brief introduction to neural networks and their functions is necessary.


## Introduction to Deep Neural Networks

Deep Neural Networks (DeepNN) [@lecun2015deep] represent a significant evolution in the field of artificial intelligence, particularly in machine learning and pattern recognition. Building on the foundation of traditional neural networks, DeepNNs consist of multiple layers of interconnected neurons, allowing them to model complex patterns and relationships in data more effectively than shallow networks. The advent of DeepNNs has revolutionized numerous fields, including computer vision, natural language processing, and reinforcement learning, marking a pivotal shift in the capabilities of AI systems. The diagram in @fig-ai-diag presents how Artificial Intelligence, Machine Learning, Neural Networks, and Deep Learning are related.

![A hierarchical diagram showing the relationship between Artificial Intelligence, Machine Learning, Neural Networks, and Deep Learning.](img/Chp1/AI_diagram.png){#fig-ai-diag fig-align="center" width="30%"}

### Perceptron

The history of the perceptron begins in 1943 with McCulloch and Pitts [@mcculloch1943logical], who developed the first theoretical model of a neuron. In 1958, Frank Rosenblatt [@Rosenblatt_1957_6098] expanded on their work and introduced the perceptron as a learning algorithm for binary classification. His work laid the foundation for neural networks and machine learning as we know them today.

It simulates the way a single neuron in the human brain works by taking multiple input signals, assigning each a weight, summing them up, and then passing the result through an activation function to produce an output. This simple model can classify data into two categories by finding a linear decision boundary, making it foundational for understanding more complex neural network structures. Despite its limitations, such as being unable to solve problems that aren't linearly separable, the perceptron laid the groundwork for modern neural networks and deep learning.

![Perceptron](img/Chp1/perceptron.png){#fig-perceptron fig-align="center" width="50%"}

### Forward Pass

In a neural network, the input data is fed forward through the network to generate an output. This involves calculating the output of each neuron in each layer by applying a weighted sum of inputs followed by an activation function.

For a given neuron, the output can be represented as: 
$$
z_j = \sum_{i} w_{ji}*x_i + b_j
$$ {#eq-prec1}

where:

$z_j$ is the weighted input to neuron $j$, $w_{ji}$ is the weight connecting input $i$ to neuron $j$, $x_i$ is the input from the previous layer, $b_j$ is the bias term.

The output $a_j$ of neuron $j$ is then calculated using an activation function $\phi$ :\
$$
a_j = \phi(z_j)
$$ {#eq-prec2}

### Backpropagation

The backpropagation algorithm, which has its roots in the 1960s, became widely known after Rumelhart, Hinton, and Williams published their breakthrough paper in 1986 [@rumelhart1986learning]. Before this, training deep neural networks was difficult due to inefficient methods for adjusting weights in multiple layers. Backpropagation solved this problem by calculating the error at the output layer and propagating it backward through the network, adjusting weights layer by layer to minimize the error. Once the forward pass generates an output, the backward pass applies backpropagation to determine how much each weight contributed to the error, significantly improving the training process and leading to the rise of deep learning in later years. The partial derivative of the output with respect to the weight $w_{ji}$ is computed using the chain rule of calculus. For a neuron in the output layer, the error gradient with respect to a weight is given by: 
$$
\frac{\partial E}{\partial w_{ji}} = \delta_j \cdot a_i
$$ {#eq-bp-1} where:

$E$ is the error associated with the output, $\delta_j$ is the error term for neuron $j$, $a_i$ is the output of the neuron in the previous layer.

The error term $\delta_j$ for each neuron in the output layer is: 
$$
\delta_j = \phi'(z_j) \cdot (a_j - y_j)
$$ {#eq-bp-2}

where $y_j$ is the target output and $\phi'(z_j)$ is the derivative of the activation function at neuron $j$'s input.

For neurons in the hidden layers, $\delta_j$ is computed as: 
$$
\delta_j = \phi'(z_j) \sum_k \delta_k w_{kj}
$$ {#eq-bp-3}

where the sum is over all neurons $k$ in the subsequent layer that receive input from neuron $j$.

### Optimizers

Optimizers are algorithms used to update the weights in a neural network to minimize the loss function and improve performance. Common optimizers include Gradient Descent, Adam and RMSProp. Optimizers determine how quickly or slowly a model learns and converge toward an optimal solution. There are various types of optimizers, each with its strengths and suited to different types of tasks and data distributions.

#### Gradient Descent

Gradient descent is the optimization technique used to adjust the network weights. In its simplest form, the weights are updated as follows: $$
w_{ji}^{new} = w_{ji}^{old} - \eta \frac{\partial E}{\partial w_{ji}}
$$ {#eq-gd}

where:

$\eta$ is the learning rate, a small positive number that controls the step size of the update,

$\frac{\partial E}{\partial w_{ji}}$ is the gradient of the error with respect to the weight.

This process is repeated iteratively across multiple epochs, gradually adjusting the weights to reduce the overall error of the network.

The error mentioned here is formally known as the loss function $L(\theta)$, which quantifies the difference between the predicted outputs and the actual target values. Here, $\theta$ represents the network's parameters (weights and biases).

$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)
$$ {#eq-gd-loss}

where $\eta$ is the learning rate, and $\nabla_\theta L(\theta_t)$ is the gradient of the loss function with respect to the parameters. The gradient $\nabla_\theta L(\theta_t)$ indicates the direction and rate of the steepest increase in the loss. By moving in the opposite direction, gradient descent reduces the loss, thereby improving the performance of the neural network. However, the success of gradient descent in deep networks relies heavily on factors such as effective weight initialization, the choice of activation functions, and strategies to mitigate challenges like overfitting and the vanishing gradient problem. The introduction of ReLU activation functions, expressed as $Relu(x)=\max(0,x)$, was instrumental in overcoming the vanishing gradient problem by ensuring that gradients remain significant in deeper layers, thus allowing for better learning and performance.

While gradient descent is the foundation of most optimization processes in deep learning, modern optimizers like Adam, RMSProp, and Adagrad improve on basic gradient descent by dynamically adjusting the learning rates and using past information (e.g., momentum or adaptive learning rates). These improvements help accelerate convergence and make training more efficient, especially in cases where the loss landscape is complex and difficult to navigate.

#### Stochastic Gradient Descent

Stochastic Gradient Descent (SGD) is a classic optimization algorithm that updates the model's parameters based on a small random subset (or "batch") of the training data, instead of using the entire dataset like standard gradient descent. This makes the process faster and less computationally expensive. However, because SGD uses a random subset, its updates tend to be noisier, which can cause fluctuations in the loss function. Adding momentum to SGD can help smooth out these noisy updates, leading to more stable convergence. SGD updates the parameters by moving in the direction of the negative gradient of the loss function with respect to the parameters. The update rule for SGD is: 
$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)
$$ {#eq-sgd}

Where:

$\theta_t$ is the parameter at time step $t$,

$\eta$ is the learning rate,

$\nabla_\theta L(\theta_t)$ is the gradient of the loss function with respect to $\theta_t$.

#### Root Mean Square Propagation

Root Mean Square Propagation (RMSProp) is an adaptive learning rate method that scales the learning rate of each parameter by dividing the gradient by a moving average of its recent squared values. This helps RMSProp to handle non-stationary objectives (where the data distribution changes over time) and improves convergence. It is particularly effective when dealing with mini-batch gradient descent, as it adjusts the learning rate based on how large or small the gradients are, preventing large parameter updates that can hinder training.The update rule is:

1.  Compute the gradients: 
    $$
    g_t = \nabla_\theta L(\theta_t)
    $$ {#eq-rms-1}

2.  Update the moving average of squared gradients: 
    $$
    v_t = \beta v_{t-1} + (1 - \beta) g_t^2
    $$ {#eq-rms-2}

3.  Update the parameters: 
    $$
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} g_t
    $$ {#eq-rms-3}

Where:

$\eta$ is the learning rate, $\beta$ is the decay rate for the moving average (commonly $\beta = 0.9$), $\epsilon$ is a small constant to prevent division by zero (usually $10^{-8}$), and $v_t$ is the moving average of the squared gradients.

#### Momentum

Momentum is a technique used in optimization algorithms to accelerate convergence and reduce oscillations, especially in cases where gradients change direction frequently. It works by maintaining an exponentially decaying moving average of past gradients, which allows the optimizer to continue moving in the direction of consistent gradients, thus dampening oscillations and speeding up convergence. Momentum introduces an additional term to the update rule that accumulates the gradients from previous steps. This accumulated gradient is used to "push" the parameters more in the direction of the overall trend of the loss surface, making the updates smoother and more efficient.

The update rule for momentum is:

1.  Update the velocity (accumulated gradients): 
    $$
    v_t = \beta v_{t-1} + (1 - \beta) \nabla_\theta L(\theta_t)
    $$

2.  Update the parameters using the velocity: 
    $$
    \theta_{t+1} = \theta_t - \eta v_t
    $$

Where:

$\eta$ is the learning rate,

$\beta$ is the momentum coefficient (commonly set to 0.9),

$v_t$ is the velocity (the exponentially weighted moving average of the gradients),

$\nabla_\theta L(\theta_t)$ is the gradient of the loss function with respect to $\theta_t$.

#### Adaptive Moment Estimation

Adaptive Moment Estimation (Adam) is an adaptive learning rate optimization algorithm that combines the advantages of two popular methods: momentum and RMSProp. It computes individual learning rates for each parameter based on estimates of the first and second moments (mean and variance) of the gradients. This helps the optimizer adjust the learning rate dynamically based on the gradients' behavior. Adam generally works well across a wide range of tasks, making it a popular choice for deep learning applications. It also incorporates bias correction to ensure unbiased estimates in the early stages of training. Adam uses moving averages of the gradients and squared gradients for adaptive learning rates. The update rule is as follows:

1.  Compute the gradients: 
    $$
    g_t = \nabla_\theta L(\theta_t)
    $$ {#eq-adam-1}
2.  Update biased first moment estimate (mean of gradients): 
    $$
    m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
    $$ {#eq-adam-2}
3.  Update biased second moment estimate (variance of gradients): 
    $$
    v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
    $$ {#eq-adam-3}
4.  Bias correction for the first and second moments: 
    $$
    \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
    $$ {#eq-adam-4}
5.  Update the parameters: 
    $$
    \theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
    $$ {#eq-adam-5}

Where:

$\eta$ is the learning rate,

$\beta_1$ and $\beta_2$ are decay rates for the first and second moment estimates (default values $\beta_1 = 0.9$, $\beta_2 = 0.999$,

$\epsilon$ is a small constant to prevent division by zero (usually $10^{-8}$),

$m_t$ is the exponentially weighted average of the gradients (first moment estimate),

$v_t$ is the exponentially weighted average of the squared gradients (second moment estimate).

Usually, starting with Adam is a common approach, as it is generally effective across many tasks. Experimenting with different optimizers may be necessary if better performance is needed. Fine-tuning the optimizer's specific hyperparameters (such as $\beta_1$ and $\beta_2$ in Adam) can also lead to improved results.

### Activation Function

An activation function in neural networks is a mathematical function applied to the output of a neuron that introduces non-linearity into the model. These functions help the network learn complex patterns by allowing it to capture non-linear relationships between inputs and outputs. Activation functions determine whether a neuron should be "activated" or "fired" based on its input, controlling the flow of information in the network. Without activation functions, the model would behave like a simple linear regression and would not be able to solve more complex tasks. Below are some of the most important activation functions along with their formulas:

#### Linear Activation Function

The linear (or identity) activation function simply passes the input as it is without any transformation.

$$
f(x) = x
$$ {#eq-af-lin}

-   **Range**: (-$\infty$, $\infty$)
-   **Common use case**: Output layer in regression tasks for predicting continuous values.

#### Sigmoid Activation Function

The sigmoid function maps the input to a range between 0 and 1. It’s widely used in binary classification problems.

$$
f(x) = \frac{1}{1 + e^{-x}}
$$ {#eq-af-sig}

-   **Range**: (0, 1)
-   **Common use case**: Output layer in binary classification.

#### Hyperbolic Tangent (Tanh) Activation Function

The tanh function maps the input to a range between -1 and 1, offering a zero-centered output.

$$
\tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}
$$ {#eq-af-tanh}

-   **Range**: (-1, 1)
-   **Common use case**: Hidden layers in many neural networks.

#### ReLU (Rectified Linear Unit) Activation Function

ReLU is one of the most commonly used activation functions in modern deep learning networks. It outputs the input directly if positive, otherwise, it outputs zero.

$$
f(x) = \max(0, x)
$$ {#eq-af-relu}

-   **Range**: \[0, $\infty$)
-   **Common use case**: Hidden layers in convolutional and fully connected networks.

#### Leaky ReLU Activation Function

Leaky ReLU is a variation of ReLU that allows a small, non-zero gradient when the input is negative, helping mitigate the "dying ReLU" problem.

$$
f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}
$$ {#eq-af-lrelu}

Where $\alpha$ is a small constant (typically 0.01).

-   **Range**: (-$\infty$, $\infty$)
-   **Common use case**: Hidden layers to avoid dead neurons.

#### Sine Activation Function

The sine activation function is less commonly used but can be beneficial in certain special cases or scientific models.

$$
f(x) = \sin(x)
$$ {#eq-af-sin}

-   **Range**: (-1, 1)
-   **Common use case**: Experimental or specific tasks requiring periodic behavior.

These activation functions play critical roles in determining the performance of neural networks, with each function having its strengths and weaknesses depending on the task at hand.

### Loss Function

Loss functions serve as a measure of how well or poorly a model's predictions align with the actual target values. By quantifying the error or "loss" between the predicted outputs and the true labels, the loss function guides the adjustment of the model's weights during training. Essentially, the model seeks to minimize the loss function, and by doing so, it iteratively updates the weights to improve accuracy and performance.

In many cases, standard loss functions like Mean Squared Error (MSE) for regression tasks or Cross-Entropy Loss for classification tasks are sufficient. However, there are situations where a custom loss function is necessary to address specific challenges or nuances of the problem at hand. For instance, in imbalanced datasets, where certain classes are underrepresented, a custom loss function might be designed to assign higher penalties to misclassifications of the minority class, thereby ensuring the model learns more effectively. Crafting a custom loss function often requires deep domain knowledge and experience, as it needs to balance the specific objectives of the task with the overall training process. This expertise helps in fine-tuning the loss function to achieve optimal performance for the given problem.

#### Mean Absolute Error

L1 loss, also known as Mean Absolute Error (MAE), is a loss function commonly used in NN models, especially for regression tasks. It measures the average of the absolute differences between predicted values and actual values. The L1 loss is robust to outliers since it penalizes errors linearly rather than quadratically like L2 loss. Minimizing L1 loss encourages sparsity in the model's parameters, making it useful in contexts such as feature selection and sparse models. The formula for L1 loss is given as:
$$
MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y_i}|
$$ {#eq-mae-1}

Where $y$ represents the actual values, $\hat{y}$ the predicted values, and $n$ is the total number of data points. The absolute value function ensures that the magnitude of the error is considered regardless of its direction.

#### Mean Squared Error

MSE loss or L2 loss, is one of the most commonly used loss functions in regression problems. It measures the average squared difference between the predicted values and the actual target values. MSE is particularly sensitive to large errors due to the squaring of the differences, meaning that models producing large errors are penalized more heavily. 

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$ {#eq-mse-1}

#### Combined Loss

Integrating different loss functions into deep neural networks allows for balancing various aspects of model performance. A single loss function might not capture all the essential characteristics for optimal learning. For example, one loss function $Loss_A$ might focus on robustness to outliers, while another loss function $Loss_B$ could emphasize minimizing large errors or encouraging smooth predictions. By combining these loss functions, we can leverage the strengths of both. The combined loss can be written as:

$$
Combined Loss = \alpha \cdot {Loss_A} + \beta \cdot {Loss_B}
$$ {#eq-loss-combi}

Here, $\alpha$ and $\beta$ are non-negative hyperparameters that determine how much each loss contributes to the final combined loss. This approach allows fine-tuning to achieve a desirable trade-off between different properties, such as robustness and precision, depending on the specific task.

### Common Challenges and Techniques

Neural networks, while powerful, face several key challenges during training and application. One of the most common issues is underfitting, which occurs when the model is too simple to capture the underlying patterns in the data. This often happens when the neural network has too few neurons or layers, or if the model is not trained for long enough. Underfitting results in poor performance on both the training and test sets, as the network fails to learn the complexities of the task.

On the other hand, overfitting is the opposite problem. Overfitting happens when a neural network becomes too tailored to the training data, capturing noise and irrelevant details, which hinders its ability to generalize to new, unseen data. As a result, the model performs well on the training set but struggles with generalization, leading to poor performance on real-world tasks.Techniques such as regularization (L1/L2), dropout, and data augmentation are commonly used to prevent overfitting.

Another major challenge is convergence during training. Neural networks are trained using gradient-based optimization techniques like gradient descent, and the learning process can sometimes get stuck in local minima or saddle points. Choosing appropriate learning rates and initialization methods is crucial for ensuring smooth convergence. If the learning rate is too high, the model may oscillate or fail to converge, while a too-low learning rate can slow down training.

A particularly difficult issue, especially in deep networks, is the vanishing gradient problem. This occurs when gradients in the backpropagation process become extremely small, especially in the earlier layers of deep networks. When this happens, the weights in these layers receive little to no update, stalling learning. This problem is more prevalent with certain activation functions like sigmoid and tanh. To mitigate this, modern architectures often use activation functions like ReLU and its variants, which help maintain gradient flow. Techniques like batch normalization and proper weight initialization also help combat the vanishing gradient problem.

#### L2 Regularization

L2 regularization, also known as weight decay, is a technique used to prevent overfitting by discouraging large weights in a neural network. It works by adding a penalty term to the loss function, which is proportional to the sum of the squared weights of the network. This penalty term forces the optimization process to prefer smaller weights, making the model less complex and more generalizable to unseen data.

The L2 regularization term is scaled by a regularization parameter $\lambda$, which controls the strength of the regularization. The modified loss function with L2 regularization is:

$$
L_{\text{reg}} = L_{\text{original}} + \lambda \sum_i w_i^2
$${#eq-l2-reg}

where $L_{\text{original}}$ is the original loss function (e.g., cross-entropy or mean squared error), $\lambda$ is the regularization parameter, and \(w_i\) represents the individual weights of the model. A larger value of $\lambda$ increases the regularization effect, encouraging smaller weights but potentially causing underfitting. Conversely, a smaller value of $\lambda$ reduces the penalty, allowing the network to learn more complex patterns, though with an increased risk of overfitting.

L2 regularization is commonly used in conjunction with other techniques like dropout and is often incorporated in hyperparameter tuning to strike the right balance between model complexity and generalization.

#### L1 Regularization

L1 regularization is similar to L2 regularization in that both techniques add a penalty term to the loss function to prevent overfitting and control the complexity of the model. However, L1 regularization differs from L2 in the type of penalty applied. While L2 regularization penalizes the sum of the squared weights, L1 regularization penalizes the sum of the absolute values of the weights.

The modified loss function for L1 regularization is:

$$
L_{\text{reg}} = L_{\text{original}} + \lambda \sum_i |w_i|
$${#eq-l1-reg}

where $L_{\text{original}}$ is the original loss function, $\lambda$ is the regularization parameter, and $w_i$ are the individual weights of the model. This L1 penalty encourages sparsity, meaning it forces some weights to become exactly zero, effectively removing less important features from the model.

While L2 regularization tends to shrink all weights uniformly, L1 regularization is better suited for feature selection, as it can zero out irrelevant features, simplifying the model. This property makes L1 regularization particularly useful in models where interpretability and feature selection are important. However, in practice, L2 regularization has broader applications because it generally leads to smoother models with smaller weights, which can be more effective in large-scale neural networks.

#### Batch Normalization

Batch Normalization is a technique used to improve the training of deep neural networks by normalizing the inputs to each layer, ensuring that the data fed into each layer has a stable distribution. During training, the input to a layer can shift and change, making the optimization process slower and more prone to issues like vanishing or exploding gradients. Batch normalization addresses this by normalizing the activations of each layer based on the mean and variance of the current mini-batch. This leads to faster convergence, improved stability, and allows the use of higher learning rates, thus accelerating the training process.

The normalization process is followed by scaling and shifting parameters to allow the network to learn back the original distribution if needed. Mathematically, for each mini-batch, batch normalization is expressed as:

$$
\hat{x}_i = \frac{x_i - \mu_{\text{batch}}}{\sqrt{\sigma_{\text{batch}}^2 + \epsilon}}
$$ {#eq-bn-1}

where $x_i$ is the input, $\mu_{\text{batch}}$ is the mean of the mini-batch, $\sigma_{\text{batch}}^2$ is the variance of the mini-batch, and $\epsilon$ is a small constant added for numerical stability. After normalization, the data is scaled and shifted using learned parameters $\gamma$ (scale) and $\beta$ (shift):

$$
y_i = \gamma \hat{x}_i + \beta
$$ {#eq-bn-2}

This scaling and shifting step allows the model to retain the capacity to represent complex patterns while benefiting from the stability of normalized inputs.

#### Weight Initialization

Proper weight initialization helps the network converge faster and avoids problems like vanishing or exploding gradients. Popular initialization methods include Xavier (Glorot) initialization [@glorot2010understanding], typically used for sigmoid or tanh activation functions, and He initialization [@he2015delving], which is effective for ReLU activation functions.

**Xavier Initialization**

Xavier initialization is designed to maintain the variance of activations throughout the layers of the network. It works well with activation functions like sigmoid and tanh, which are sensitive to large input values. The weights are initialized from a uniform distribution:

$$
W \in \mathcal{U} \left( -\sqrt{\frac{6}{N_{\text{in}} + N_{\text{out}}}}, \sqrt{\frac{6}{N_{\text{in}} + N_{\text{out}}}} \right)
$$ {#eq-gl}

where $N_{in}$ and $N_{out}$ are the number of input and output units in the layer, respectively. This ensures that the variance of the activations remains consistent across layers, preventing vanishing gradients.

**He Initialization**

He initialization, also known as Kaiming initialization, is specifically designed for layers with ReLU or PReLU activation functions, which discard half of the input values (those less than zero). It ensures that the variance of the activations remains controlled throughout the network. The weights are initialized from a normal distribution:

$$
W \in \mathcal{N} \left( 0, \sqrt{\frac{2}{N_{\text{in}}}} \right)
$$ {#eq-he}

where $N_{in}$ is the number of input units in the layer. This initialization is more suited to ReLU-based networks, where it compensates for the ReLU’s tendency to zero-out half of the input activations.

Choosing the Initialization

The choice of initialization method depends on the activation function used. He initialization is particularly effective for ReLU-based layers, while Xavier initialization is a better fit for sigmoid or tanh-based layers. Fine-tuning the initialization may not be necessary unless the network encounters convergence issues such as slow training or unstable gradients.

-   When using Batch Normalization, the bias can be initialized to 0.
-   The weights are correctly initialized by most deep learning frameworks (e.g., TensorFlow, PyTorch), but manual control can be applied if needed.

### Network Hyperparameters

In the field of neural networks, hyperparameters are settings that define the architecture and training process of the model. Unlike the model's parameters (such as weights and biases), hyperparameters are set before the learning process begins and are not learned from the data. Proper tuning of these hyperparameters is crucial for improving the performance of a neural network. Below are some common hyperparameters and the general approach to tuning them.

#### Learning Rate

The learning rate controls how much to adjust the model's weights with respect to the loss gradient during each update. A small learning rate may lead to slow convergence, whereas a large learning rate could cause the model to overshoot optimal weights, preventing convergence or leading to suboptimal results. Learning rate schedules or adaptive learning rate methods like Adam or RMSProp can help dynamically adjust this parameter during training.

For the tuning, a common approach is to begin with a standard value, such as 0.001 for Adam or 0.01 for stochastic gradient descent (SGD). Methods like grid search or random search are typically used to explore different values. Additionally, learning rate schedules (e.g., exponential decay, step decay) are often applied to improve training outcomes.

#### Batch Size

Batch size determines how many samples from the training data are used to calculate the gradient before updating the model's parameters. Smaller batch sizes often provide noisier updates but require less memory, making them suitable for limited hardware resources. Larger batch sizes lead to smoother gradient estimates but require more memory.

A common approach is to use batch sizes ranging from 16 to 512, with experimentation often conducted using powers of 2 (e.g., 16, 32, 64, etc.). Smaller batch sizes tend to aid generalization, while larger batches can accelerate the training process.

#### Number of Epochs

The number of epochs defines how many times the entire dataset is passed through the neural network during training. Too few epochs can result in underfitting, where the model fails to learn meaningful patterns from the data. Too many epochs can lead to overfitting, where the model memorizes the training data but fails to generalize to unseen data.

A common approach involves applying early stopping, which halts the training process when the performance on a validation set begins to degrade, thereby preventing overfitting. This method starts with a relatively large number of epochs and continuously monitors the model's performance on the validation set to determine the optimal point to stop training.

#### Dropout Rate

Dropout is a regularization technique that randomly drops neurons during training to prevent overfitting. The dropout rate controls the percentage of neurons to drop during each forward pass. High dropout rates can prevent the network from learning effectively, while low dropout rates might not provide enough regularization.

Dropout rates typically range from 0.2 to 0.5. Starting with a dropout rate of 0.3 and adjusting based on the model’s performance on the validation set is a common practice.

#### Number of Layers and Neurons

The architecture of a neural network is another important aspect of hyperparameter tuning. The number of layers (depth) and units per layer (width) determine the capacity of the model to learn from data. More layers and units increase the network’s capacity but also make it more prone to overfitting and computationally expensive to train.

It's common to start with a simple architecture and then gradually increase the depth and width of the network. Techniques like grid search or random search are typically used to explore different combinations, and regularization methods such as dropout or L2 regularization are often applied to prevent overfitting.

### Hyperparameter Tuning

Hyperparameter tuning is an essential step in developing neural networks that perform well on specific tasks. While no universal set of hyperparameters guarantees success, systematic experimentation and techniques like random search, grid search, or Bayesian optimization can greatly assist in finding the optimal configuration for a given task.

1.  **Grid Search**: It involves manually specifying a range of values for each hyperparameter and training a model for each combination. While thorough, it can be computationally expensive.

2.  **Random Search**: Instead of trying every combination, random search selects random combinations of hyperparameters. It often performs comparably to grid search but is computationally more efficient.

3.  **Bayesian Optimization**: This method builds a probabilistic model of the objective function and uses it to select hyperparameters in a way that balances exploration and exploitation. It is more sophisticated than grid or random search and can lead to better performance with fewer trials.

4.  **Automated Hyperparameter Tuning (e.g., Hyperopt, Optuna)**: Automated libraries can simplify the tuning process by intelligently searching for the optimal hyperparameters. These libraries often use strategies like Bayesian optimization, tree-structured Parzen estimators, or evolutionary algorithms to find optimal settings with minimal manual intervention.

## Neural Network Architectures

Neural networks are remarkably flexible, allowing various architectures and units to be connected and combined to create more complex models tailored to specific problems. For instance, by increasing or decreasing the number of layers and the number of neurons in each layer, the network's performance can be improved. Another crucial factor is the choice of activation functions, which play a significant role in the learning process. Additionally, specialized layers such as convolutional layers and recurrent layers can be added or removed depending on the data type, the complexity of the problem, and the specific requirements. These adjustments are made to achieve the most suitable architecture for the task at hand.

For instance, a Convolutional Neural Network (CNN) can be enhanced with conditional layers, forming a Conditional Convolutional Neural Network (CCNN) that generates different outputs based on additional context. This modularity adds significant power to the neural network, but it also introduces a considerable level of complexity. The goal is to design models that are as simple as possible while still providing adequate performance to solve the targeted problems effectively. A number of widely adopted neural network architectures will be covered in the following sections.

### Multi-Layer Perceptron

Multi-Layer Perceptron (MLP) is a type of feedforward artificial neural network that consists of multiple layers of neurons, where each layer is fully connected to the next one. The basic unit of an MLP is the perceptron, which only computes a weighted sum of its input features and passes the result through an activation function. By adding multiple layers to a perceptron, an MLP becomes capable of solving more complex problems. MLPs typically consist of an input layer, one or more hidden layers, and an output layer. The input layer receives the initial data, the hidden layers process the data through multiple transformations, and the output layer produces the final prediction or classification. MLPs are capable of approximating complex functions and are commonly used in tasks such as classification, regression, and pattern recognition. For an MLP with a single hidden layer:

$$
z^{(1)} = W^{(1)} {x} + b^{(1)}
$$ {#eq-mlp-1}

$$
a^{(1)} = \phi(z^{(1)})
$$ {#eq-mlp-2}

$$
z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}
$$ {#eq-mlp-3}

$$
\hat{y} = \phi(z^{(2)})
$$ {#eq-mlp-4}

Where:

$x$ is the input vector.

$W^{(1)}$ and $W^{(2)}$ are the weight matrices for the first and second layers, respectively.

$b^{(1)}$ and $b^{(2)}$ are the bias vectors for the first and second layers, respectively.

$\phi$ is the activation function (e.g., sigmoid, ReLU, etc.).

and $\hat{y}$ is the final output (prediction) of the MLP.

![An MLP with 2 hidden layers](img/Chp1/mlp.png){#fig-mlp fig-align="center" width="50%"}

### Autoencoders

An autoencoder is a type of neural network designed to learn a compressed, efficient representation of input data. It consists of two main components: an encoder and a decoder. The encoder compresses the input data into a lower-dimensional representation, often referred to as the latent space or bottleneck. The decoder then attempts to reconstruct the original input from this compressed representation. The goal is to minimize the difference between the input and the reconstructed output.

![an Autoencoder Architecture](img/Chp1/ae.png){#fig-ae fig-align="center" width="50%"}

Autoencoders, used for tasks like dimensionality reduction or feature learning, can also be improved by incorporating elements such as convolutional layers, recurrent units, or Generative Adversarial Networks (GANs). This integration allows autoencoders to learn more complex data representations, enhancing their effectiveness in applications like anomaly detection or data compression.

Mathematically, the process can be represented as: 
$$
z = f_\theta(x)
$$ {#eq-ae-1}

$$
\hat{x} = g_\phi({z})
$$ {#eq-ae-2}

Here, $z$ is the latent representation of the input $x$, and $\hat{x}$ is the reconstructed output. The objective of the autoencoder is to minimize the reconstruction loss, commonly measured as the mean squared error (MSE).

Autoencoders are particularly useful in areas such as dimensionality reduction and reconstruction, where they compress high-dimensional data into a lower-dimensional latent space and then accurately reconstruct the original data. They also excel in denoising by removing noise from corrupted data, and in anomaly detection by identifying outliers based on reconstruction errors. Additionally, autoencoders are valuable in generative modeling, contributing to the creation of new, similar data samples, which is especially beneficial in image synthesis and data augmentation.

### Variational Autoencoders

A Variational Autoencoder (VAE) is a generative model that extends the concept of autoencoders by incorporating probabilistic elements. Unlike traditional autoencoders, which learn deterministic mappings from input data to a latent space, VAEs aim to model the latent space as a probability distribution. This enables VAEs to not only reconstruct input data but also generate new samples from the learned distribution.

In a VAE, the encoder maps the input data to a distribution over the latent space, typically a Gaussian distribution with mean $\mu$ and variance $\sigma^2$. Instead of directly using the encoder's output, a latent vector $z$ is sampled from this distribution. The decoder then reconstructs the input data from the sampled latent vector. This sampling introduces variability and allows VAEs to generate new data by sampling from the latent space.

The VAE training objective consists of two components: the reconstruction loss (similar to a traditional autoencoder) and a regularization term that enforces the learned latent space to follow a prior distribution (commonly a standard normal distribution). This regularization is achieved through the Kullback-Leibler (KL) divergence, which measures how much the learned distribution deviates from the prior.

![Variational Autoencoder](img/Chp1/vae.png){#fig-conv fig-align="center" width="60%"}

The process is mathematically represented as:

$$
z \sim q_\phi(z|x) = \mathcal{N}(z|\mu(x), \sigma(x)^2)
$$ {#eq-vae-1}

$$
\hat{x} = g_\theta(z)
$$ {#eq-vae-2}

Here, $q_\phi(z|x)$ is the approximate posterior distribution generated by the encoder, and $\hat{x}$ is the reconstructed output. The objective of the VAE is to minimize the total loss, which consists of the reconstruction loss and the KL divergence:

$$
\mathcal{L}_{\text{VAE}} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{\text{KL}}(q_\phi(z|x) \| p(z))
$$ {#eq-vae-3}

-   The first term is the reconstruction loss, encouraging accurate reconstruction of input data.
-   The second term is the KL divergence, ensuring that the learned latent space distribution stays close to the prior distribution $p(z)$.

Variational Autoencoders are widely used in generative tasks such as image synthesis, data augmentation, and anomaly detection. By sampling from the learned latent space, VAEs can generate new data points that resemble the original dataset, making them a powerful tool for creating realistic data in various domains.

### Convolutional Neural Networks

Convolutional Neural Networks (CNNs) are a class of deep neural networks specifically designed to process data with a grid-like structure, such as images. Unlike fully connected networks, where each neuron is connected to every neuron in the previous layer, CNNs take advantage of the spatial structure of the data by using convolutional layers. These layers apply convolutional filters to local regions of the input, which can be mathematically represented as:

$$
y = f * x + b
$$ {#eq-cnn-1}

where $y$ is the output feature map, $f$ is the filter, $*$ denotes the convolution operation, $x$ is the input, and $b$ is the bias term. This operation allows the network to detect patterns such as edges, textures, and shapes in the input data.

![Example of a 2D convolution operation. The input matrix I is convolved with a kernel K to produce an output matrix O. The red-highlighted elements demonstrate how the center of the kernel is aligned with the center element of the input, and the convolution result (sum of element-wise multiplication) is calculated to give the value 2 in the output matrix.](img/Chp1/conv.png){#fig-conv fig-align="center" width="30%"}

A typical CNN architecture consists of multiple layers, including convolutional layers, pooling layers, and fully connected layers. Convolutional layers apply filters to the input to extract features, while pooling layers perform down-sampling, reducing the spatial dimensions of the data. A common pooling operation is max pooling, defined as:

$$
y_{i,j} = \max_{m,n} (x_{i+m,j+n})
$$ {#eq-max}

where $y_{i,j}$ represents the output after pooling, and $x_{i+m,j+n}$ is the region of the input over which the pooling operation is applied. The final fully connected layers combine the features extracted by the convolutional and pooling layers to make predictions, typically using a softmax function for classification:

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$ {#eq-softmax}

This equation represents the softmax function, where:

$z_i$ is the input to the softmax function for the $i$-th class. $K$ is the total number of classes.

The output of the softmax function is the probability distribution over $K$ classes, with each value $z_i$ transformed into a probability between 0 and 1. This ensures that the sum of all output probabilities equals 1, making it useful for multi-class classification.

![CNN Architecture for image classification](img/Chp1/CNN.png){#fig-cnn-layers fig-align="center" width="50%"}

CNNs are widely used in various applications, including image classification, object detection, and image segmentation. Their ability to automatically learn and extract features from raw pixel data has led to significant advancements in computer vision. The modularity of CNNs allows them to be easily adapted to different tasks by adjusting the architecture, such as changing the number of layers or the size of filters. This flexibility, combined with their high accuracy and efficiency, makes CNNs a cornerstone of modern deep learning applications.

### Conditional Neural Networks

Conditional Neural Networks are a specialized type of neural network where the output is conditioned not only on the input data but also on an additional context or condition. This conditioning can be represented as an additional input that influences the network's behavior, enabling it to generate different outputs depending on the given condition.

Consider a standard neural network where the input data is $x$ and the output is $y$. The network typically learns a function $f_\theta(x) = y$, where $\theta$ represents the parameters of the network. In a Conditional Neural Network, an additional condition $c$ is introduced, modifying the function to:

$$
f_{\theta}({x}, {c}) = {y}
$$ {#eq-cond}

Here, $c$ is the condition that influences the network's output. This condition could be a class label, a set of parameters, or any other contextual information relevant to the task.

If a neural network is designed to perform a specific task on a dataset, and we want to generalize that task to different categories of data, we can introduce a condition or context to the network. This allows the network to adapt its behavior based on the category or condition, making it more versatile. Moreover, this conditioning is not tied to a specific architecture; for example, a condition can be added to a convolutional network or any other architecture. For instance, in a conditional image generation task, $x$ might represent a latent vector, and $c$ could be a label corresponding to the class of the image to be generated. The network would then generate an image $y$ conditioned on both $x$ and $c$:

$$
{y} = G_{\theta}({x}, {c})
$$ {#eq-gen}

Where $G_\theta$ is the generator function of the Conditional Neural Network.

The loss function in a Conditional Neural Network often takes the condition into account as well. For example, in a supervised learning scenario, the loss function $L$ could be defined as:

$$
L_\theta = \frac{1}{N} \sum_{i=1}^{N} \ell\left(f_{\theta}({x}_i, {c}_i), {y}_i\right)
$$ {#eq-cond-gen}

Where $\ell$ is a suitable loss function (e.g., cross-entropy or mean squared error), $N$ is the number of training examples, and $y_i$ is the true output corresponding to input $x_i$ under condition $c_i$.

![Conditional Variational Autoencoder](img/Chp1/cvae.png){#fig-cvae fig-align="center" width="50%"}

Conditional Neural Networks are widely used in various applications, such as conditional image generation, style transfer, and structured prediction tasks. Their ability to incorporate context or conditions makes them highly versatile and effective for problems where the output must be tailored based on specific criteria. This flexibility allows conditions to be added to various architectures, whether it's a convolutional network or any other type, broadening their applicability.


## An Overview : 3D Deep Learning {#sec-dnn-3d-reps}

As we navigate a world inherently shaped by three dimensions, it's exciting to see how modern algorithms and computational tools are increasingly harnessing the full potential of 3D data. While early strides in artificial intelligence primarily focused on processing images, audio, and text, there has been a remarkable shift toward exploring the complexities of the 3D domain, opening new frontiers for innovation and discovery. According to @ahmed2018survey, 3D data processing using deep learning has become increasingly important in fields such as computer vision, robotics, and virtual reality, driving advancements in object recognition, spatial understanding, and immersive experiences. The way 3D data is represented plays a critical role in the design of models, the training process, and the final output. Depending on how the data is structured and represented, 3D deep learning can be categorized into several approaches. The most common representations include point clouds, meshes, and multi-view images, which are considered explicit forms of 3D data. Each of these representations has its unique challenges and benefits, influencing the choice of network architecture and the methods used for training.

In addition to explicit representations, implicit representations have gained significant attention in recent years. Unlike explicit forms, where the 3D data is directly stored and processed, implicit representations model the 3D structure in a more abstract way, such as through occupancy fields or signed distance functions. These newer approaches allow for more flexible and continuous representation of 3D shapes, often leading to better generalization and smoother reconstructions. Due to their efficiency and ability to handle complex geometries, implicit representations are becoming increasingly popular in state-of-the-art 3D deep learning applications.

### NN and Explicit representations : Point Clouds

A point cloud is a collection of data points defined in a three-dimensional coordinate system. Each point represents a specific location in 3D space, often capturing the surface of an object or environment. Point clouds are widely used in various applications, including 3D modeling, computer graphics, and autonomous driving, as they offer a straightforward and efficient way to represent 3D shapes. Due to their simplicity and lightweight nature, point clouds are particularly well-suited for neural network processing, making them a popular choice for tasks such as classification and segmentation.

Point clouds have strong compatibility with neural networks, especially those designed for 3D data. Unlike other data types, such as meshes or volumetric grids, point clouds do not require complex preprocessing and can be directly fed into neural networks like PointNet or PointNet++. These networks are specifically designed to handle the unordered and irregular nature of point clouds, enabling effective learning and accurate predictions. However, a significant limitation of point clouds is their lack of detailed surface information. While they provide a basic representation of shape, they do not inherently capture the intricate surface geometry of objects, which can be a drawback in applications where precise surface details are critical. Despite this, the simplicity and efficiency of point clouds make them a powerful tool in 3D deep learning.

#### PointNet

PointNet is a pioneering neural network architecture specifically designed for processing point cloud data. Unlike traditional methods that rely on structured data representations like grids or meshes, PointNet directly consumes raw point clouds, making it highly efficient and versatile for tasks such as 3D object classification, segmentation, and scene understanding. The core innovation of PointNet is its use of a symmetric function, such as max-pooling, applied across all points to achieve permutation invariance, ensuring that the network's output remains consistent regardless of the order in which the points are presented. This design enables PointNet to effectively capture global features of the point cloud while also incorporating local point features through a series of multi-layer perceptron (MLP) layers.

However, despite its innovative approach, PointNet has certain limitations. One of the key drawbacks is its inability to capture fine-grained local structures and relationships between points, as it primarily focuses on global feature aggregation. This makes it less suitable for representing 3D shape deformations, where understanding local geometric variations is crucial. Additionally, PointNet's reliance on a global max-pooling operation can lead to the loss of important spatial information, which is essential for tasks that require detailed shape analysis. As a result, while PointNet is highly effective for tasks like classification and basic segmentation, it may not be the best choice for applications that require a nuanced understanding of 3D shape deformations or other complex geometric transformations.

#### PointNet++

PointNet++ is an extension of the original PointNet architecture designed to address some of its limitations, particularly in capturing local geometric details. While PointNet excels at processing global features of point clouds, it struggles with recognizing fine-grained structures and the spatial relationships between nearby points. PointNet++ enhances this capability by introducing a hierarchical structure that processes the point cloud in a nested manner, similar to how CNNs work on images by capturing local features at various scales. In PointNet++, the point cloud is divided into smaller neighborhoods or regions, and the PointNet architecture is applied recursively within these regions to learn local features. These local features are then aggregated hierarchically to form a more comprehensive understanding of the overall shape.

Despite these improvements, PointNet++ still faces challenges when dealing with 3D shape deformations. Although it can capture local features more effectively than its predecessor, the architecture is primarily designed for static or rigid objects and is less suited for tasks that require understanding complex deformations or non-rigid transformations. The hierarchical structure, while powerful, does not inherently model the continuous and flexible nature of deformations, making it less ideal for applications where capturing subtle changes in shape is crucial. Thus, while PointNet++ represents a significant advancement over the original PointNet, it is still not recommended for scenarios that involve significant 3D shape deformations.

### NN and Explicit representations : Mesh

Meshes are one of the most widely used explicit representations in 3D deep learning, particularly in fields like computer graphics, medical imaging, and 3D modeling. Already explained in the thesis Introduction, this representation is highly expressive, capturing intricate details of an object's surface geometry and topology. Unlike more regular data structures such as images or point clouds, meshes have a non-Euclidean structure, presenting unique challenges for neural networks that need to process this type of data. To address these challenges, specialized neural network architectures like graph neural networks (GNNs) or convolutional neural networks adapted for non-Euclidean spaces (e.g., mesh convolutional networks) are often used. These networks are designed to leverage the connectivity information inherent in meshes, allowing them to learn and extract meaningful features directly from the mesh's topology. Therefore, these GNNs are highly effective for data with a graph-like structure, where connections between verices are crucial, such as social network or molecular graphs, where understanding complex relational patterns is crucial. However, when the mesh geometry and the positions of the vertices themselves become important, these networks face significant challenges. Examples of networks that have addressed this issue in geometric deep learning for classification, under specific constraints, include MeshCNN, SplineCNN, and CoMA @ranjan2018generating.

\[add images of mesh, face, vertex or edge, node \]

#### Convoutional Mesh AutoEncoder (CoMA)

The Convolutional Mesh AutoEncoder (CoMA), introduced in 2017, is a neural network architecture specifically designed to process 3D mesh data.

Unlike traditional autoencoders, which are inherently designed for structured, grid-like data such as images, CoMA is specifically engineered to tackle the complexities of irregular 3D mesh structures. Traditional convolutional neural networks (CNNs) excel in processing data arranged in regular grids, where each pixel or voxel is neatly aligned with its neighbors. However, 3D meshes are fundamentally different—they consist of vertices connected by edges forming irregular polygons, typically triangles, that define the surface of a shape. These irregularities present significant challenges for standard CNNs, which rely on the spatial consistency of grid data to apply convolutional operations effectively.

CoMA overcomes these challenges by extending convolutional operations from regular grids to graph structures, where the mesh vertices and edges form a graph. This adaptation is made possible through spectral graph convolutional layers, which operate in the frequency domain, allowing the network to process the mesh’s geometry in a way that respects its inherent irregularity. These layers perform convolutions not in the traditional spatial sense, but by filtering the mesh’s geometric features across its graph-based structure. This allows CoMA to capture both local and global geometric information, which is crucial for accurately representing complex 3D shapes.

The architecture of CoMA is built around a symmetric encoder-decoder design, where both the encoder and decoder consist of four layers. The encoder’s role is to compress the high-dimensional mesh data into a lower-dimensional latent space. This compression is achieved through the spectral graph convolutional layers, which progressively reduce the resolution of the mesh while preserving its most significant geometric features. The latent space effectively captures the high-level, abstract representation of the 3D shape, distilling its most important characteristics into a compact form.

Once the mesh data is compressed into this latent space, the decoder takes over, reconstructing the original high-resolution mesh from the compact representation. The decoder mirrors the encoder’s structure, using upsampling operations and inverse convolutions to progressively restore the mesh’s resolution. This reconstruction process allows CoMA to generate a detailed and accurate representation of the original 3D shape, ensuring that essential geometric properties are preserved throughout the process. The combination of these advanced techniques enables CoMA to learn and manipulate complex 3D shapes efficiently, making it a powerful tool for tasks like shape reconstruction, deformation transfer, and facial expression synthesis.

A key innovation of CoMA is its ability to handle the irregular topology of meshes through spectral graph convolutions, which operate in the frequency domain. By leveraging Chebyshev polynomials and fast localized convolutions, the model efficiently processes mesh data. However, it is important to note that all mesh samples in the dataset must share the same topology, and a model trained on one dataset is not easily extendable to others. Additionally, the input mesh must exhibit properties such as regularity, uniform connectivity, and also a consistent hierarchical structure to support both downsampling and upsampling operations. Without these properties, operations like pooling and unpooling can become problematic, potentially necessitating remeshing to create a dataset suitable for CoMA. Furthermore, the network’s first - and last - layers must have a size of 3 times the number of vertices to represent XYZ positions, which can result in a large model when dealing with high-resolution meshes. Therefore, while CoMA is powerful for tasks like 3D shape reconstruction, facial expression synthesis, and deformation transfer, it does have specific requirements and limitations regarding the structure and properties of the input meshes.

#### SplineCNN

@fey2018splinecnn SplineCNN is a type of convolutional neural network designed to operate on non-Euclidean domains, such as graphs and meshes, where data is irregular and connectivity information is crucial. Unlike traditional CNNs that use fixed rectangular kernels, SplineCNN employs learnable B-spline kernels that can adapt to the underlying structure of the data, allowing the network to perform convolutions directly on the graph or mesh. This flexibility enables SplineCNN to effectively capture both the local geometric structure and the connectivity patterns, making it well-suited for tasks like 3D shape analysis and graph-based classification.

![](img/Chp1/splineCNN.png){fig-align="center" width="50%"}

SplineCNN has demonstrated its effectiveness by improving state-of-the-art results in several benchmark tasks, including image graph classification, graph node classification, and shape correspondence on meshes. For the task of shape correspondence, SplineCNN was validated on a collection of 3D meshes, solving the challenge of matching each node of a given shape to the corresponding node of a reference shape. However, the dataset used in this experiment had a significant limitation: all meshes shared the same topology, requiring that the mesh sizes and node orders remain consistent across the entire dataset, which is a considerable constraint.

#### MeshCNN

@hanocka2019meshcnn

MeshCNN is a specialized neural network architecture designed to process 3D mesh data by adapting convolutional operations to the irregular, non-Euclidean structure of meshes. Unlike traditional CNNs that operate on grid-like data, MeshCNN applies convolutions directly on the edges of a mesh, enabling the network to capture the geometric and topological features inherent in 3D shapes. The architecture employs edge-based convolutions and pooling operations to reduce the mesh's complexity while preserving its essential properties, making it particularly effective for tasks such as 3D shape classification and segmentation. This approach is well-suited for scenarios where understanding the detailed geometry and topology of 3D objects is crucial, such as distinguishing between different types of 3D models or segmenting parts of a 3D object.

However, MeshCNN also introduces certain complexities and limitations. The input dimensionality is defined by the number of features per edge multiplied by the total number of edges, which can lead to increased computational demands, particularly for large meshes. Additionally, the architecture requires a consistent and well-defined mesh structure, often necessitating preprocessing steps like remeshing or simplification to ensure compatibility. This dependence on specific mesh topologies can limit the network's generalization to different types of meshes, and the computational load may challenge scalability in scenarios with varying mesh structures or limited resources.

#### MeshNet

@feng2019meshnet

Designed for tasks like 3D shape classification and retrieval, MeshNet effectively addresses the challenges posed by the complexity and irregularity of mesh data. The architecture incorporates face-unit and feature-splitting techniques, along with specialized blocks for capturing spatial and structural features of the mesh, allowing the network to learn and aggregate information from mesh data efficiently. This makes MeshNet particularly effective for tasks such as 3D shape classification and retrieval.

However, MeshNet also has its drawbacks and limitations. One significant challenge is the need for consistent and high-quality mesh data, as the network's performance heavily depends on the structure and regularity of the input meshes. Additionally, processing complex or high-resolution meshes can be computationally expensive, potentially limiting the scalability of the model in larger or more detailed datasets. Despite these challenges, MeshNet has shown promising results in applications requiring precise 3D shape analysis, such as in the classification and retrieval of 3D models, demonstrating its potential in the field of computer vision and graphics.

### NN and Multi-view 2D images

Multi-view approaches combined with neural networks for 3D reconstruction involve capturing multiple images or views of an object from different angles and using neural network models to integrate this information into a detailed 3D model. These methods leverage the complementary information provided by different views about an object’s shape, texture, and depth. By aligning and combining these views, neural networks can infer the 3D structure of the object with greater accuracy and detail. Examples include multi-view learning approaches, photogrammetry enhanced by neural networks, and advanced methods like Neural Radiance Fields (NeRFs).

Despite their effectiveness, multi-view approaches that incorporate neural networks also present certain challenges. Ensuring precise alignment and calibration between views remains crucial, as even minor errors can result in significant inaccuracies in the 3D reconstruction. These methods can be computationally demanding, especially when processing high-resolution images or complex scenes, requiring substantial computational resources. Additionally, multi-view neural network techniques may struggle with objects that have complex geometries or surfaces that are difficult to capture from certain angles, leading to incomplete or less detailed reconstructions. Consequently, while these multi-view neural network approaches are powerful tools for 3D reconstruction, they may not be suitable for all scenarios, particularly those involving dynamic changes or where real-time processing is required.

#### Multi-view learning

@zhao2017multi Multi-view learning is a machine learning approach that uses multiple representations or views of the same data to improve model performance by capturing richer and more diverse features than a single view could offer. This method is particularly effective in scenarios like image recognition, text analysis, and bioinformatics, where data is naturally multi-modal. However, multi-view learning faces challenges such as integrating heterogeneous views, managing computational complexity, and dealing with incomplete data. With the emergence of newer techniques like NeRF (Neural Radiance Fields) and photogrammetry, which utilize more advanced methods for 3D reconstruction, the prominence of multi-view learning approaches has gradually diminished. These newer methods are capable of extracting and integrating information from multiple views with greater accuracy and efficiency, making them more suitable replacements for traditional multi-view learning in many applications.

#### Photogrammetry

Enhanced photogrammetry with neural networks builds on the traditional photogrammetry @kraus2011photogrammetry technique, which reconstructs 3D models from a series of 2D images taken from different angles. In traditional photogrammetry, geometric principles such as triangulation are used to align and integrate these images, producing a detailed 3D model. However, this process can be limited by the quality of feature matching, image alignment, and the ability to handle complex or noisy datasets. Neural networks have been introduced to overcome some of these limitations by improving various stages of the photogrammetry pipeline, such as feature detection, depth estimation, and image matching.

Neural networks enhance photogrammetry by making the reconstruction process more robust and accurate. For example, deep learning models can better handle variations in lighting, texture, and occlusion, which often pose challenges in traditional methods. They can also improve the efficiency of processing large datasets by automating parts of the reconstruction process that would otherwise require manual intervention or complex algorithms. As a result, neural network-enhanced photogrammetry can produce more precise 3D models, especially in challenging scenarios where traditional methods might struggle. This makes it a valuable tool in fields such as archaeology, architecture, and geospatial analysis, where high-quality 3D reconstructions are critical. However, these techniques are primarily suited for environmental reconstructions and are not as effective for detailed reconstruction of individual 3D objects.

#### Neural Radiance Fields (NeRFs)

NeRFs are a cutting-edge approach to 3D scene representation and reconstruction using neural networks. Introduced in 2020, NeRFs model the volumetric scene by encoding the radiance (color) and density at each point in a 3D space, which is then used to render images from any viewpoint. The core idea is to use a neural network to learn a continuous volumetric scene function that takes as input a 3D coordinate (along with a viewing direction) and outputs the color and density at that point. This allows NeRFs to synthesize highly realistic images of a scene from novel viewpoints, even when only a sparse set of 2D images are provided during training.

NeRFs have demonstrated remarkable results in tasks like view synthesis, where they can generate photorealistic images of complex scenes with fine details, including soft shadows and reflections. However, NeRFs also have some limitations. Training a NeRF model can be computationally intensive, often requiring powerful GPUs and significant processing time, as the network must learn the entire scene's radiance field from the input images. Additionally, while NeRFs excel at capturing static scenes, they struggle with dynamic scenes or objects with complex deformations, making them less suitable for real-time applications or scenarios involving moving subjects. Despite these challenges, NeRFs represent a significant advancement in the field of 3D reconstruction and have opened up new possibilities for high-quality, neural-based rendering in various applications, from virtual reality to filmmaking.

### NN and Implicit representations

Implicit representations describe 3D shapes or objects using mathematical functions that define the shape implicitly, rather than explicitly storing surface geometry like vertices, edges, or faces. Instead of representing a shape as a collection of discrete elements, implicit representations use continuous functions to define the geometry. Neural networks are particularly well-suited for modeling these representations as they function effectively as estimators of complex, non-linear functions. A trained neural network can embed the shape information within its structure, allowing it to learn functions that represent shape details implicitly without the need for explicit geometric data. This approach results in more compact and flexible structures capable of accurately modeling intricate geometries, offering a smoother and more continuous alternative to traditional explicit representations. Two of the most prominent and widely adopted methods in recent years are signed distance functions (SDFs) and occupancy fields, which have gained significant attention for their effectiveness in representing 3D shapes implicitly.

#### Occupancy Fields

Occupancy fields are a mathematical framework used to implicitly represent 3D shapes by defining a continuous function $f(\mathbf{x})$ that maps points $\mathbf{x} = (x, y, z)$ in a three-dimensional space to a scalar value, typically within the range \[0, 1\]. This scalar value indicates the probability or likelihood that a given point lies inside the shape or object. Specifically, points with values close to 1 are considered to be within the object's volume, while those with values near 0 are outside. Unlike explicit representations that store geometry using discrete elements like vertices, edges, and faces, occupancy fields offer a more continuous and fluid description of 3D geometry. This allows for the representation of complex shapes without the need for dense, memory-intensive data structures, making them particularly effective for capturing intricate details in a more compact form.

In scientific and computational applications, occupancy fields are often employed in conjunction with neural networks, which excel at learning and approximating continuous functions from data. A neural network can be trained to approximate the occupancy function $f(\mathbf{x})$ for a given set of shapes, effectively encoding the geometry of these shapes within its learned parameters. Once trained, the network can quickly evaluate the occupancy of any point in space, enabling rapid and accurate reconstructions of 3D objects. This method provides significant advantages over traditional explicit representations, particularly in terms of memory efficiency and the ability to smoothly interpolate and generate new shapes. The continuous nature of occupancy fields also lends itself well to tasks such as 3D object reconstruction, shape completion, and scene understanding, where capturing the fine details and complex structures of objects is crucial.

#### DeepSDF

@park2019deepsdf DeepSDF is a powerful technique that leverages neural networks to learn implicit representations of 3D shapes through Signed Distance Functions (SDFs). The core idea is to train a deep neural network to approximate the SDF of a target shape, which describes the distance from any point in space to the surface of the shape. The SDF function is signed, meaning that it returns positive values for points outside the shape, negative values for points inside, and zero at the surface.

In the DeepSDF approach, the network is trained on a dataset $X$ composed of pairs of 3D points $x$ and their corresponding SDF values $s$. Mathematically, this can be represented as:

$$
X := \{(x, s) : \text{SDF}(x) = s\}
$$

The goal is to train the parameters $\theta$ of a multi-layer fully-connected neural network $f_\theta$ so that it becomes a good approximator of the SDF function across the target domain $\Omega$. The training process involves minimizing the difference between the network's output and the actual SDF values for the sampled points. This relationship is expressed as:

$$
f_\theta(x) \approx \text{SDF}(x), \forall x \in \Omega
$$

Once trained, the neural network can efficiently predict the SDF for any point in space, allowing it to reconstruct or generate highly detailed 3D shapes based on the learned implicit representation. This method is particularly useful for tasks requiring smooth and continuous shape representations, such as 3D modeling, shape interpolation, and generation.

#### Converting implicit fields to 3D Meshes

In the end, implicit fields need to be converted into meshes because this transformation is essential for visualizing, simulating, and processing the shape in a tangible, usable form. Converting implicit fields into 3D meshes involves extracting a clear surface representation from the volumetric data. Methods like the Marching Cubes algorithm are widely used for this purpose, where the 3D space is divided into small cubes, and the surface is triangulated based on the values at the corners of these cubes. Other techniques, such as Dual Contouring or surface nets, offer alternatives for surface extraction, sometimes providing better handling of sharp features or producing more efficient meshes. These methods are essential in transforming the implicit definition of shapes into explicit 3D models for visualization, simulation, and further processing.

##### Marching Cubes

The Marching Cubes algorithm is a widely used method for converting implicit fields, such as Signed Distance Functions (SDF) or occupancy fields, into explicit 3D meshes. These implicit fields define a shape indirectly by specifying whether a point in space is inside or outside the shape (occupancy fields) or by providing the shortest distance from any point to the surface of the shape (SDF). The challenge is to extract a clear, explicit surface from this data, which is where Marching Cubes comes in.

The algorithm works by dividing the 3D space into a grid of small cubes, where each corner of a cube corresponds to a sample point in the implicit field. By evaluating the field at each corner, the algorithm determines whether that corner is inside or outside the shape. Based on the combination of these inside/outside states at the eight corners of a cube, the algorithm references a precomputed lookup table to decide how to triangulate the surface that passes through the cube. The result is a collection of triangles that approximate the surface of the shape, effectively converting the implicit field into an explicit mesh.

Marching Cubes is particularly popular due to its efficiency and simplicity, as well as its ability to handle complex shapes and produce smooth surfaces. However, while it is effective, it can sometimes produce meshes with a high number of triangles, especially in areas of high curvature, which may require further optimization or simplification depending on the application. Nonetheless, it remains a fundamental tool in computer graphics, medical imaging, and 3D modeling for converting implicit representations into usable 3D models.

##### Dual Contouring

Dual Contouring is an algorithm used for extracting a surface mesh from an implicit field, such as a Signed Distance Function (SDF) or an occupancy field. Unlike the Marching Cubes algorithm, which operates directly on the corners of grid cells to produce triangles, Dual Contouring focuses on the edges of the grid cells and generates vertices directly on the surface of the shape being represented. This approach often results in better preservation of sharp features like edges and corners, which can be challenging for Marching Cubes to capture accurately.

The algorithm works by first identifying edges of grid cells where the implicit field changes sign, indicating that the surface passes through that edge. For each such edge, a vertex is placed at the point on the edge that is closest to the actual surface, usually computed by minimizing a quadric error function derived from the field's gradient. The vertices created this way are then connected to form polygons, typically quads, that approximate the surface.

One of the key advantages of Dual Contouring is its ability to generate meshes that accurately capture sharp features without the need for excessive subdivision, leading to more efficient meshes with fewer polygons. This makes it particularly useful in applications where precise geometry is important, such as in computer-aided design (CAD) and high-quality 3D modeling. Additionally, Dual Contouring can be extended to handle adaptive octrees, allowing it to efficiently manage complex, highly detailed surfaces.

##### Surface Nets

Surface Nets is another algorithm used for extracting a surface mesh from an implicit field. The method can be seen as a middle ground between algorithms like Marching Cubes and Dual Contouring, aiming to create a more straightforward, yet effective, way of generating a 3D mesh that captures the surface of a shape.

In Surface Nets, the 3D space is divided into a grid, similar to Marching Cubes. However, instead of focusing on the grid cell corners or edges, the algorithm places a single vertex within each grid cell that contains part of the surface (where the implicit function changes sign). The position of this vertex is determined by averaging the positions where the surface intersects the edges of the grid cell, effectively creating a "net" of vertices that approximates the surface.

Once these vertices are placed, they are connected to form a mesh of quads or triangles, representing the surface of the shape. Surface Nets have the advantage of producing smoother surfaces with fewer vertices compared to Marching Cubes, especially in areas where the surface is relatively flat. Additionally, because it doesn't require complex computations to preserve sharp features, Surface Nets is simpler and faster than methods like Dual Contouring.

However, Surface Nets might not capture sharp features as accurately as Dual Contouring, making it more suitable for applications where smooth surfaces are more critical than precise feature preservation. Its simplicity and efficiency make it a good choice for applications like real-time rendering or scenarios where computational resources are limited.

## Reinforcement Learning

Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative rewards. Unlike supervised learning, where the model learns from labeled data, RL relies on the trial-and-error approach, continuously interacting with the environment to improve its policy.

Key Concepts in Reinforcement Learning:

**Agent**: The learner or decision maker.

**Environment**: Everything the agent interacts with.

**State** (s): A representation of the current situation of the agent in the environment.

**Action** (a): All possible moves the agent can take.

**Reward** (r): Feedback from the environment based on the action taken by the agent.

**Policy** (π): A strategy that defines the agent's action selection, mapping states to actions.

**Value Function** (V): Estimates the expected reward for an agent starting from a state and following a policy.

**Q-Value** (Q): Estimates the expected reward for taking an action in a state and following a policy thereafter.

The RL process begins with the agent starting with an initial policy, which is often random. The agent then interacts with the environment by taking actions based on this policy. After each action, the agent receives feedback in the form of a reward and observes the new state. Using this feedback, the agent updates its policy, aiming to maximize future rewards. This process repeats iteratively, with the policy improving over time as the agent gains more experience.

Reinforcement Learning can be categorized into model-based and model-free approaches. Model-based RL involves learning a model of the environment's dynamics to plan and make decisions, while model-free RL relies solely on learning from interactions with the environment without an explicit model. Several algorithms help in learning the optimal policy in these approaches. Q-Learning is a value-based method where the agent learns the Q-values for state-action pairs and uses them to make decisions. Deep Q-Networks (DQN) combine Q-learning with deep neural networks to handle high-dimensional state spaces. Policy Gradient Methods directly optimize the policy by adjusting it in the direction that increases expected rewards. Actor-Critic Methods combine value-based and policy-based methods, with an actor making decisions and a critic evaluating them.

### World Models in Reinforcement Learning

World models are an advanced concept in reinforcement learning that involve creating an internal model of the environment. This model allows the agent to predict future states and outcomes based on its current state and actions, effectively simulating the environment internally. By leveraging world models, agents can plan and make decisions more efficiently, even in complex and high-dimensional spaces.

World models offer several advantages in reinforcement learning. By simulating the environment internally, agents can learn and plan more efficiently, reducing the need for extensive real-world interactions. This internal simulation enables better exploration strategies, allowing agents to evaluate the consequences of exploratory actions without performing them in the real environment. World models are particularly useful in handling complex environments with high-dimensional state spaces or intricate dynamics, where model-free methods might struggle.

Applications of world models span various domains, including robotics, where they are used for planning and control tasks that require an understanding of the environment's dynamics; autonomous driving, where they help predict and react to the behavior of other vehicles and pedestrians; and game playing, where they enhance performance in complex strategy games by simulating future game states and planning accordingly.

The workflow of a world model-based agent involves data collection through interactions with the environment, model learning to develop perception, transition, and reward models, state inference using the perception module, planning by simulating future states and rewards, and action execution in the real environment. By integrating world models into reinforcement learning, agents achieve more robust and intelligent behavior, making them well-suited for complex and dynamic tasks.

![image caption](img/Chp3/Planet.png){#fig-planet fig-align="center" width="50%"}

### PlaNet: Learning Latent Dynamics for Planning from Pixels

The paper "Learning Latent Dynamics for Planning from Pixels" presents a novel approach to model-based reinforcement learning that addresses the challenge of planning in environments with high-dimensional observations, such as images. The authors introduce the Deep Planning Network (PlaNet), an agent that learns a world model from image inputs and leverages it for planning. This method combines the benefits of model-free and model-based approaches, allowing for data-efficient learning and effective planning in complex environments.


At the core of PlaNet is a latent dynamics model that consists of several components: (1) a representation model that encodes observations into a compact latent state, (2) a transition model that predicts the next latent state given the current state and action, (3) a reward model that predicts the immediate reward, and (4) an observation model that reconstructs the image observation from the latent state. These components are trained jointly using variational inference, maximizing a lower bound on the log-likelihood of the observed data.

The planning process in PlaNet is performed entirely in the learned latent space, which is significantly more compact than the original image space. The agent uses the Cross-Entropy Method (CEM) to optimize action sequences that maximize predicted rewards over a finite horizon. This planning process is repeated at each time step in a receding horizon fashion, allowing the agent to adapt its plan based on new observations. The authors demonstrate that PlaNet outperforms both model-free and model-based baselines on several challenging continuous control tasks, achieving high performance from pixels with remarkably sample efficiency.

The significance of this work lies in its ability to learn effective world models from high-dimensional inputs without any environment-specific inductive biases. This makes PlaNet applicable to a wide range of tasks without requiring task-specific engineering. Moreover, the latent space planning approach enables the agent to reason about future states and rewards in a computationally efficient manner, overcoming the limitations of planning directly in image space. The paper's results suggest that this approach could be a promising direction for scaling reinforcement learning to more complex, real-world tasks where sample efficiency and the ability to plan from raw sensory inputs are crucial.

**Perception Module**

The perception module processes raw sensory inputs from the environment and transforms them into more compact and useful representations. This typically involves using neural networks such as autoencoders. By efficiently encoding sensory information, the perception module helps the agent understand and navigate its environment more effectively.

**Transition Model**

The transition model, also known as the dynamics model, predicts the next state given the current state and action. It captures the temporal dynamics of the environment and can be implemented using various types of neural networks, including state space models. This model allows the agent to simulate future states and understand how its actions affect the environment over time.

**Reward Model**

The reward model predicts the expected reward for a given state-action pair. By understanding the reward structure of the environment, the agent can better evaluate the potential benefits of different actions. This helps the agent to make decisions that maximize long-term rewards, guiding it towards achieving its goals more effectively.

**Planning Module**

With the internal model of the environment, the agent can simulate different action sequences to plan its moves. This planning is often done using model-based algorithms such as Monte Carlo Tree Search (MCTS), trajectory optimization methods, or the Cross Entropy Method (CEM). MCTS explores possible future states by building a search tree, trajectory optimization refines action sequences to maximize rewards, and CEM generates and evaluates a population of action sequences, iteratively refining them based on the best performers. By utilizing these methods, the agent can effectively predict future states and rewards, allowing it to choose actions that optimize its long-term success.

![image caption](img/Chp3/CEM_1.png){#fig-cem fig-align="center" width="50%"}

**Caption:** Formulas representing key components of world models in reinforcement learning, including

the perception module: 
$$ 
z = f_\text{perception}(x)
$$

the transition model : 
$$ 
s_{t+1} = f_\text{transition}(s_t, a_t)
$$

the reward model : 
$$ 
r_t = f_\text{reward}(s_t, a_t)
$$
and the planning module : 
$$
\text{Best Action Sequence} = \arg\max_{a_1, a_2, \ldots, a_T} \sum_{t=1}^{T} f_\text{reward}(s_t, a_t)
$$
