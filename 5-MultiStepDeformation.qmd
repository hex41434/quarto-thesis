# Multi Step Deformation {#sec-msd}

## Multi Step Deformation

This chapter explores the multi-step deformation (MSD) process of a cuboid that is fixed at the bottom while sequential force vectors are applied to the top. The objective is to achieve a desired shape through a series of deformations. Here, I used Reinforcement Learning (RL) because it excels in sequential decision-making and learning optimal strategies through trial and error, making it ideal for optimizing the deformation sequence to achieve precise outcomes. World Models, such as PlaNet or Dreamer, are utilized because they effectively learn compact representations of the environment dynamics, enabling efficient planning and control. By combining RL with these World Models, we can accurately predict deformation outcomes and optimize the sequence of applied forces, ensuring a smooth and continuous transformation of the initial cuboid into the desired shape.

![deformed mesh in 5 steps](img/Chp3/mesh_states.png){#fig-mesh-states fig-align="center" width="40%"}

As we need to keep track of the state changes, the predictor NN should contain the state information.

The Cross Entropy Method (CEM) is employed to identify the optimal sequence of force vectors (actions) through several key steps. At first, a distribution over possible force vectors is initialized. From this distribution, a set of action sequences is sampled. Each sequence is then evaluated by simulating the deformation process using the neural network, with performance measured by comparing the deformed shape to the desired shape. Based on these evaluations, the distribution is updated to focus on better-performing sequences, increasing the likelihood of sampling optimal sequences in subsequent iterations. This iterative process continues until the deformation sequence converges to an optimal solution.

The optimization continues until a convergence criterion is met. The criterion is defined based on the similarity between the deformed and the target shape. Once convergence is achieved, the final sequence of force vectors represents the optimal strategy for achieving the desired deformation. First we tried 2D approach and then continued with both mesh and implicit representation of the 3D and compare the methods.

## Reinforcement Learning

Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative rewards. Unlike supervised learning, where the model learns from labeled data, RL relies on the trial-and-error approach, continuously interacting with the environment to improve its policy.

Key Concepts in Reinforcement Learning:

**Agent**: The learner or decision maker.

**Environment**: Everything the agent interacts with.

**State** (s): A representation of the current situation of the agent in the environment.

**Action** (a): All possible moves the agent can take.

**Reward** (r): Feedback from the environment based on the action taken by the agent.

**Policy** (Ï€): A strategy that defines the agent's action selection, mapping states to actions.

**Value Function** (V): Estimates the expected reward for an agent starting from a state and following a policy.

**Q-Value** (Q): Estimates the expected reward for taking an action in a state and following a policy thereafter.

The RL process begins with the agent starting with an initial policy, which is often random. The agent then interacts with the environment by taking actions based on this policy. After each action, the agent receives feedback in the form of a reward and observes the new state. Using this feedback, the agent updates its policy, aiming to maximize future rewards. This process repeats iteratively, with the policy improving over time as the agent gains more experience.

Reinforcement Learning can be categorized into model-based and model-free approaches. Model-based RL involves learning a model of the environment's dynamics to plan and make decisions, while model-free RL relies solely on learning from interactions with the environment without an explicit model. Several algorithms help in learning the optimal policy in these approaches. Q-Learning is a value-based method where the agent learns the Q-values for state-action pairs and uses them to make decisions. Deep Q-Networks (DQN) combine Q-learning with deep neural networks to handle high-dimensional state spaces. Policy Gradient Methods directly optimize the policy by adjusting it in the direction that increases expected rewards. Actor-Critic Methods combine value-based and policy-based methods, with an actor making decisions and a critic evaluating them.

### World Models in Reinforcement Learning

World models are an advanced concept in reinforcement learning that involve creating an internal model of the environment. This model allows the agent to predict future states and outcomes based on its current state and actions, effectively simulating the environment internally. By leveraging world models, agents can plan and make decisions more efficiently, even in complex and high-dimensional spaces.

World models offer several advantages in reinforcement learning. By simulating the environment internally, agents can learn and plan more efficiently, reducing the need for extensive real-world interactions. This internal simulation enables better exploration strategies, allowing agents to evaluate the consequences of exploratory actions without performing them in the real environment. World models are particularly useful in handling complex environments with high-dimensional state spaces or intricate dynamics, where model-free methods might struggle.

Applications of world models span various domains, including robotics, where they are used for planning and control tasks that require an understanding of the environment's dynamics; autonomous driving, where they help predict and react to the behavior of other vehicles and pedestrians; and game playing, where they enhance performance in complex strategy games by simulating future game states and planning accordingly.

The workflow of a world model-based agent involves data collection through interactions with the environment, model learning to develop perception, transition, and reward models, state inference using the perception module, planning by simulating future states and rewards, and action execution in the real environment. By integrating world models into reinforcement learning, agents achieve more robust and intelligent behavior, making them well-suited for complex and dynamic tasks.

![image caption](img/Chp3/Planet.png){#fig-planet fig-align="center" width="50%"}

### Perception Module

The perception module processes raw sensory inputs from the environment and transforms them into more compact and useful representations. This typically involves using neural networks such as autoencoders. By efficiently encoding sensory information, the perception module helps the agent understand and navigate its environment more effectively.

### Transition Model

The transition model, also known as the dynamics model, predicts the next state given the current state and action. It captures the temporal dynamics of the environment and can be implemented using various types of neural networks, including state space models. This model allows the agent to simulate future states and understand how its actions affect the environment over time.

### Reward Model

The reward model predicts the expected reward for a given state-action pair. By understanding the reward structure of the environment, the agent can better evaluate the potential benefits of different actions. This helps the agent to make decisions that maximize long-term rewards, guiding it towards achieving its goals more effectively.

### Planning Module

With the internal model of the environment, the agent can simulate different action sequences to plan its moves. This planning is often done using model-based algorithms such as Monte Carlo Tree Search (MCTS), trajectory optimization methods, or the Cross Entropy Method (CEM). MCTS explores possible future states by building a search tree, trajectory optimization refines action sequences to maximize rewards, and CEM generates and evaluates a population of action sequences, iteratively refining them based on the best performers. By utilizing these methods, the agent can effectively predict future states and rewards, allowing it to choose actions that optimize its long-term success.

![image caption](img/Chp3/CEM_1.png){#fig-cem fig-align="center" width="50%"}

**Caption:** Formulas representing key components of world models in reinforcement learning, including

the perception module: $$ 
z = f_\text{perception}(x)
$$

the transition model : $$ 
s_{t+1} = f_\text{transition}(s_t, a_t)
$$

the reward model : $$ 
r_t = f_\text{reward}(s_t, a_t)
$$ , and the planning module : $$
\text{Best Action Sequence} = \arg\max_{a_1, a_2, \ldots, a_T} \sum_{t=1}^{T} f_\text{reward}(s_t, a_t)
$$

### Problem Definition in RL

state^t^ : the cuboid shape in time t

action^t^ : force vector

state^t+1^ : the cuboid shape in time t+1

and The transition function that models how the state changes due to the applied action:

$$
\text{state}_{t+1} = f_{\text{transition}}(\text{state}_t, \text{action}_t)
$$

I solved the problem with 2D and 3D approaches, that will be explained in the next sections.

## 2D Approach - Image based

As the cuboid is only subjected to force from the top, an image of the top surface can adequately represent the entire shape. For this purpose, I selected the top vertices of the mesh and used the 'Kriging' method from Openturns library @baudin2015open to interpolate the z-values across the entire surface. Kriging @cressie2015statistics also known as "Gaussian process regression", is a method of interpolation based on a Gaussian process governed by prior covariances. It is widely used in spatial analysis and computer experiments to predict values of a spatially distributed variable from sparse data points. Under suitable assumptions of the prior, Kriging provides the Best Linear Unbiased Prediction (BLUP) at unsampled locations, outperforming other interpolation methods based on criteria such as smoothness (e.g., smoothing spline). By considering both the distance and the degree of variation between known data points, Kriging generates a smooth, continuous surface that accurately reflects underlying spatial trends, making it ideal for interpolating from sparse pixels to a smooth image. \[image size: 20x50\]

Ordinary Kriging has been used here because it assumes the process mean to be constant and unknown. By utilizing a covariance model (specifically, the SquaredExponential covariance model) and the openturns Kriging algorithm, the Kriging metamodel can be fitted using the provided coordinates and observations. These characteristics align well with the Ordinary Kriging method, as accurate spatial data estimations can be provided. Originally developed for geostatistical applications, Kriging is a versatile statistical interpolation method used across various disciplines for sampled data from random fields that meet certain mathematical assumptions. It is particularly useful for estimating data in the spatial gaps between measured points, whether the data is collected in 2D or 3D.

![the depth image of the top surface (left) top vertices visualized on top surface as circles (right) the color shows the depth (z)](img/Chp3/kriging_1.png){fig-align="center" width="40%"}

![the regular grid interpolated using kriging method](img/Chp3/kriging_2.png){#fig-krig2 fig-align="center" width="40%"}

![the deformed shape and its corresponding top image](img/Chp3/mesh_krig.png){#fig-mesh-krig fig-align="center" width="40%"}

### World Model

the world model is our predictor, in a latent space domain. the WMs can be trained offline or online to reflect the effect of the Force (as action) on the compressed version of the state (shape). first I compressed the data utilizing a Conditional ConvolutionalAutoencoder. My model includes an encoder-decoder structure, where the encoder comprises two convolutional layers with ReLU activation functions and max pooling operations to progressively reduce the spatial dimensions while increasing feature depth. The latent space is further manipulated through linear layers that incorporate external conditioning information. The decoder then reconstructs the original input using two deconvolutional layers, aiming to produce an output that closely resembles the initial data. The CCAE is trained to minimize the reconstruction loss, making it suitable for image compression.

n_latent: compression rate ##todo the best parameters - reported from HPO : ...

![from initial state to final state - NN prediction of states](img/Chp3/cem_2d_seq.png){fig-align="center" width="80%"}

the proposed approach is limited to top surface and can not effectively generalize to 3D. however has an acceptable accuracy and performance and could be applied for similar problems such as \[# todo examples\] where the 2D prediction would be sufficient.

## 3D Approach - Vertex based

instead of converting the top surface to the image, we could directly work on 3D coordinates of vertices on top. in this approach, the NN input would be the same as number of top vertices coordinates (x,y,z) , so usually we could have a smaller NN. as the order of NN inputs are important, we need to keep the topology fixed and assure that the order of vertices is remained fix.

Here I -manually- kept the topology fixed. therefore, my vertices have index and they are ordered. so I trained a simple MLP that directly receives the xyz coordinates of the top vertices and the action , to predict the next placement of vectors. as in real, it rarely happens that we have a mesh dataset with identical topology for all samples, that would be an easy and low-cost approach if needed. the same as image approach, all data ( here vertices' coordinates ) corresponding to each state, is fed to the network at once. so the network has a "global" view of each state.

as the data size is relatively small, the vertex positions could be applied in RL loop directly. the network has the size of - - - layers , optimized by --. error plots... #todo

## 3D Approach - Mesh based

For processing the mesh data, we needed to compress the meshes utilizing the well-known CoMA @ranjan2018generating generating model. the Mesh autoencoder is trained to minimize the reconstruction error on our mesh dataset and using HPO, the best set of parameters are selected. So the trained encoder is now utilized to encode all meshes in the dataset in offline mode.

#todo HPO parameters , mish activation function, compression rate

The encoded meshes are again collected in a form of a dataset to train the WM. During the training of the World Model, the neural network is fed with \[State_t, action_t\] as input, to predict the State_t+1. please notice that the state is the encoded shape or the so called latent representation of the mesh.

the same as before, the PlaNet is used here, this time for encoded mesh representation. $$
\text{NN}([\text{State}_{t}, \text{action}_t]) \rightarrow \text{State}_{t+1}
$$

### Challenges of working with meshes

Working with meshes in combination with neural networks presents several fundamental challenges that we address: The first challenge is the lack of regular structure in mesh vertices, unlike the pixels in an image. Mesh vertices are scattered in three-dimensional space and cannot be easily vectorized from the top-left to the bottom-right like image pixels for input into the network. The second challenge is that the mesh structure must be presented to the neural network all at once, causing the network to become significantly large when dealing with a large mesh with many vertices, thereby increasing the number of learning parameters. The third challenge is that maintaining the mesh topology is very difficult and in many cases seems impossible. In tasks like physical simulation and finite element analysis, the resulting mesh needs to be re-meshed for reprocessing, which alters the mesh topology. (In my case however, I had to preserve the mesh topology manually!) Therefore, using alternative methods that can overcome these problems can be very helpful. In the next section, we introduce implicit methods that can serve as a good alternative to meshes and address these issues.

## 3D Approach - Implicit data

For SDF representation, a fixed grid is established, and the distance from the surface is calculated for all meshes within this grid. This grid is randomly sampled in the space surrounding the shape, both inside and near the surface. The fixed grid is essential for tracking distance values, as the network needs to be aware of the current state to predict the subsequent state. The inputs to the network are the xyz coordinates of the query point, the current SDF value at that point (SDF^t^), and action^~t~^ (a force vector) as a condition, with the output being SDF^t+1^.

![image caption](img/Chp3/net_sequence.png){#fig-implicit-net fig-align="center" width="80%"}

For visualization and evaluation, error metrics are used to compare predicted values with target values in the test set. The test set includes various mesh sequences selected from the dataset to assess our method. Since the current SDF for each point is necessary to predict the next SDF value, the process starts from the initial (undeformed) mesh to generate SDF values sequentially. Initially, an SSD (Single-Step Deformation) Network is used, followed by MSD (Multi-Step Deformation) Networks to predict subsequent SDF values.

This SSD Network is trained on undeformed mesh samples and are the same as model described in \[chapter X\]. the only difference is that, the query points are chosen from our fixed grid point. also a new input (sdft) is fed to the network to represent the current sdf of the query point.

### Challenges of working with Implicit data

use of implicit data has a great benefit over explicit methods in applications such as reconstruction, classification etc. they have a good binding with NNs and small network size for embedding and encoding of 3d shapes. however in context of deformation, we need to have a global understanding of the shape - or state - to generalize the action ( Force ) effect on the whole body. as the data unit provided to the network is related to one point ( and not the whole shape ), here the network has a partial observation of the state each time.