[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Title of the thesis :D",
    "section": "",
    "text": "Abstract\nAbstract of the thesis.\nkeywords: Structural Analysis Finite Element Analysis/Method Deep Neural Networks Neural Encoding Implicit Representations Signed Distance Functions/ Fields",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "1-Introduction.html",
    "href": "1-Introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Problem Statement",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1-Introduction.html#problem-statement",
    "href": "1-Introduction.html#problem-statement",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 ml@karoprod BMBF Project\nTitle: Machine Learning for the Prediction of Process Parameters and Component Quality in Automotive Body Production (ML@Karoprod)\nThe goal of the project from a machine learning perspective is to optimize the process parameters of a product chain to deform a metal plate through a series of operations. Typically, an expert runs FEM simulations, adjusts the parameters, and evaluates the results to achieve the desired shape. This iterative process, which takes about 20 minutes per cycle, continues until the desired accuracy is achieved, with the expert manually applying the necessary changes and running subsequent simulations.\n\n\n\n\n\n\nFigure 1.1: cap\n\n\n\nThe process begins with deep drawing, a sheet metal forming technique where a metal blank is radially drawn into a forming die by a punch, with the goal of producing a wrinkle-free and crack-free product. Our simulated deep drawing data consists of a number of valid experiments, each characterized by various process parameters such as blank holder force, insertion position, material ID, and punch ID. In collaboration with our partners, we have identified the relevant set of features that are necessary and sufficient to fully determine the effects of a simulation.\n\n\n\n\n\n\nFigure 1.2: cap\n\n\n\n\n\n\n\n\n\n\n\nInput parameters\nFrom\nto\n\n\ndie diameter\n7.5mm \n12.5mm\n\n\ndie depth in relation to total sheet thickness \n0.2 \n0.6\n\n\ndie diameter bottom in relation to punch diameter\n0.8\n1.2\n\n\nangle die bottom\n15 deg\n45 deg\n\n\nangle die side face\n0 deg\n5 deg\n\n\npunch diameter in relation to die diameter\n0.5\n0.75\n\n\nradius punch\n0.25mm\n0.5mm\n\n\nangle punch face\n0 deg\n2 deg\n\n\nangle punch side face\n0 deg  \n4 deg\n\n\nthickness upper sheet in relation to total sheet thickness\n0.2\n0.8\n\n\nbottom thickness\n0.589mm\n5.035mm\n\n\n\nThe project partner generated approximately 1000 simulation files in mesh format by varying process parameters. After data cleaning, about 880 of these files were usable. The meshes had three different geometries (with three different depths: 30, 50, and 70), and parameters such as force and material varied, resulting in different final mesh outcomes. The ultimate goal in this section was to train a neural network to predict the dependency between changes in input parameters and the generated mesh. With sufficient training, the model would be able to accurately predict the shape of the final mesh even with new inputs that were not seen during training. This allows the specialist to quickly and accurately observe the final simulation results by manipulating parameters, in a much shorter time compared to traditional finite element methods.\n\n\n\nThe reference mesh for extracting face centers (Drawing depth Zt=70).\n\n\n\n\n\nThe same mesh in a closer view\n\n\n\n\n\ncaption\n\n\n\n\n\n\n\n\nFigure 1.3: caption",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1-Introduction.html#research-questions",
    "href": "1-Introduction.html#research-questions",
    "title": "1  Introduction",
    "section": "1.2 Research Questions",
    "text": "1.2 Research Questions\nreplacing mesh processing with implicit representations to have a continuous representation of models\ncontinuous fields\ninfinite resolution",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "2-StateOfTheArt.html",
    "href": "2-StateOfTheArt.html",
    "title": "2  State of the art",
    "section": "",
    "text": "2.1 FEM Simulations and applications\nThe Finite Element Method (FEM) is a powerful computational technique used for solving complex structural, fluid, and thermal problems in engineering and physical sciences. It works by breaking down a large problem into smaller, simpler parts known as finite elements, and then systematically solving these elements to understand the behavior of the entire system.\nFEM is a numerical approximation of the continuous solution field u of any partial differential equation (PDE) given by Eq. (1) on a given domain \\Omega can be performed by various methods. Some of the widely used techniques include finite element method [ref1], finite volume method [ref2], particle methods [ref3], and finite cell method [ref4]. In this contribution, we restrict the discussion to Galerkin-based finite element methods.\n\\mathcal{L}(u) = 0 \\quad \\text{on } \\Omega\nu = u_d \\quad \\text{on } \\Gamma_D\n\\frac{\\partial u}{\\partial x} = g \\quad \\text{on } \\Gamma_N\nConsider the PDE in Eq. (1) defined on a domain \\Omega together with the boundary conditions given by Eqs. 2 and 3. Here, u_d and g are the Dirichlet and Neumann boundary conditions on the respective boundaries. A finite element formulation of Eq. (1) on a discretization of the domain with m elements and n nodes, together with boundary conditions, will result in the system of equations shown by Eq. (4). We assume all the necessary conditions on the test and trial spaces [ref1] are fulfilled.\n\\begin{pmatrix}\nk_{1,1} & k_{1,2} & \\cdots & k_{1,n} \\\\\nk_{2,1} & k_{2,2} & \\cdots & k_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nk_{n,1} & k_{n,2} & \\cdots & k_{n,n}\n\\end{pmatrix}\n\\begin{pmatrix}\nu_1 \\\\\nu_2 \\\\\n\\vdots \\\\\nu_n\n\\end{pmatrix} =\n\\begin{pmatrix}\nF_1 \\\\\nF_2 \\\\\n\\vdots \\\\\nF_n\n\\end{pmatrix}\nIn Eq. (4), \\mathbf{K}(u) is the non-linear left hand side matrix, also called the stiffness matrix. \\mathbf{u} is the discrete solution field, and \\mathbf{F} is the right hand side vector. The residual of the system of equations in Eq. (4) can be written as\nr(u^h) = K(u^h)u^h - F\nTo obtain the solution u^h, a Newton–Raphson iteration technique can be employed using the linearization of r(u^h) and its tangent matrix. This requires the solution of a linear system of equations in every iteration. These iterations are carried out until the residual norm \\|r(u^h)\\| meets the tolerance requirements. For a detailed discussion of the methodology, readers are referred to [ref2]. For this residual-based formulation, in case of a linear operator K, it takes only one iteration to converge. For a large number of elements and nodes, among different steps of the finite element methodology, the most computationally demanding step is the solution of the linear system of equations. In an application where computational efficiency is critical, like real time simulations [ref3] and digital twins [ref4], it is imperative that this step be avoided. Techniques suitable for such applications, like model order reduction [ref5, ref6], construct a surrogate model of Eq. (4) to reduce this cost significantly. Techniques involving neural-networks can completely avoid this cost, but will require a significant amount of training and test data, which is typically generated by simulating the underlying finite element problem. In “Finite element method-enhanced neural network for forward problems” section, we discuss an algorithm that combines residual information from a numerical method to train a neural network for linear PDEs. In this case the residual r(u^h) becomes\nr(u^h) = Ku^h - F\nFEM is widely used in various industries to simulate physical phenomena and optimize designs, significantly reducing production costs by allowing for virtual testing and refinement before physical prototypes are created. The method gained prominence in the 1960s and has since undergone significant advancements, driven by improvements in computer technology and numerical algorithms. Today, FEM is integral to fields such as aerospace, automotive, civil engineering, and biomedical engineering, offering highly accurate predictions and contributing to innovation and efficiency in product development.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>State of the art</span>"
    ]
  },
  {
    "objectID": "2-StateOfTheArt.html#fem-simulations-and-applications",
    "href": "2-StateOfTheArt.html#fem-simulations-and-applications",
    "title": "2  State of the art",
    "section": "",
    "text": "2.1.1 Current Advancements and Challenges\nDespite its significant power and advantages, FEM’s primary focus on mesh construction and solving numerous complex partial differential equations (PDEs) can lead to slow performance when dealing with highly intricate problems. This method demands substantial computational power, and each parameter change necessitates a complete re-execution, demonstrating limited flexibility in adapting to changes. Consequently, scientists are actively seeking to improve and potentially replace FEM with more efficient methods.\nDue to the challenges associated with traditional FEM, mesh-free methods have consistently garnered attention. These approaches aim to address FEM’s limitations by offering greater flexibility, faster computations, and the ability to handle complex scenarios more efficiently. However, it is important to note that FEM remains comprehensive and applicable for a wide range of problems, including solid and fluid mechanics, among others. Alternative methods are often tailored to specific types of problems and applications, meaning that there is currently no complete replacement for FEM. Instead, these alternative methods are being developed and refined to provide better and more suitable solutions for specific issues​. for instance in Zhang et al. (2024) Mesh-free methods for crack problems have been reviewed.\nAmong the proposed approaches, the Smoothed Particle Hydrodynamics (SPH) [] , the Element-Free Galerkin Method (EFGM) [] and the Material Point Method (MPM) [] have been notable efforts. SPH employs particles to simulate fluid elements and interactions, proving particularly effective for complex fluid dynamics problems. EFGM, on the other hand, utilizes nodes and shape functions to approximate solutions, providing significant flexibility and accuracy for intricate geometries and boundary conditions. Additionally, MPM represents materials as moving points through a computational grid, making it especially suitable for scenarios involving large deformations and complex material behaviors. These mesh-free approaches offer enhanced adaptability and computational efficiency, addressing some of the core limitations of FEM​​.\nIn this thesis however, our focus on FEM is specifically related to structural analysis and deformation problems. Among the mesh-free methods introduced in this field, the EFGM and the MPM are particularly relevant. These methods provide promising alternatives to traditional FEM by enhancing computational efficiency and adaptability in solving mechanical and deformation-related problems​.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>State of the art</span>"
    ]
  },
  {
    "objectID": "2-StateOfTheArt.html#replacing-fem-with-ai-approaches",
    "href": "2-StateOfTheArt.html#replacing-fem-with-ai-approaches",
    "title": "2  State of the art",
    "section": "2.2 Replacing FEM with AI Approaches",
    "text": "2.2 Replacing FEM with AI Approaches\nSimilar to other fields, the application of AI in FEM has a relatively long history. While a comprehensive solution to completely replace FEM is yet to be found, AI’s advancements in this area are promising.\nin Zhang et al. (2024) the Mesh-free methods for crack problems have been reviewed.\nwith examples such as neural networks for stress analysis, which have been proposed for specific applications[].\nFor deformation problems, solutions often involve simplifying the problem to 2D images and employing image processing techniques[].\nAdvancements in replacing the Finite Element Method (FEM) are not confined to a few specific techniques. Instead, various solutions have been proposed depending on the nature of the problem, the type of data, and the complexities involved. These diverse approaches reflect the need for tailored solutions to effectively address the unique challenges presented by different FEM applications.\nPhysics-Informed Neural Networks (PINNs) are another innovative approach, integrating physical laws into the learning process to solve PDEs. In the following sections, we will review some of the most significant works related to our problem:\n\n2.2.1 Physics informed Neural Networks (PINN)\nPhysics-Informed Neural Networks (PINNs) Raissi et al. (2019) are a class of neural networks that integrate physical laws described by partial differential equations (PDEs) into the learning process. They leverage the universal approximation capability of neural networks to solve forward and inverse problems governed by PDEs. The core idea of PINNs is to minimize a loss function that includes both the data-driven error and the residuals of the PDEs, thereby ensuring that the learned solution satisfies the underlying physical laws.\nIn the context of a PDE, such as \\mathcal{N}(u(x)) = 0, where \\mathcal{N} is a differential operator and u(x) is the solution, the loss function \\mathcal{L} for a PINN can be expressed as:\n\n\\mathcal{L} = \\mathcal{L}_{\\text{data}} + \\mathcal{L}_{\\text{PDE}}\n\nHere, \\mathcal{L}_{\\text{data}} represents the mean squared error (MSE) between the neural network’s predictions u_{\\theta}(x) and the observed data points u_{\\text{obs}}(x):\n\n\\mathcal{L}_{\\text{data}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( u_{\\theta}(x_i) - u_{\\text{obs}}(x_i) \\right)^2\n\nThe term \\mathcal{L}_{\\text{PDE}} enforces the PDE constraints by computing the MSE of the residuals at collocation points x_c:\n\n\\mathcal{L}_{\\text{PDE}} = \\frac{1}{M} \\sum_{j=1}^{M} \\left( \\mathcal{N}(u_{\\theta}(x_{c_j})) \\right)^2\n\nBy optimizing the combined loss \\mathcal{L}, the neural network is trained to produce a solution that fits the observed data while also satisfying the physical constraints imposed by the PDE.\nPINNs have been successfully applied to various challenging problems, including fluid dynamics, structural mechanics, and electromagnetic simulations. Their ability to incorporate prior physical knowledge directly into the learning process makes them a powerful tool for modeling complex systems where data is scarce or noisy. Additionally, PINNs can be used for solving inverse problems, where the goal is to infer unknown parameters or functions within the PDEs, by including terms in the loss function that account for the discrepancies between the predicted and observed data, as well as the governing physical laws. This versatility highlights the potential of PINNs in enhancing the accuracy and robustness of simulations in scientific and engineering applications.\n\n\n2.2.2 Image based fems …\nDespite significant advancements in various fields, a comprehensive model specifically designed to address the problem of 3D shape deformation using AI has not yet been developed (as of the time of writing this thesis). Many existing methods are frequently confined to 2D spaces, with fewer efforts made to extend these solutions to 3D problems. In real-world applications, interacting with 3D data is preferable as it more closely resembles actual conditions, enhancing the realism and accuracy of simulations. Expanding AI applications to 3D FEM simulations can significantly improve their applicability and fidelity in real-world scenarios.\nOn the other hand, various AI techniques for working with 3D data have been actively pursued in domains such as computer graphics, 3D reconstruction, 3D object classification etc. It is important to note that this is a multidisciplinary issue, requiring collaboration across different scientific fields. Interaction among specialists from various domains is crucial to finding a common ground and proposing more effective solutions. This interdisciplinary cooperation is essential for aligning different areas of expertise to develop more robust and effective methods for 3D shape deformation.\nTherefore, in this section, we will explore computer science methods that are similar to our problem and work with 3D data using AI tools. The work presented here is relatively specialized, and as of now, no existing AI method has been found that directly addresses our specific problem. However, we can expect to see an increase in intelligent models tackling this issue in the near future. The continued development of AI in this area holds great promise for improving the accuracy and efficiency of simulations involving 3D shape deformation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>State of the art</span>"
    ]
  },
  {
    "objectID": "2-StateOfTheArt.html#introduction-to-deep-neural-networks",
    "href": "2-StateOfTheArt.html#introduction-to-deep-neural-networks",
    "title": "2  State of the art",
    "section": "2.3 Introduction to Deep Neural Networks",
    "text": "2.3 Introduction to Deep Neural Networks\nDeep Neural Networks (DNN) represent a significant evolution in the field of artificial intelligence, particularly in machine learning and pattern recognition. Building on the foundation of traditional neural networks, DeepNNs consist of multiple layers of interconnected neurons, allowing them to model complex patterns and relationships in data more effectively than shallow networks. The advent of DeepNNs has revolutionized numerous fields, including computer vision, natural language processing, and reinforcement learning, marking a pivotal shift in the capabilities of AI systems.\nThe concept of neural networks dates back to the mid-20th century, with the introduction of the perceptron by Frank Rosenblatt in 1958. However, the journey towards DeepNNs gained momentum only in the 1980s, with the development of the backpropagation algorithm, a critical breakthrough enabling the training of multi-layered networks. Backpropagation, introduced by Rumelhart, Hinton, and Williams in 1986, made it feasible to adjust the weights of neural networks through gradient descent, efficiently minimizing the error between predicted and actual outcomes. This algorithm remains at the heart of training deep networks, enabling them to learn complex functions from data.\nDespite these early advances, DeepNNs struggled to gain traction due to computational limitations and the challenge of vanishing gradients, a problem where gradients used to update the weights become increasingly small in deeper layers, hindering effective learning. This issue was addressed by the introduction of more advanced activation functions like ReLU (Rectified Linear Unit), which helped maintain more consistent gradients, and by innovations such as batch normalization and more sophisticated weight initialization techniques.\nKey Concepts and Advancements One of the core principles behind the success of DeepNNs is gradient descent, an optimization algorithm used to minimize the loss function. The loss function L(\\theta) quantifies the error in the network’s predictions, where \\theta represents the network’s parameters (weights and biases). Gradient descent iteratively adjusts \\theta in the direction opposite to the gradient of the loss function, formally expressed as:\n\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t)\nwhere η is the learning rate, and \\nabla_\\theta L(\\theta_t) is the gradient of the loss function with respect to the parameters. However, the success of gradient descent in deep networks relies heavily on effective weight initialization, appropriate activation functions, and strategies to mitigate overfitting and vanishing gradients. The introduction of ReLU activation functions, expressed as f(x)=max(0,x), was instrumental in overcoming the vanishing gradient problem by ensuring that gradients remain significant in deeper layers, thus allowing for better learning and performance.\nAdditionally, innovations like convolutional layers in CNNs, recurrent layers in RNNs, and attention mechanisms in transformers have expanded the applicability and power of DeepNNs. CNNs have revolutionized image processing, RNNs have enabled effective modeling of sequential data, and transformers have set new benchmarks in natural language processing.\nA crucial aspect of neural networks is the dataset. Typically, the model is trained on a large amount of training data, validated on a separate validation set, and then tested on unseen test data to evaluate its performance. The selection and preprocessing of data are of particular importance in this process. Neural networks often operate best when the data is scaled within a specific range, usually between 0 and 1. Therefore, considerable effort is made to ensure that the data is normalized to fall within this optimal range, which significantly contributes to the effectiveness of the model.\nThe success of these architectures has been further amplified by large-scale datasets and the availability of massive computational resources, leading to groundbreaking achievements such as Google’s AlphaGo, OpenAI’s GPT models, and various state-of-the-art systems in image and speech recognition.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>State of the art</span>"
    ]
  },
  {
    "objectID": "2-StateOfTheArt.html#autoencoders",
    "href": "2-StateOfTheArt.html#autoencoders",
    "title": "2  State of the art",
    "section": "2.4 Autoencoders",
    "text": "2.4 Autoencoders\nAn autoencoder is a type of neural network designed to learn a compressed, efficient representation of input data. It consists of two main components: an encoder and a decoder. The encoder compresses the input data into a lower-dimensional representation, often referred to as the latent space or bottleneck. The decoder then attempts to reconstruct the original input from this compressed representation. The goal is to minimize the difference between the input and the reconstructed output.\nMathematically, the process can be represented as: \n\\mathbf{z} = f_\\theta(\\mathbf{x})\n\n\n\\hat{\\mathbf{x}} = g_\\phi(\\mathbf{z})\n\nHere, \\mathbf{z} is the latent representation of the input \\mathbf{x}, and \\hat{\\mathbf{x}} is the reconstructed output. The objective of the autoencoder is to minimize the reconstruction loss, commonly measured as the mean squared error (MSE):\n L(\\mathbf{x}, \\hat{\\mathbf{x}}) = \\| \\mathbf{x} - \\hat{\\mathbf{x}} \\|^2_2 \nAutoencoders are particularly useful in areas such as dimensionality reduction and reconstruction, where they compress high-dimensional data into a lower-dimensional latent space and then accurately reconstruct the original data. They also excel in denoising by removing noise from corrupted data, and in anomaly detection by identifying outliers based on reconstruction errors. Additionally, autoencoders are valuable in generative modeling, contributing to the creation of new, similar data samples, which is especially beneficial in image synthesis and data augmentation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>State of the art</span>"
    ]
  },
  {
    "objectID": "2-StateOfTheArt.html#d-deep-learning",
    "href": "2-StateOfTheArt.html#d-deep-learning",
    "title": "2  State of the art",
    "section": "2.5 3D Deep Learning",
    "text": "2.5 3D Deep Learning\nThree-dimensional (3D) data processing using deep learning has become increasingly important in fields such as computer vision, robotics, and virtual reality. The way 3D data is represented plays a critical role in the design of models, the training process, and the final output. Depending on how the data is structured and represented, 3D deep learning can be categorized into several approaches. The most common representations include point clouds, meshes, and multi-view images, which are considered explicit forms of 3D data. Each of these representations has its unique challenges and benefits, influencing the choice of network architecture and the methods used for training.\nIn addition to explicit representations, implicit representations have gained significant attention in recent years. Unlike explicit forms, where the 3D data is directly stored and processed, implicit representations model the 3D structure in a more abstract way, such as through occupancy fields or signed distance functions. These newer approaches allow for more flexible and continuous representation of 3D shapes, often leading to better generalization and smoother reconstructions. Due to their efficiency and ability to handle complex geometries, implicit representations are becoming increasingly popular in state-of-the-art 3D deep learning applications.\n\n2.5.1 NN and Explicit representations : Point Clouds\n\n2.5.1.1 PointNet, PointNet++\n\n\n\n2.5.2 NN and Explicit representations : Mesh\nMeshes are one of the most widely used explicit representations in 3D deep learning, particularly in fields like computer graphics, medical imaging, and 3D modeling. A mesh is essentially a specific form of graph, composed of vertices (points in 3D space) and edges (connections between vertices), which together form faces that define the surface of a 3D object. This representation is highly expressive, capturing intricate details of an object’s surface geometry and topology. Unlike more regular data structures such as images or point clouds, meshes have a non-Euclidean structure, presenting unique challenges for neural networks that need to process this type of data. To address these challenges, specialized neural network architectures like graph neural networks (GNNs) or convolutional neural networks adapted for non-Euclidean spaces (e.g., mesh convolutional networks) are often used. These networks are designed to leverage the connectivity information inherent in meshes, allowing them to learn and extract meaningful features directly from the mesh’s topology. Therefore, these GNNs are highly effective for data with a graph-like structure, where connections between verices are crucial, such as social network or molecular graphs, where understanding complex relational patterns is crucial. However, when the mesh geometry and the positions of the vertices themselves become important, these networks face significant challenges. Examples of networks that have addressed this issue in geometric deep learning for classification, under specific constraints, include MeshCNN, SplineCNN, and CoMA Ranjan et al. (2018).\n\n2.5.2.1 Convoutional Mesh AutoEncoder (CoMA)\nThe Convolutional Mesh AutoEncoder (CoMA), introduced in 2017, is a neural network architecture specifically designed to process 3D mesh data.\nUnlike traditional autoencoders, which are inherently designed for structured, grid-like data such as images, CoMA is specifically engineered to tackle the complexities of irregular 3D mesh structures. Traditional convolutional neural networks (CNNs) excel in processing data arranged in regular grids, where each pixel or voxel is neatly aligned with its neighbors. However, 3D meshes are fundamentally different—they consist of vertices connected by edges forming irregular polygons, typically triangles, that define the surface of a shape. These irregularities present significant challenges for standard CNNs, which rely on the spatial consistency of grid data to apply convolutional operations effectively.\nCoMA overcomes these challenges by extending convolutional operations from regular grids to graph structures, where the mesh vertices and their connections (edges) form a graph. This adaptation is made possible through spectral graph convolutional layers, which operate in the frequency domain, allowing the network to process the mesh’s geometry in a way that respects its inherent irregularity. These layers perform convolutions not in the traditional spatial sense, but by filtering the mesh’s geometric features across its graph-based structure. This allows CoMA to capture both local and global geometric information, which is crucial for accurately representing complex 3D shapes.\nThe architecture of CoMA is built around a symmetric encoder-decoder design, where both the encoder and decoder consist of four layers. The encoder’s role is to compress the high-dimensional mesh data into a lower-dimensional latent space. This compression is achieved through the spectral graph convolutional layers, which progressively reduce the resolution of the mesh while preserving its most significant geometric features. The latent space effectively captures the high-level, abstract representation of the 3D shape, distilling its most important characteristics into a compact form.\nOnce the mesh data is compressed into this latent space, the decoder takes over, reconstructing the original high-resolution mesh from the compact representation. The decoder mirrors the encoder’s structure, using upsampling operations and inverse convolutions to progressively restore the mesh’s resolution. This reconstruction process allows CoMA to generate a detailed and accurate representation of the original 3D shape, ensuring that essential geometric properties are preserved throughout the process. The combination of these advanced techniques enables CoMA to learn and manipulate complex 3D shapes efficiently, making it a powerful tool for tasks like shape reconstruction, deformation transfer, and facial expression synthesis.\nA key innovation of CoMA is its ability to handle the irregular topology of meshes through spectral graph convolutions, which operate in the frequency domain. By leveraging Chebyshev polynomials and fast localized convolutions, the model efficiently processes mesh data. However, it is important to note that all mesh samples in the dataset must share the same topology, and a model trained on one dataset is not easily extendable to others. Additionally, the input mesh must exhibit properties such as regularity, uniform connectivity, and also a consistent hierarchical structure to support both downsampling and upsampling operations. Without these properties, operations like pooling and unpooling can become problematic, potentially necessitating remeshing to create a dataset suitable for CoMA. Furthermore, the network’s first - and last - layers must have a size of 3 times the number of vertices to represent XYZ positions, which can result in a large model when dealing with high-resolution meshes. Therefore, while CoMA is powerful for tasks like 3D shape reconstruction, facial expression synthesis, and deformation transfer, it does have specific requirements and limitations regarding the structure and properties of the input meshes.\n\n\n2.5.2.2 SplineCNN\nFey et al. (2018) SplineCNN is a type of convolutional neural network designed to operate on non-Euclidean domains, such as graphs and meshes, where data is irregular and connectivity information is crucial. Unlike traditional CNNs that use fixed rectangular kernels, SplineCNN employs learnable B-spline kernels that can adapt to the underlying structure of the data, allowing the network to perform convolutions directly on the graph or mesh. This flexibility enables SplineCNN to effectively capture both the local geometric structure and the connectivity patterns, making it well-suited for tasks like 3D shape analysis and graph-based classification.\n\n\n\n\n\nSplineCNN has demonstrated its effectiveness by improving state-of-the-art results in several benchmark tasks, including image graph classification, graph node classification, and shape correspondence on meshes. For the task of shape correspondence, SplineCNN was validated on a collection of 3D meshes, solving the challenge of matching each node of a given shape to the corresponding node of a reference shape. However, the dataset used in this experiment had a significant limitation: all meshes shared the same topology, requiring that the mesh sizes and node orders remain consistent across the entire dataset, which is a considerable constraint.\n\n\n2.5.2.3 MeshCNN\nHanocka et al. (2019)\nMeshCNN is a specialized neural network architecture designed to process 3D mesh data by adapting convolutional operations to the irregular, non-Euclidean structure of meshes. Unlike traditional CNNs that operate on grid-like data, MeshCNN applies convolutions directly on the edges of a mesh, enabling the network to capture the geometric and topological features inherent in 3D shapes. The architecture employs edge-based convolutions and pooling operations to reduce the mesh’s complexity while preserving its essential properties, making it particularly effective for tasks such as 3D shape classification and segmentation. This approach is well-suited for scenarios where understanding the detailed geometry and topology of 3D objects is crucial, such as distinguishing between different types of 3D models or segmenting parts of a 3D object.\nHowever, MeshCNN also introduces certain complexities and limitations. The input dimensionality is defined by the number of features per edge multiplied by the total number of edges, which can lead to increased computational demands, particularly for large meshes. Additionally, the architecture requires a consistent and well-defined mesh structure, often necessitating preprocessing steps like remeshing or simplification to ensure compatibility. This dependence on specific mesh topologies can limit the network’s generalization to different types of meshes, and the computational load may challenge scalability in scenarios with varying mesh structures or limited resources.\n\n\n2.5.2.4 MeshNet\nFeng et al. (2019)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>State of the art</span>"
    ]
  },
  {
    "objectID": "2-StateOfTheArt.html#nn-and-imlicit-representations",
    "href": "2-StateOfTheArt.html#nn-and-imlicit-representations",
    "title": "2  State of the art",
    "section": "2.6 NN and Imlicit representations",
    "text": "2.6 NN and Imlicit representations\n\n2.6.0.1 DeepSDF\nPark et al. (2019)\nThe most direct application of this approach is to train a single deep network for a given target shape as depicted in Fig X Given a target shape, we prepare a set of pairs ( X ) composed of 3D point samples and their SDF values:\n\nX := \\{(x, s) : \\text{SDF}(x) = s\\}\n\nWe train the parameters \\theta of a multi-layer fully-connected neural network f\\_\\theta on the training set $ S $ to make f\\_\\theta a good approximator of the given SDF in the target domain \\Omega :\n\nf_\\theta(x) \\approx \\text{SDF}(x), \\forall x \\in \\Omega\n\n\n\n\n\nFeng, Y., Feng, Y., You, H., Zhao, X., and Gao, Y. (2019). Meshnet: Mesh neural network for 3d shape representation. in Proceedings of the AAAI conference on artificial intelligence, 8279–8286.\n\n\nFey, M., Lenssen, J. E., Weichert, F., and Müller, H. (2018). Splinecnn: Fast geometric deep learning with continuous b-spline kernels. in Proceedings of the IEEE conference on computer vision and pattern recognition, 869–877.\n\n\nHanocka, R., Hertz, A., Fish, N., Giryes, R., Fleishman, S., and Cohen-Or, D. (2019). Meshcnn: A network with an edge. ACM Transactions on Graphics (ToG) 38, 1–12.\n\n\nPark, J. J., Florence, P., Straub, J., Newcombe, R., and Lovegrove, S. (2019). Deepsdf: Learning continuous signed distance functions for shape representation. in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 165–174.\n\n\nRaissi, M., Perdikaris, P., and Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics 378, 686–707.\n\n\nRanjan, A., Bolkart, T., Sanyal, S., and Black, M. J. (2018). Generating 3D faces using convolutional mesh autoencoders. in Proceedings of the european conference on computer vision (ECCV), 704–720.\n\n\nZhang, M., Abidin, A. R. Z., and Tan, C. S. (2024). State-of-the-art review on meshless methods in the application of crack problems. Theoretical and Applied Fracture Mechanics, 104348.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>State of the art</span>"
    ]
  },
  {
    "objectID": "3-Methods.html",
    "href": "3-Methods.html",
    "title": "3  Methods",
    "section": "",
    "text": "3.1 Dataset\nTo train a neural network effectively, the first essential component is a dataset. A dataset provides the necessary examples for the network to learn patterns and make accurate predictions. In the context of 3D shape deformation and FEM simulations, having a comprehensive and high-quality dataset is crucial for the network to understand the complex relationships involved in deformation processes.\nUnfortunately, there are no public datasets specifically for 3D deformation available. Additionally, our project partner had a limited number of mesh simulations, which required significant time to generate. Consequently, I needed to create my own dataset to test and train my models. This would allow me to proceed with the project and later apply the developed models to the partner’s data.\nWhen searching for deformation and FEM examples, one commonly encountered scenario is a beam fixed at one end, undergoing bending and deformation when a force is applied.\nThis classic example inspired my dataset design. I modeled a slender, rectangular cuboid beam, fixed at both ends, with a point force applied from various directions to induce deformation.\nThe resulting dataset consisted of approximately 6300 mesh simulations, capturing the deformations caused by different force vectors at various points on top or bottom surface. This extensive dataset was suitable for training the neural network. I used FreeCAD software for this purpose, with detailed steps and methodologies described in section -XYZ- of this thesis.\nBy generating this dataset, I ensured that my models had sufficient data to learn from, allowing for robust training and validation. This foundation enabled me to develop and refine the models before applying them to the partner’s specific data, ensuring a smooth transition and effective implementation of AI techniques for 3D shape deformation.\nFreeCAD is a general-purpose parametric 3D computer-aided design modeler and a building information modeling software application with finite element method support. I used FreeCAD for several reasons: it is open-source, supports Python scripting, offers an easy setup for FEM, and has a rich forum and community support.\nTo perform FEM simulations in FreeCAD, I followed these steps:\nGeometry Definition: Create the 3D model of the beam. Fixed Constraints: Define the fixed points where the beam is held. Force Constraints: Apply forces at specific points and directions on the beam. (this should change during the data generation loop) Add Material: Assign material properties to the beam. Create FEM Mesh: Generate a triangular mesh for the model. Solving Equations: Use CalculiX to solve the FEM equations and obtain the deformation results.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "3-Methods.html#dataset",
    "href": "3-Methods.html#dataset",
    "title": "3  Methods",
    "section": "",
    "text": "Figure 3.1: Beam Structure from Ansys Website\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Dataset samples - first version\n\n\n\n\n\n\n\n\n\n3.1.1 FCAD data version 1 (first beam)\nThe output of FEM is typically a deformed mesh, which is a structure used to represent 3D data. A mesh is a special type of graph characterized by its vertices, edges, and faces, making it an excellent and popular data structure for representing 3D data with various complexities and curvatures, especially useful for depicting deformations.\nThe output from FreeCAD was a deformed mesh where only the vertex positions differed from the initial mesh. The generated meshes all have the same topology, meaning they have the same number of vertices, and the neighborhood of each vertex remains consistent across all meshes. This consistency in topology ensures that the dataset is suitable for training neural networks, as the structural integrity of the meshes is maintained throughout the simulations. However, as the complexity of the mesh increases, its size also grows, which can make rendering and processing somewhat slower. This trade-off between detail and computational efficiency is a key consideration in the use of meshes for 3D data representation.\n\n\n3.1.2 version 2 (press on top)\nTo display multistep deformation, I decided to generate a new dataset with different conditions. I considered a cube with a larger surface area and shorter height, fixing it from the bottom. Random force vectors were applied to the surface and perpendicular to it at various points with different magnitudes. The deformation results were recorded for each step. Since these forces were applied sequentially to the object, it was necessary to record the name of each mesh and the state before and after the force application, along with the force specifications, in a separate table. This was done using a pandas DataFrame to facilitate easier access to the data files.\n\n\n3.1.3 2D images (kriging)\n\n\n3.1.4 SDF datasets",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "3-Methods.html#single-step-deformation",
    "href": "3-Methods.html#single-step-deformation",
    "title": "3  Methods",
    "section": "3.2 Single Step Deformation",
    "text": "3.2 Single Step Deformation\nSingle-step deformation (SSD) involves predicting the reshaping of an object when a force vector is applied to it at a specific point.\nIn DeepSDF paper, it was demonstrated that a neural network is capable of encoding a 3D object (and a series of 3D objects). In this scenario, the neural network acts as a signed distance function (SDF), where, after training, the network can be queried to determine the distance of any arbitrary point from the surface. By querying the network for a sufficient number of points, we can generate a point cloud, where the distances of these points from the surface are estimated by the neural network. This point cloud allows us to determine the surface boundary of the desired object.\nIn this study, each shape is encoded using an autodecoder and fed as a reference to the network, allowing the network to encode a wider range of objects. As shown in the image, at the top, two codes corresponding to two objects (leftmost and rightmost ones) are encoded by the network, forming the shapes of a sofa and a chair. By interpolating these codes, we observe a subtle transition between the sofa and the chair, with the reconstructed shapes visible on top. Instead of applying this concept to classes of different objects, we applied it to deformed versions of one object. This way, the network can generate the desired deformed version of an object based on the code, which represents the force applied to the object.\nquarto\n\n3.2.1 SSD on Watertight Mesh - Data preparation\nIn DeepSDF, the object mesh is required to be watertight. which means that the object surface devides the input and output space to distinguish the positive and negative distances from the surface. we used our First Dataset (FCAD Ver1) for training our network. The dataset were a set of triangular meshes, generated by applying various force constraints to a simple beam. To convert the meshes into SDF format, the following steps are required:\n\nNormalization and Scaling:\nEach 3D mesh is scaled to fit within a unit sphere. This normalization step ensures consistency across different meshes, making the SDF values comparable.\nVirtual Camera Rendering:\nThe normalized mesh is virtually rendered from multiple viewpoints. Tipically 100 virtual cameras are placed uniformly on the surface of the unit sphere to capture the shape from different angles.\nDistance Calculation:\nFor each viewpoint, the distance from the camera to the closest point on the mesh surface is calculated. This involves projecting points from the 3D space onto the mesh and computing the shortest distance to the mesh triangles.\nPoint Sampling:\nPoints are sampled more densely near the mesh surface to ensure higher accuracy in regions of interest. In our implementation, we sampled 400,000 points for each shape in the dataset.\nSigned Distance Computation:\nEach sampled point is assigned a signed distance value. The sign indicates whether the point is inside (+) or outside (-) the object, and the magnitude represents the shortest distance to the surface.\nSDF Representation Storage:\nThe computed signed distance values for all sampled points are stored, creating a dense representation of the shape’s geometry. which contains x,y,z coordinates of points and the corresponding SDF value.\n\nafter converting all meshes in SDF format, we can create a robust dataset and split it to train/test/validation sets. Our fully-connected neural network, designed with six inputs (spatial coordinates and force vectors), demonstrated the capability to accurately predict the SDF values for given deformation scenarios. This approach offers an efficient and flexible solution for modeling geometric deformations in 3D shapes.\n\n\n3.2.2 Training Neural Network\nthe designed neural network needs to map the 3D points in the space, to a corresponding SDF value, based on a provided force vector. similar to the DeepSDF paper, we choose a Multilayer Preceptron for this purpose. HPO\n** effect of different parameters on estimation -&gt; table\nactivation function\n\n\n3.2.3 SSD on non-Watertight Mesh\n*** (could be considered as point cloud?!) the data provided by our industrial partner however, was non-watertight. therefore the original DeepSDF method could not be applied there. instead we modify the input and train the neural network. the network could predict the thickness, thinning and deviation with an acceptable error.\n\n3.2.3.1 1D approach\n\n\n\n\n\n\nFigure 3.3: image caption\n\n\n\n\n\n\n\n\n\nFigure 3.4: a) Position of the x0 cut through the deep-drawn element. b) Deviations from the template for the 880 simulations. The three drawing depths (30, 50, and 70) are shown in different colors.\n\n\n\n\n\n3.2.3.2 2D approach\nThe deviation and thickness values are available not only along the cuts but for all vertices of the reference mesh. The values determined for each vertex of the 3D mesh can be projected onto a 2D plane using a cylindrical projection. Similar to the previous approach, we can train a model to predict the relevant attributes based on the projected 2D coordinates and the process parameters. We use a regression method based on random forests, but the principle remains the same as with neural networks. Fig. N shows the deviation and thickness values predicted by the model compared to the ground truth.\n\n\n\n\n\n\nFigure 3.5: Predicted values compared to the ground truth for the cylindrical projection (Experiment 742).\n\n\n\n\n\n3.2.3.3 3D approach\nproviding the x,y,z coordinates of the mesh elements and corresponding properties: thickness, thinning and deviation, the trained network could provide a good estimation of these properties.\nsimilar approach can be applied to the 3D mesh. We have access to the x-, y-, and z-coordinates of the center of each face of the reference mesh as well as the process parameters. These can be used similarly to the 1D cuts to predict the deviation at specific points of the reference mesh (but not the deformed meshes obtained after the FEM simulation, as these have a different number of vertices). The size of the dataset for each drawing depth “Zt” is described in the following table:\n\n\n\n\n\n\n\n\n\nZt\nNumber of faces per mesh\nNumber of simulations\nDataset size\n\n\n\n\n30\n26,759\n500\n13,379,500\n\n\n50\n28,587\n250\n7,146,750\n\n\n70\n31,976\n250\n7,994,000",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "3-Methods.html#multi-step-deformation",
    "href": "3-Methods.html#multi-step-deformation",
    "title": "3  Methods",
    "section": "3.3 Multi Step Deformation",
    "text": "3.3 Multi Step Deformation\nThis chapter explores the multi-step deformation (MSD) process of a cuboid that is fixed at the bottom while sequential force vectors are applied to the top. The objective is to achieve a desired shape through a series of deformations. Here, I used Reinforcement Learning (RL) because it excels in sequential decision-making and learning optimal strategies through trial and error, making it ideal for optimizing the deformation sequence to achieve precise outcomes. World Models, such as PlaNet or Dreamer, are utilized because they effectively learn compact representations of the environment dynamics, enabling efficient planning and control. By combining RL with these World Models, we can accurately predict deformation outcomes and optimize the sequence of applied forces, ensuring a smooth and continuous transformation of the initial cuboid into the desired shape.\n\n\n\n\n\n\nFigure 3.6: deformed mesh in 5 steps\n\n\n\nAs we need to keep track of the state changes, the predictor NN should contain the state information.\nThe Cross Entropy Method (CEM) is employed to identify the optimal sequence of force vectors (actions) through several key steps. At first, a distribution over possible force vectors is initialized. From this distribution, a set of action sequences is sampled. Each sequence is then evaluated by simulating the deformation process using the neural network, with performance measured by comparing the deformed shape to the desired shape. Based on these evaluations, the distribution is updated to focus on better-performing sequences, increasing the likelihood of sampling optimal sequences in subsequent iterations. This iterative process continues until the deformation sequence converges to an optimal solution.\nThe optimization continues until a convergence criterion is met. The criterion is defined based on the similarity between the deformed and the target shape. Once convergence is achieved, the final sequence of force vectors represents the optimal strategy for achieving the desired deformation. First we tried 2D approach and then continued with both mesh and implicit representation of the 3D and compare the methods.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "3-Methods.html#reinforcement-learning",
    "href": "3-Methods.html#reinforcement-learning",
    "title": "3  Methods",
    "section": "3.4 Reinforcement Learning",
    "text": "3.4 Reinforcement Learning\nReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative rewards. Unlike supervised learning, where the model learns from labeled data, RL relies on the trial-and-error approach, continuously interacting with the environment to improve its policy.\nKey Concepts in Reinforcement Learning:\nAgent: The learner or decision maker.\nEnvironment: Everything the agent interacts with.\nState (s): A representation of the current situation of the agent in the environment.\nAction (a): All possible moves the agent can take.\nReward (r): Feedback from the environment based on the action taken by the agent.\nPolicy (π): A strategy that defines the agent’s action selection, mapping states to actions.\nValue Function (V): Estimates the expected reward for an agent starting from a state and following a policy.\nQ-Value (Q): Estimates the expected reward for taking an action in a state and following a policy thereafter.\nThe RL process begins with the agent starting with an initial policy, which is often random. The agent then interacts with the environment by taking actions based on this policy. After each action, the agent receives feedback in the form of a reward and observes the new state. Using this feedback, the agent updates its policy, aiming to maximize future rewards. This process repeats iteratively, with the policy improving over time as the agent gains more experience.\nReinforcement Learning can be categorized into model-based and model-free approaches. Model-based RL involves learning a model of the environment’s dynamics to plan and make decisions, while model-free RL relies solely on learning from interactions with the environment without an explicit model. Several algorithms help in learning the optimal policy in these approaches. Q-Learning is a value-based method where the agent learns the Q-values for state-action pairs and uses them to make decisions. Deep Q-Networks (DQN) combine Q-learning with deep neural networks to handle high-dimensional state spaces. Policy Gradient Methods directly optimize the policy by adjusting it in the direction that increases expected rewards. Actor-Critic Methods combine value-based and policy-based methods, with an actor making decisions and a critic evaluating them.\n\n3.4.1 World Models in Reinforcement Learning\nWorld models are an advanced concept in reinforcement learning that involve creating an internal model of the environment. This model allows the agent to predict future states and outcomes based on its current state and actions, effectively simulating the environment internally. By leveraging world models, agents can plan and make decisions more efficiently, even in complex and high-dimensional spaces.\nWorld models offer several advantages in reinforcement learning. By simulating the environment internally, agents can learn and plan more efficiently, reducing the need for extensive real-world interactions. This internal simulation enables better exploration strategies, allowing agents to evaluate the consequences of exploratory actions without performing them in the real environment. World models are particularly useful in handling complex environments with high-dimensional state spaces or intricate dynamics, where model-free methods might struggle.\nApplications of world models span various domains, including robotics, where they are used for planning and control tasks that require an understanding of the environment’s dynamics; autonomous driving, where they help predict and react to the behavior of other vehicles and pedestrians; and game playing, where they enhance performance in complex strategy games by simulating future game states and planning accordingly.\nThe workflow of a world model-based agent involves data collection through interactions with the environment, model learning to develop perception, transition, and reward models, state inference using the perception module, planning by simulating future states and rewards, and action execution in the real environment. By integrating world models into reinforcement learning, agents achieve more robust and intelligent behavior, making them well-suited for complex and dynamic tasks.\n\n\n\n\n\n\nFigure 3.7: image caption\n\n\n\n\n\n3.4.2 Perception Module\nThe perception module processes raw sensory inputs from the environment and transforms them into more compact and useful representations. This typically involves using neural networks such as autoencoders. By efficiently encoding sensory information, the perception module helps the agent understand and navigate its environment more effectively.\n\n\n3.4.3 Transition Model\nThe transition model, also known as the dynamics model, predicts the next state given the current state and action. It captures the temporal dynamics of the environment and can be implemented using various types of neural networks, including state space models. This model allows the agent to simulate future states and understand how its actions affect the environment over time.\n\n\n3.4.4 Reward Model\nThe reward model predicts the expected reward for a given state-action pair. By understanding the reward structure of the environment, the agent can better evaluate the potential benefits of different actions. This helps the agent to make decisions that maximize long-term rewards, guiding it towards achieving its goals more effectively.\n\n\n3.4.5 Planning Module\nWith the internal model of the environment, the agent can simulate different action sequences to plan its moves. This planning is often done using model-based algorithms such as Monte Carlo Tree Search (MCTS), trajectory optimization methods, or the Cross Entropy Method (CEM). MCTS explores possible future states by building a search tree, trajectory optimization refines action sequences to maximize rewards, and CEM generates and evaluates a population of action sequences, iteratively refining them based on the best performers. By utilizing these methods, the agent can effectively predict future states and rewards, allowing it to choose actions that optimize its long-term success.\n\n\n\n\n\n\nFigure 3.8: image caption\n\n\n\nCaption: Formulas representing key components of world models in reinforcement learning, including\nthe perception module: \nz = f_\\text{perception}(x)\n\nthe transition model : \ns_{t+1} = f_\\text{transition}(s_t, a_t)\n\nthe reward model : \nr_t = f_\\text{reward}(s_t, a_t)\n , and the planning module : \n\\text{Best Action Sequence} = \\arg\\max_{a_1, a_2, \\ldots, a_T} \\sum_{t=1}^{T} f_\\text{reward}(s_t, a_t)\n\n\n\n3.4.6 Problem Definition in RL\nstatet : the cuboid shape in time t\nactiont : force vector\nstatet+1 : the cuboid shape in time t+1\nand The transition function that models how the state changes due to the applied action:\n\n\\text{state}_{t+1} = f_{\\text{transition}}(\\text{state}_t, \\text{action}_t)\n\nI solved the problem with 2D and 3D approaches, that will be explained in the next sections.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "3-Methods.html#d-approach---image-based",
    "href": "3-Methods.html#d-approach---image-based",
    "title": "3  Methods",
    "section": "3.5 2D Approach - Image based",
    "text": "3.5 2D Approach - Image based\nAs the cuboid is only subjected to force from the top, an image of the top surface can adequately represent the entire shape. For this purpose, I selected the top vertices of the mesh and used the ‘Kriging’ method from Openturns library Baudin et al. (2015) to interpolate the z-values across the entire surface. Kriging Cressie (2015) also known as “Gaussian process regression”, is a method of interpolation based on a Gaussian process governed by prior covariances. It is widely used in spatial analysis and computer experiments to predict values of a spatially distributed variable from sparse data points. Under suitable assumptions of the prior, Kriging provides the Best Linear Unbiased Prediction (BLUP) at unsampled locations, outperforming other interpolation methods based on criteria such as smoothness (e.g., smoothing spline). By considering both the distance and the degree of variation between known data points, Kriging generates a smooth, continuous surface that accurately reflects underlying spatial trends, making it ideal for interpolating from sparse pixels to a smooth image. [image size: 20x50]\nOrdinary Kriging has been used here because it assumes the process mean to be constant and unknown. By utilizing a covariance model (specifically, the SquaredExponential covariance model) and the openturns Kriging algorithm, the Kriging metamodel can be fitted using the provided coordinates and observations. These characteristics align well with the Ordinary Kriging method, as accurate spatial data estimations can be provided. Originally developed for geostatistical applications, Kriging is a versatile statistical interpolation method used across various disciplines for sampled data from random fields that meet certain mathematical assumptions. It is particularly useful for estimating data in the spatial gaps between measured points, whether the data is collected in 2D or 3D.\n\n\n\nthe depth image of the top surface (left) top vertices visualized on top surface as circles (right) the color shows the depth (z)\n\n\n\n\n\n\n\n\nFigure 3.9: the regular grid interpolated using kriging method\n\n\n\n\n\n\n\n\n\nFigure 3.10: the deformed shape and its corresponding top image\n\n\n\n\n3.5.1 World Model\nthe world model is our predictor, in a latent space domain. the WMs can be trained offline or online to reflect the effect of the Force (as action) on the compressed version of the state (shape). first I compressed the data utilizing a Conditional ConvolutionalAutoencoder. My model includes an encoder-decoder structure, where the encoder comprises two convolutional layers with ReLU activation functions and max pooling operations to progressively reduce the spatial dimensions while increasing feature depth. The latent space is further manipulated through linear layers that incorporate external conditioning information. The decoder then reconstructs the original input using two deconvolutional layers, aiming to produce an output that closely resembles the initial data. The CCAE is trained to minimize the reconstruction loss, making it suitable for image compression.\nn_latent: compression rate ##todo the best parameters - reported from HPO : …\n\n\n\nfrom initial state to final state - NN prediction of states\n\n\nthe proposed approach is limited to top surface and can not effectively generalize to 3D. however has an acceptable accuracy and performance and could be applied for similar problems such as [# todo examples] where the 2D prediction would be sufficient.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "3-Methods.html#d-approach---vertex-based",
    "href": "3-Methods.html#d-approach---vertex-based",
    "title": "3  Methods",
    "section": "3.6 3D Approach - Vertex based",
    "text": "3.6 3D Approach - Vertex based\ninstead of converting the top surface to the image, we could directly work on 3D coordinates of vertices on top. in this approach, the NN input would be the same as number of top vertices coordinates (x,y,z) , so usually we could have a smaller NN. as the order of NN inputs are important, we need to keep the topology fixed and assure that the order of vertices is remained fix.\nHere I -manually- kept the topology fixed. therefore, my vertices have index and they are ordered. so I trained a simple MLP that directly receives the xyz coordinates of the top vertices and the action , to predict the next placement of vectors. as in real, it rarely happens that we have a mesh dataset with identical topology for all samples, that would be an easy and low-cost approach if needed. the same as image approach, all data ( here vertices’ coordinates ) corresponding to each state, is fed to the network at once. so the network has a “global” view of each state.\nas the data size is relatively small, the vertex positions could be applied in RL loop directly. the network has the size of - - - layers , optimized by –. error plots… #todo",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "3-Methods.html#d-approach---mesh-based",
    "href": "3-Methods.html#d-approach---mesh-based",
    "title": "3  Methods",
    "section": "3.7 3D Approach - Mesh based",
    "text": "3.7 3D Approach - Mesh based\nFor processing the mesh data, we needed to compress the meshes utilizing the well-known CoMA Ranjan et al. (2018) generating model. the Mesh autoencoder is trained to minimize the reconstruction error on our mesh dataset and using HPO, the best set of parameters are selected. So the trained encoder is now utilized to encode all meshes in the dataset in offline mode.\n#todo HPO parameters , mish activation function, compression rate\nThe encoded meshes are again collected in a form of a dataset to train the WM. During the training of the World Model, the neural network is fed with [State_t, action_t] as input, to predict the State_t+1. please notice that the state is the encoded shape or the so called latent representation of the mesh.\nthe same as before, the PlaNet is used here, this time for encoded mesh representation. \n\\text{NN}([\\text{State}_{t}, \\text{action}_t]) \\rightarrow \\text{State}_{t+1}\n\n\n3.7.1 Challenges of working with meshes\nWorking with meshes in combination with neural networks presents several fundamental challenges that we address: The first challenge is the lack of regular structure in mesh vertices, unlike the pixels in an image. Mesh vertices are scattered in three-dimensional space and cannot be easily vectorized from the top-left to the bottom-right like image pixels for input into the network. The second challenge is that the mesh structure must be presented to the neural network all at once, causing the network to become significantly large when dealing with a large mesh with many vertices, thereby increasing the number of learning parameters. The third challenge is that maintaining the mesh topology is very difficult and in many cases seems impossible. In tasks like physical simulation and finite element analysis, the resulting mesh needs to be re-meshed for reprocessing, which alters the mesh topology. (In my case however, I had to preserve the mesh topology manually!) Therefore, using alternative methods that can overcome these problems can be very helpful. In the next section, we introduce implicit methods that can serve as a good alternative to meshes and address these issues.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "3-Methods.html#d-approach---implicit-data",
    "href": "3-Methods.html#d-approach---implicit-data",
    "title": "3  Methods",
    "section": "3.8 3D Approach - Implicit data",
    "text": "3.8 3D Approach - Implicit data\nFor SDF representation, a fixed grid is established, and the distance from the surface is calculated for all meshes within this grid. This grid is randomly sampled in the space surrounding the shape, both inside and near the surface. The fixed grid is essential for tracking distance values, as the network needs to be aware of the current state to predict the subsequent state. The inputs to the network are the xyz coordinates of the query point, the current SDF value at that point (SDFt), and actiont (a force vector) as a condition, with the output being SDFt+1.\n\n\n\n\n\n\nFigure 3.11: image caption\n\n\n\nFor visualization and evaluation, error metrics are used to compare predicted values with target values in the test set. The test set includes various mesh sequences selected from the dataset to assess our method. Since the current SDF for each point is necessary to predict the next SDF value, the process starts from the initial (undeformed) mesh to generate SDF values sequentially. Initially, an SSD (Single-Step Deformation) Network is used, followed by MSD (Multi-Step Deformation) Networks to predict subsequent SDF values.\nThis SSD Network is trained on undeformed mesh samples and are the same as model described in [chapter X]. the only difference is that, the query points are chosen from our fixed grid point. also a new input (sdft) is fed to the network to represent the current sdf of the query point.\n\n3.8.1 Challenges of working with Implicit data\nuse of implicit data has a great benefit over explicit methods in applications such as reconstruction, classification etc. they have a good binding with NNs and small network size for embedding and encoding of 3d shapes. however in context of deformation, we need to have a global understanding of the shape - or state - to generalize the action ( Force ) effect on the whole body. as the data unit provided to the network is related to one point ( and not the whole shape ), here the network has a partial observation of the state each time.\n\n\n\n\nBaudin, M., Dutfoy, A., Iooss, B., and Popelin, A.-L. (2015). Open TURNS: An industrial software for uncertainty quantification in simulation. arXiv preprint arXiv:1501.05242.\n\n\nCressie, N. (2015). Statistics for spatial data. John Wiley & Sons.\n\n\nRanjan, A., Bolkart, T., Sanyal, S., and Black, M. J. (2018). Generating 3D faces using convolutional mesh autoencoders. in Proceedings of the european conference on computer vision (ECCV), 704–720.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "5-Discussion.html",
    "href": "5-Discussion.html",
    "title": "5  Discussion",
    "section": "",
    "text": "Given the increasing complexity of systems and tools, and the need to construct and measure more intricate structures, there is a growing necessity to revisit and refine our methods. The finite element method (FEM), as one of the most powerful and practical tools available, has shown remarkable capabilities in solving current problems. However, it seems that we are on the verge of a significant transformation in this field. The industry is increasingly leaning towards alternative processing methods such as meshfree techniques, which are becoming more prevalent and continually advancing.\nThe ability of implicit methods to learn in a continuous rather than discrete space provides greater flexibility in solving these problems. Moreover, effectively combining these methods with neural networks can lead to more efficient use of intelligent techniques. Neural networks have significant potential in solving physical problems due to their ability to encode data and function as a mapping between different spaces.\nConsidering that there has been little similar work in this area so far, it is hoped that the ideas presented in this thesis can contribute modestly to the development of such models for the intended applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "Baudin, M., Dutfoy, A., Iooss, B., and Popelin, A.-L. (2015). Open\nTURNS: An industrial software for uncertainty quantification in\nsimulation. arXiv preprint arXiv:1501.05242.\n\n\nCressie, N. (2015). Statistics for spatial data. John Wiley\n& Sons.\n\n\nFeng, Y., Feng, Y., You, H., Zhao, X., and Gao, Y. (2019). Meshnet: Mesh\nneural network for 3d shape representation. in Proceedings of the\nAAAI conference on artificial intelligence, 8279–8286.\n\n\nFey, M., Lenssen, J. E., Weichert, F., and Müller, H. (2018). Splinecnn:\nFast geometric deep learning with continuous b-spline kernels. in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 869–877.\n\n\nHanocka, R., Hertz, A., Fish, N., Giryes, R., Fleishman, S., and\nCohen-Or, D. (2019). Meshcnn: A network with an edge. ACM\nTransactions on Graphics (ToG) 38, 1–12.\n\n\nPark, J. J., Florence, P., Straub, J., Newcombe, R., and Lovegrove, S.\n(2019). Deepsdf: Learning continuous signed distance functions for shape\nrepresentation. in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 165–174.\n\n\nRaissi, M., Perdikaris, P., and Karniadakis, G. E. (2019).\nPhysics-informed neural networks: A deep learning framework for solving\nforward and inverse problems involving nonlinear partial differential\nequations. Journal of Computational physics 378, 686–707.\n\n\nRanjan, A., Bolkart, T., Sanyal, S., and Black, M. J. (2018). Generating\n3D faces using convolutional mesh autoencoders. in Proceedings of\nthe european conference on computer vision (ECCV), 704–720.\n\n\nZhang, M., Abidin, A. R. Z., and Tan, C. S. (2024). State-of-the-art\nreview on meshless methods in the application of crack problems.\nTheoretical and Applied Fracture Mechanics, 104348.",
    "crumbs": [
      "References"
    ]
  }
]