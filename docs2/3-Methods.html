<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Title of the thesis :D - 3&nbsp; Methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./4-Results.html" rel="next">
<link href="./2-StateOfTheArt.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./3-Methods.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Methods</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./img/tuc.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Title of the thesis :D</a> 
        <div class="sidebar-tools-main">
    <a href="./thesis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Abstract</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-StateOfTheArt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">State of the art</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-Methods.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-Results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Results</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-Discussion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Discussion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#dataset" id="toc-dataset" class="nav-link active" data-scroll-target="#dataset"><span class="header-section-number">3.1</span> Dataset</a>
  <ul class="collapse">
  <li><a href="#fcad-data-version-1-first-beam" id="toc-fcad-data-version-1-first-beam" class="nav-link" data-scroll-target="#fcad-data-version-1-first-beam"><span class="header-section-number">3.1.1</span> FCAD data version 1 (first beam)</a></li>
  <li><a href="#version-2-press-on-top" id="toc-version-2-press-on-top" class="nav-link" data-scroll-target="#version-2-press-on-top"><span class="header-section-number">3.1.2</span> version 2 (press on top)</a></li>
  <li><a href="#d-images-kriging" id="toc-d-images-kriging" class="nav-link" data-scroll-target="#d-images-kriging"><span class="header-section-number">3.1.3</span> 2D images (kriging)</a></li>
  <li><a href="#sdf-datasets" id="toc-sdf-datasets" class="nav-link" data-scroll-target="#sdf-datasets"><span class="header-section-number">3.1.4</span> SDF datasets</a></li>
  </ul></li>
  <li><a href="#single-step-deformation" id="toc-single-step-deformation" class="nav-link" data-scroll-target="#single-step-deformation"><span class="header-section-number">3.2</span> Single Step Deformation</a>
  <ul class="collapse">
  <li><a href="#ssd-on-watertight-mesh---data-preparation" id="toc-ssd-on-watertight-mesh---data-preparation" class="nav-link" data-scroll-target="#ssd-on-watertight-mesh---data-preparation"><span class="header-section-number">3.2.1</span> SSD on Watertight Mesh - Data preparation</a></li>
  <li><a href="#training-neural-network" id="toc-training-neural-network" class="nav-link" data-scroll-target="#training-neural-network"><span class="header-section-number">3.2.2</span> Training Neural Network</a></li>
  <li><a href="#ssd-on-non-watertight-mesh" id="toc-ssd-on-non-watertight-mesh" class="nav-link" data-scroll-target="#ssd-on-non-watertight-mesh"><span class="header-section-number">3.2.3</span> SSD on non-Watertight Mesh</a></li>
  </ul></li>
  <li><a href="#multi-step-deformation" id="toc-multi-step-deformation" class="nav-link" data-scroll-target="#multi-step-deformation"><span class="header-section-number">3.3</span> Multi Step Deformation</a></li>
  <li><a href="#reinforcement-learning" id="toc-reinforcement-learning" class="nav-link" data-scroll-target="#reinforcement-learning"><span class="header-section-number">3.4</span> Reinforcement Learning</a>
  <ul class="collapse">
  <li><a href="#world-models-in-reinforcement-learning" id="toc-world-models-in-reinforcement-learning" class="nav-link" data-scroll-target="#world-models-in-reinforcement-learning"><span class="header-section-number">3.4.1</span> World Models in Reinforcement Learning</a></li>
  <li><a href="#perception-module" id="toc-perception-module" class="nav-link" data-scroll-target="#perception-module"><span class="header-section-number">3.4.2</span> Perception Module</a></li>
  <li><a href="#transition-model" id="toc-transition-model" class="nav-link" data-scroll-target="#transition-model"><span class="header-section-number">3.4.3</span> Transition Model</a></li>
  <li><a href="#reward-model" id="toc-reward-model" class="nav-link" data-scroll-target="#reward-model"><span class="header-section-number">3.4.4</span> Reward Model</a></li>
  <li><a href="#planning-module" id="toc-planning-module" class="nav-link" data-scroll-target="#planning-module"><span class="header-section-number">3.4.5</span> Planning Module</a></li>
  <li><a href="#problem-definition-in-rl" id="toc-problem-definition-in-rl" class="nav-link" data-scroll-target="#problem-definition-in-rl"><span class="header-section-number">3.4.6</span> Problem Definition in RL</a></li>
  </ul></li>
  <li><a href="#d-approach---image-based" id="toc-d-approach---image-based" class="nav-link" data-scroll-target="#d-approach---image-based"><span class="header-section-number">3.5</span> 2D Approach - Image based</a>
  <ul class="collapse">
  <li><a href="#world-model" id="toc-world-model" class="nav-link" data-scroll-target="#world-model"><span class="header-section-number">3.5.1</span> World Model</a></li>
  </ul></li>
  <li><a href="#d-approach---vertex-based" id="toc-d-approach---vertex-based" class="nav-link" data-scroll-target="#d-approach---vertex-based"><span class="header-section-number">3.6</span> 3D Approach - Vertex based</a></li>
  <li><a href="#d-approach---mesh-based" id="toc-d-approach---mesh-based" class="nav-link" data-scroll-target="#d-approach---mesh-based"><span class="header-section-number">3.7</span> 3D Approach - Mesh based</a>
  <ul class="collapse">
  <li><a href="#challenges-of-working-with-meshes" id="toc-challenges-of-working-with-meshes" class="nav-link" data-scroll-target="#challenges-of-working-with-meshes"><span class="header-section-number">3.7.1</span> Challenges of working with meshes</a></li>
  </ul></li>
  <li><a href="#d-approach---implicit-data" id="toc-d-approach---implicit-data" class="nav-link" data-scroll-target="#d-approach---implicit-data"><span class="header-section-number">3.8</span> 3D Approach - Implicit data</a>
  <ul class="collapse">
  <li><a href="#challenges-of-working-with-implicit-data" id="toc-challenges-of-working-with-implicit-data" class="nav-link" data-scroll-target="#challenges-of-working-with-implicit-data"><span class="header-section-number">3.8.1</span> Challenges of working with Implicit data</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-methods" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Methods</span></span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<section id="dataset" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="dataset"><span class="header-section-number">3.1</span> Dataset</h2>
<p>To train a neural network effectively, the first essential component is a dataset. A dataset provides the necessary examples for the network to learn patterns and make accurate predictions. In the context of 3D shape deformation and FEM simulations, having a comprehensive and high-quality dataset is crucial for the network to understand the complex relationships involved in deformation processes.</p>
<p>Unfortunately, there are no public datasets specifically for 3D deformation available. Additionally, our project partner had a limited number of mesh simulations, which required significant time to generate. Consequently, I needed to create my own dataset to test and train my models. This would allow me to proceed with the project and later apply the developed models to the partner’s data.</p>
<p>When searching for deformation and FEM examples, one commonly encountered scenario is a beam fixed at one end, undergoing bending and deformation when a force is applied.</p>
<div id="fig-fem-ansys" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fem-ansys-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./img/Chp1/Ansys_beam.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fem-ansys-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Beam Structure from Ansys Website
</figcaption>
</figure>
</div>
<p>This classic example inspired my dataset design. I modeled a slender, rectangular cuboid beam, fixed at both ends, with a point force applied from various directions to induce deformation.</p>
<div id="fig-fem-ds1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fem-ds1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./img/Chp1/deformation_dataset.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fem-ds1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Dataset samples - first version
</figcaption>
</figure>
</div>
<p>The resulting dataset consisted of approximately 6300 mesh simulations, capturing the deformations caused by different force vectors at various points on top or bottom surface. This extensive dataset was suitable for training the neural network. I used FreeCAD software for this purpose, with detailed steps and methodologies described in section -XYZ- of this thesis.</p>
<p>By generating this dataset, I ensured that my models had sufficient data to learn from, allowing for robust training and validation. This foundation enabled me to develop and refine the models before applying them to the partner’s specific data, ensuring a smooth transition and effective implementation of AI techniques for 3D shape deformation.</p>
<p>FreeCAD is a general-purpose parametric 3D computer-aided design modeler and a building information modeling software application with finite element method support. I used FreeCAD for several reasons: it is open-source, supports Python scripting, offers an easy setup for FEM, and has a rich forum and community support.</p>
<p>To perform FEM simulations in FreeCAD, I followed these steps:</p>
<p>Geometry Definition: Create the 3D model of the beam. Fixed Constraints: Define the fixed points where the beam is held. Force Constraints: Apply forces at specific points and directions on the beam. (this should change during the data generation loop) Add Material: Assign material properties to the beam. Create FEM Mesh: Generate a triangular mesh for the model. Solving Equations: Use CalculiX to solve the FEM equations and obtain the deformation results.</p>
<section id="fcad-data-version-1-first-beam" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="fcad-data-version-1-first-beam"><span class="header-section-number">3.1.1</span> FCAD data version 1 (first beam)</h3>
<p>The output of FEM is typically a deformed mesh, which is a structure used to represent 3D data. A mesh is a special type of graph characterized by its vertices, edges, and faces, making it an excellent and popular data structure for representing 3D data with various complexities and curvatures, especially useful for depicting deformations.</p>
<p>The output from FreeCAD was a deformed mesh where only the vertex positions differed from the initial mesh. The generated meshes all have the same topology, meaning they have the same number of vertices, and the neighborhood of each vertex remains consistent across all meshes. This consistency in topology ensures that the dataset is suitable for training neural networks, as the structural integrity of the meshes is maintained throughout the simulations. However, as the complexity of the mesh increases, its size also grows, which can make rendering and processing somewhat slower. This trade-off between detail and computational efficiency is a key consideration in the use of meshes for 3D data representation.</p>
</section>
<section id="version-2-press-on-top" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="version-2-press-on-top"><span class="header-section-number">3.1.2</span> version 2 (press on top)</h3>
<p>To display multistep deformation, I decided to generate a new dataset with different conditions. I considered a cube with a larger surface area and shorter height, fixing it from the bottom. Random force vectors were applied to the surface and perpendicular to it at various points with different magnitudes. The deformation results were recorded for each step. Since these forces were applied sequentially to the object, it was necessary to record the name of each mesh and the state before and after the force application, along with the force specifications, in a separate table. This was done using a pandas DataFrame to facilitate easier access to the data files.</p>
</section>
<section id="d-images-kriging" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="d-images-kriging"><span class="header-section-number">3.1.3</span> 2D images (kriging)</h3>
</section>
<section id="sdf-datasets" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="sdf-datasets"><span class="header-section-number">3.1.4</span> SDF datasets</h3>
</section>
</section>
<section id="single-step-deformation" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="single-step-deformation"><span class="header-section-number">3.2</span> Single Step Deformation</h2>
<p>Single-step deformation (SSD) involves predicting the reshaping of an object when a force vector is applied to it at a specific point.</p>
<p>In DeepSDF paper, it was demonstrated that a neural network is capable of encoding a 3D object (and a series of 3D objects). In this scenario, the neural network acts as a signed distance function (SDF), where, after training, the network can be queried to determine the distance of any arbitrary point from the surface. By querying the network for a sufficient number of points, we can generate a point cloud, where the distances of these points from the surface are estimated by the neural network. This point cloud allows us to determine the surface boundary of the desired object.</p>
<p>In this study, each shape is encoded using an autodecoder and fed as a reference to the network, allowing the network to encode a wider range of objects. As shown in the image, at the top, two codes corresponding to two objects (leftmost and rightmost ones) are encoded by the network, forming the shapes of a sofa and a chair. By interpolating these codes, we observe a subtle transition between the sofa and the chair, with the reconstructed shapes visible on top. Instead of applying this concept to classes of different objects, we applied it to deformed versions of one object. This way, the network can generate the desired deformed version of an object based on the code, which represents the force applied to the object.</p>
<p>quarto</p>
<section id="ssd-on-watertight-mesh---data-preparation" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="ssd-on-watertight-mesh---data-preparation"><span class="header-section-number">3.2.1</span> SSD on Watertight Mesh - Data preparation</h3>
<p>In DeepSDF, the object mesh is required to be watertight. which means that the object surface devides the input and output space to distinguish the positive and negative distances from the surface. we used our First Dataset (FCAD Ver1) for training our network. The dataset were a set of triangular meshes, generated by applying various force constraints to a simple beam. To convert the meshes into SDF format, the following steps are required:</p>
<ul>
<li><p><strong>Normalization and Scaling</strong>:</p>
<p>Each 3D mesh is scaled to fit within a unit sphere. This normalization step ensures consistency across different meshes, making the SDF values comparable.</p></li>
<li><p><strong>Virtual Camera Rendering</strong>:</p>
<p>The normalized mesh is virtually rendered from multiple viewpoints. Tipically 100 virtual cameras are placed uniformly on the surface of the unit sphere to capture the shape from different angles.</p></li>
<li><p><strong>Distance Calculation</strong>:</p>
<p>For each viewpoint, the distance from the camera to the closest point on the mesh surface is calculated. This involves projecting points from the 3D space onto the mesh and computing the shortest distance to the mesh triangles.</p></li>
<li><p><strong>Point Sampling</strong>:</p>
<p>Points are sampled more densely near the mesh surface to ensure higher accuracy in regions of interest. In our implementation, we sampled 400,000 points for each shape in the dataset.</p></li>
<li><p><strong>Signed Distance Computation</strong>:</p>
<p>Each sampled point is assigned a signed distance value. The sign indicates whether the point is inside (+) or outside (-) the object, and the magnitude represents the shortest distance to the surface.</p></li>
<li><p><strong>SDF Representation Storage</strong>:</p>
<p>The computed signed distance values for all sampled points are stored, creating a dense representation of the shape’s geometry. which contains x,y,z coordinates of points and the corresponding SDF value.</p></li>
</ul>
<p>after converting all meshes in SDF format, we can create a robust dataset and split it to train/test/validation sets. Our fully-connected neural network, designed with six inputs (spatial coordinates and force vectors), demonstrated the capability to accurately predict the SDF values for given deformation scenarios. This approach offers an efficient and flexible solution for modeling geometric deformations in 3D shapes.</p>
</section>
<section id="training-neural-network" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="training-neural-network"><span class="header-section-number">3.2.2</span> Training Neural Network</h3>
<p>the designed neural network needs to map the 3D points in the space, to a corresponding SDF value, based on a provided force vector. similar to the DeepSDF paper, we choose a Multilayer Preceptron for this purpose. HPO</p>
<p>** effect of different parameters on estimation -&gt; table</p>
<p>activation function</p>
</section>
<section id="ssd-on-non-watertight-mesh" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="ssd-on-non-watertight-mesh"><span class="header-section-number">3.2.3</span> SSD on non-Watertight Mesh</h3>
<p>*** (could be considered as point cloud?!) the data provided by our industrial partner however, was non-watertight. therefore the original DeepSDF method could not be applied there. instead we modify the input and train the neural network. the network could predict the thickness, thinning and deviation with an acceptable error.</p>
<section id="d-approach" class="level4" data-number="3.2.3.1">
<h4 data-number="3.2.3.1" class="anchored" data-anchor-id="d-approach"><span class="header-section-number">3.2.3.1</span> 1D approach</h4>
<div id="fig-karo-1d-1" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-karo-1d-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/Chp1/karo_1D.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-karo-1d-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: image caption
</figcaption>
</figure>
</div>
<div id="fig-karo-1d-2" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-karo-1d-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/Chp1/karo_1D_cut.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-karo-1d-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: a) Position of the x0 cut through the deep-drawn element. b) Deviations from the template for the 880 simulations. The three drawing depths (30, 50, and 70) are shown in different colors.
</figcaption>
</figure>
</div>
</section>
<section id="d-approach-1" class="level4" data-number="3.2.3.2">
<h4 data-number="3.2.3.2" class="anchored" data-anchor-id="d-approach-1"><span class="header-section-number">3.2.3.2</span> 2D approach</h4>
<p>The deviation and thickness values are available not only along the cuts but for all vertices of the reference mesh. The values determined for each vertex of the 3D mesh can be projected onto a 2D plane using a cylindrical projection. Similar to the previous approach, we can train a model to predict the relevant attributes based on the projected 2D coordinates and the process parameters. We use a regression method based on random forests, but the principle remains the same as with neural networks. <strong>Fig. N</strong> shows the deviation and thickness values predicted by the model compared to the ground truth.</p>
<div id="fig-karo-2d-1" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-karo-2d-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/Chp1/karo_2D.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:87.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-karo-2d-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: Predicted values compared to the ground truth for the cylindrical projection (Experiment 742).
</figcaption>
</figure>
</div>
</section>
<section id="d-approach-2" class="level4" data-number="3.2.3.3">
<h4 data-number="3.2.3.3" class="anchored" data-anchor-id="d-approach-2"><span class="header-section-number">3.2.3.3</span> 3D approach</h4>
<p>providing the x,y,z coordinates of the mesh elements and corresponding properties: thickness, thinning and deviation, the trained network could provide a good estimation of these properties.</p>
<p>similar approach can be applied to the 3D mesh. We have access to the x-, y-, and z-coordinates of the center of each face of the reference mesh as well as the process parameters. These can be used similarly to the 1D cuts to predict the deviation at specific points of the reference mesh (but not the deformed meshes obtained after the FEM simulation, as these have a different number of vertices). The size of the dataset for each drawing depth “Zt” is described in the following table:</p>
<table class="table">
<colgroup>
<col style="width: 7%">
<col style="width: 38%">
<col style="width: 33%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Zt</th>
<th>Number of faces per mesh</th>
<th>Number of simulations</th>
<th>Dataset size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>30</td>
<td>26,759</td>
<td>500</td>
<td>13,379,500</td>
</tr>
<tr class="even">
<td>50</td>
<td>28,587</td>
<td>250</td>
<td>7,146,750</td>
</tr>
<tr class="odd">
<td>70</td>
<td>31,976</td>
<td>250</td>
<td>7,994,000</td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="multi-step-deformation" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="multi-step-deformation"><span class="header-section-number">3.3</span> Multi Step Deformation</h2>
<p>This chapter explores the multi-step deformation (MSD) process of a cuboid that is fixed at the bottom while sequential force vectors are applied to the top. The objective is to achieve a desired shape through a series of deformations. Here, I used Reinforcement Learning (RL) because it excels in sequential decision-making and learning optimal strategies through trial and error, making it ideal for optimizing the deformation sequence to achieve precise outcomes. World Models, such as PlaNet or Dreamer, are utilized because they effectively learn compact representations of the environment dynamics, enabling efficient planning and control. By combining RL with these World Models, we can accurately predict deformation outcomes and optimize the sequence of applied forces, ensuring a smooth and continuous transformation of the initial cuboid into the desired shape.</p>
<div id="fig-mesh-states" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mesh-states-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/Chp3/mesh_states.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mesh-states-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.6: deformed mesh in 5 steps
</figcaption>
</figure>
</div>
<p>As we need to keep track of the state changes, the predictor NN should contain the state information.</p>
<p>The Cross Entropy Method (CEM) is employed to identify the optimal sequence of force vectors (actions) through several key steps. At first, a distribution over possible force vectors is initialized. From this distribution, a set of action sequences is sampled. Each sequence is then evaluated by simulating the deformation process using the neural network, with performance measured by comparing the deformed shape to the desired shape. Based on these evaluations, the distribution is updated to focus on better-performing sequences, increasing the likelihood of sampling optimal sequences in subsequent iterations. This iterative process continues until the deformation sequence converges to an optimal solution.</p>
<p>The optimization continues until a convergence criterion is met. The criterion is defined based on the similarity between the deformed and the target shape. Once convergence is achieved, the final sequence of force vectors represents the optimal strategy for achieving the desired deformation. First we tried 2D approach and then continued with both mesh and implicit representation of the 3D and compare the methods.</p>
</section>
<section id="reinforcement-learning" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="reinforcement-learning"><span class="header-section-number">3.4</span> Reinforcement Learning</h2>
<p>Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative rewards. Unlike supervised learning, where the model learns from labeled data, RL relies on the trial-and-error approach, continuously interacting with the environment to improve its policy.</p>
<p>Key Concepts in Reinforcement Learning:</p>
<p><strong>Agent</strong>: The learner or decision maker.</p>
<p><strong>Environment</strong>: Everything the agent interacts with.</p>
<p><strong>State</strong> (s): A representation of the current situation of the agent in the environment.</p>
<p><strong>Action</strong> (a): All possible moves the agent can take.</p>
<p><strong>Reward</strong> (r): Feedback from the environment based on the action taken by the agent.</p>
<p><strong>Policy</strong> (π): A strategy that defines the agent’s action selection, mapping states to actions.</p>
<p><strong>Value Function</strong> (V): Estimates the expected reward for an agent starting from a state and following a policy.</p>
<p><strong>Q-Value</strong> (Q): Estimates the expected reward for taking an action in a state and following a policy thereafter.</p>
<p>The RL process begins with the agent starting with an initial policy, which is often random. The agent then interacts with the environment by taking actions based on this policy. After each action, the agent receives feedback in the form of a reward and observes the new state. Using this feedback, the agent updates its policy, aiming to maximize future rewards. This process repeats iteratively, with the policy improving over time as the agent gains more experience.</p>
<p>Reinforcement Learning can be categorized into model-based and model-free approaches. Model-based RL involves learning a model of the environment’s dynamics to plan and make decisions, while model-free RL relies solely on learning from interactions with the environment without an explicit model. Several algorithms help in learning the optimal policy in these approaches. Q-Learning is a value-based method where the agent learns the Q-values for state-action pairs and uses them to make decisions. Deep Q-Networks (DQN) combine Q-learning with deep neural networks to handle high-dimensional state spaces. Policy Gradient Methods directly optimize the policy by adjusting it in the direction that increases expected rewards. Actor-Critic Methods combine value-based and policy-based methods, with an actor making decisions and a critic evaluating them.</p>
<section id="world-models-in-reinforcement-learning" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="world-models-in-reinforcement-learning"><span class="header-section-number">3.4.1</span> World Models in Reinforcement Learning</h3>
<p>World models are an advanced concept in reinforcement learning that involve creating an internal model of the environment. This model allows the agent to predict future states and outcomes based on its current state and actions, effectively simulating the environment internally. By leveraging world models, agents can plan and make decisions more efficiently, even in complex and high-dimensional spaces.</p>
<p>World models offer several advantages in reinforcement learning. By simulating the environment internally, agents can learn and plan more efficiently, reducing the need for extensive real-world interactions. This internal simulation enables better exploration strategies, allowing agents to evaluate the consequences of exploratory actions without performing them in the real environment. World models are particularly useful in handling complex environments with high-dimensional state spaces or intricate dynamics, where model-free methods might struggle.</p>
<p>Applications of world models span various domains, including robotics, where they are used for planning and control tasks that require an understanding of the environment’s dynamics; autonomous driving, where they help predict and react to the behavior of other vehicles and pedestrians; and game playing, where they enhance performance in complex strategy games by simulating future game states and planning accordingly.</p>
<p>The workflow of a world model-based agent involves data collection through interactions with the environment, model learning to develop perception, transition, and reward models, state inference using the perception module, planning by simulating future states and rewards, and action execution in the real environment. By integrating world models into reinforcement learning, agents achieve more robust and intelligent behavior, making them well-suited for complex and dynamic tasks.</p>
<div id="fig-planet" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-planet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/Chp3/Planet.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-planet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.7: image caption
</figcaption>
</figure>
</div>
</section>
<section id="perception-module" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="perception-module"><span class="header-section-number">3.4.2</span> Perception Module</h3>
<p>The perception module processes raw sensory inputs from the environment and transforms them into more compact and useful representations. This typically involves using neural networks such as autoencoders. By efficiently encoding sensory information, the perception module helps the agent understand and navigate its environment more effectively.</p>
</section>
<section id="transition-model" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="transition-model"><span class="header-section-number">3.4.3</span> Transition Model</h3>
<p>The transition model, also known as the dynamics model, predicts the next state given the current state and action. It captures the temporal dynamics of the environment and can be implemented using various types of neural networks, including state space models. This model allows the agent to simulate future states and understand how its actions affect the environment over time.</p>
</section>
<section id="reward-model" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="reward-model"><span class="header-section-number">3.4.4</span> Reward Model</h3>
<p>The reward model predicts the expected reward for a given state-action pair. By understanding the reward structure of the environment, the agent can better evaluate the potential benefits of different actions. This helps the agent to make decisions that maximize long-term rewards, guiding it towards achieving its goals more effectively.</p>
</section>
<section id="planning-module" class="level3" data-number="3.4.5">
<h3 data-number="3.4.5" class="anchored" data-anchor-id="planning-module"><span class="header-section-number">3.4.5</span> Planning Module</h3>
<p>With the internal model of the environment, the agent can simulate different action sequences to plan its moves. This planning is often done using model-based algorithms such as Monte Carlo Tree Search (MCTS), trajectory optimization methods, or the Cross Entropy Method (CEM). MCTS explores possible future states by building a search tree, trajectory optimization refines action sequences to maximize rewards, and CEM generates and evaluates a population of action sequences, iteratively refining them based on the best performers. By utilizing these methods, the agent can effectively predict future states and rewards, allowing it to choose actions that optimize its long-term success.</p>
<div id="fig-cem" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/Chp3/CEM_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.8: image caption
</figcaption>
</figure>
</div>
<p><strong>Caption:</strong> Formulas representing key components of world models in reinforcement learning, including</p>
<p>the perception module: <span class="math display">
z = f_\text{perception}(x)
</span></p>
<p>the transition model : <span class="math display">
s_{t+1} = f_\text{transition}(s_t, a_t)
</span></p>
<p>the reward model : <span class="math display">
r_t = f_\text{reward}(s_t, a_t)
</span> , and the planning module : <span class="math display">
\text{Best Action Sequence} = \arg\max_{a_1, a_2, \ldots, a_T} \sum_{t=1}^{T} f_\text{reward}(s_t, a_t)
</span></p>
</section>
<section id="problem-definition-in-rl" class="level3" data-number="3.4.6">
<h3 data-number="3.4.6" class="anchored" data-anchor-id="problem-definition-in-rl"><span class="header-section-number">3.4.6</span> Problem Definition in RL</h3>
<p>state<sup>t</sup> : the cuboid shape in time t</p>
<p>action<sup>t</sup> : force vector</p>
<p>state<sup>t+1</sup> : the cuboid shape in time t+1</p>
<p>and The transition function that models how the state changes due to the applied action:</p>
<p><span class="math display">
\text{state}_{t+1} = f_{\text{transition}}(\text{state}_t, \text{action}_t)
</span></p>
<p>I solved the problem with 2D and 3D approaches, that will be explained in the next sections.</p>
</section>
</section>
<section id="d-approach---image-based" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="d-approach---image-based"><span class="header-section-number">3.5</span> 2D Approach - Image based</h2>
<p>As the cuboid is only subjected to force from the top, an image of the top surface can adequately represent the entire shape. For this purpose, I selected the top vertices of the mesh and used the ‘Kriging’ method from Openturns library <span class="citation" data-cites="baudin2015open">Baudin et al. (<a href="References.html#ref-baudin2015open" role="doc-biblioref">2015</a>)</span> to interpolate the z-values across the entire surface. Kriging <span class="citation" data-cites="cressie2015statistics">Cressie (<a href="References.html#ref-cressie2015statistics" role="doc-biblioref">2015</a>)</span> also known as “Gaussian process regression”, is a method of interpolation based on a Gaussian process governed by prior covariances. It is widely used in spatial analysis and computer experiments to predict values of a spatially distributed variable from sparse data points. Under suitable assumptions of the prior, Kriging provides the Best Linear Unbiased Prediction (BLUP) at unsampled locations, outperforming other interpolation methods based on criteria such as smoothness (e.g., smoothing spline). By considering both the distance and the degree of variation between known data points, Kriging generates a smooth, continuous surface that accurately reflects underlying spatial trends, making it ideal for interpolating from sparse pixels to a smooth image. [image size: 20x50]</p>
<p>Ordinary Kriging has been used here because it assumes the process mean to be constant and unknown. By utilizing a covariance model (specifically, the SquaredExponential covariance model) and the openturns Kriging algorithm, the Kriging metamodel can be fitted using the provided coordinates and observations. These characteristics align well with the Ordinary Kriging method, as accurate spatial data estimations can be provided. Originally developed for geostatistical applications, Kriging is a versatile statistical interpolation method used across various disciplines for sampled data from random fields that meet certain mathematical assumptions. It is particularly useful for estimating data in the spatial gaps between measured points, whether the data is collected in 2D or 3D.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/Chp3/kriging_1.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>the depth image of the top surface (left) top vertices visualized on top surface as circles (right) the color shows the depth (z)</figcaption>
</figure>
</div>
<div id="fig-krig2" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-krig2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/Chp3/kriging_2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-krig2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.9: the regular grid interpolated using kriging method
</figcaption>
</figure>
</div>
<div id="fig-mesh-krig" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mesh-krig-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/Chp3/mesh_krig.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mesh-krig-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.10: the deformed shape and its corresponding top image
</figcaption>
</figure>
</div>
<section id="world-model" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="world-model"><span class="header-section-number">3.5.1</span> World Model</h3>
<p>the world model is our predictor, in a latent space domain. the WMs can be trained offline or online to reflect the effect of the Force (as action) on the compressed version of the state (shape). first I compressed the data utilizing a Conditional ConvolutionalAutoencoder. My model includes an encoder-decoder structure, where the encoder comprises two convolutional layers with ReLU activation functions and max pooling operations to progressively reduce the spatial dimensions while increasing feature depth. The latent space is further manipulated through linear layers that incorporate external conditioning information. The decoder then reconstructs the original input using two deconvolutional layers, aiming to produce an output that closely resembles the initial data. The CCAE is trained to minimize the reconstruction loss, making it suitable for image compression.</p>
<p>n_latent: compression rate ##todo the best parameters - reported from HPO : …</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/Chp3/cem_2d_seq.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>from initial state to final state - NN prediction of states</figcaption>
</figure>
</div>
<p>the proposed approach is limited to top surface and can not effectively generalize to 3D. however has an acceptable accuracy and performance and could be applied for similar problems such as [# todo examples] where the 2D prediction would be sufficient.</p>
</section>
</section>
<section id="d-approach---vertex-based" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="d-approach---vertex-based"><span class="header-section-number">3.6</span> 3D Approach - Vertex based</h2>
<p>instead of converting the top surface to the image, we could directly work on 3D coordinates of vertices on top. in this approach, the NN input would be the same as number of top vertices coordinates (x,y,z) , so usually we could have a smaller NN. as the order of NN inputs are important, we need to keep the topology fixed and assure that the order of vertices is remained fix.</p>
<p>Here I -manually- kept the topology fixed. therefore, my vertices have index and they are ordered. so I trained a simple MLP that directly receives the xyz coordinates of the top vertices and the action , to predict the next placement of vectors. as in real, it rarely happens that we have a mesh dataset with identical topology for all samples, that would be an easy and low-cost approach if needed. the same as image approach, all data ( here vertices’ coordinates ) corresponding to each state, is fed to the network at once. so the network has a “global” view of each state.</p>
<p>as the data size is relatively small, the vertex positions could be applied in RL loop directly. the network has the size of - - - layers , optimized by –. error plots… #todo</p>
</section>
<section id="d-approach---mesh-based" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="d-approach---mesh-based"><span class="header-section-number">3.7</span> 3D Approach - Mesh based</h2>
<p>For processing the mesh data, we needed to compress the meshes utilizing the well-known CoMA <span class="citation" data-cites="ranjan2018generating">Ranjan et al. (<a href="References.html#ref-ranjan2018generating" role="doc-biblioref">2018</a>)</span> generating model. the Mesh autoencoder is trained to minimize the reconstruction error on our mesh dataset and using HPO, the best set of parameters are selected. So the trained encoder is now utilized to encode all meshes in the dataset in offline mode.</p>
<p>#todo HPO parameters , mish activation function, compression rate</p>
<p>The encoded meshes are again collected in a form of a dataset to train the WM. During the training of the World Model, the neural network is fed with [State_t, action_t] as input, to predict the State_t+1. please notice that the state is the encoded shape or the so called latent representation of the mesh.</p>
<p>the same as before, the PlaNet is used here, this time for encoded mesh representation. <span class="math display">
\text{NN}([\text{State}_{t}, \text{action}_t]) \rightarrow \text{State}_{t+1}
</span></p>
<section id="challenges-of-working-with-meshes" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="challenges-of-working-with-meshes"><span class="header-section-number">3.7.1</span> Challenges of working with meshes</h3>
<p>Working with meshes in combination with neural networks presents several fundamental challenges that we address: The first challenge is the lack of regular structure in mesh vertices, unlike the pixels in an image. Mesh vertices are scattered in three-dimensional space and cannot be easily vectorized from the top-left to the bottom-right like image pixels for input into the network. The second challenge is that the mesh structure must be presented to the neural network all at once, causing the network to become significantly large when dealing with a large mesh with many vertices, thereby increasing the number of learning parameters. The third challenge is that maintaining the mesh topology is very difficult and in many cases seems impossible. In tasks like physical simulation and finite element analysis, the resulting mesh needs to be re-meshed for reprocessing, which alters the mesh topology. (In my case however, I had to preserve the mesh topology manually!) Therefore, using alternative methods that can overcome these problems can be very helpful. In the next section, we introduce implicit methods that can serve as a good alternative to meshes and address these issues.</p>
</section>
</section>
<section id="d-approach---implicit-data" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="d-approach---implicit-data"><span class="header-section-number">3.8</span> 3D Approach - Implicit data</h2>
<p>For SDF representation, a fixed grid is established, and the distance from the surface is calculated for all meshes within this grid. This grid is randomly sampled in the space surrounding the shape, both inside and near the surface. The fixed grid is essential for tracking distance values, as the network needs to be aware of the current state to predict the subsequent state. The inputs to the network are the xyz coordinates of the query point, the current SDF value at that point (SDF<sup>t</sup>), and action<sup><sub>t</sub></sup> (a force vector) as a condition, with the output being SDF<sup>t+1</sup>.</p>
<div id="fig-implicit-net" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-implicit-net-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/Chp3/net_sequence.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-implicit-net-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.11: image caption
</figcaption>
</figure>
</div>
<p>For visualization and evaluation, error metrics are used to compare predicted values with target values in the test set. The test set includes various mesh sequences selected from the dataset to assess our method. Since the current SDF for each point is necessary to predict the next SDF value, the process starts from the initial (undeformed) mesh to generate SDF values sequentially. Initially, an SSD (Single-Step Deformation) Network is used, followed by MSD (Multi-Step Deformation) Networks to predict subsequent SDF values.</p>
<p>This SSD Network is trained on undeformed mesh samples and are the same as model described in [chapter X]. the only difference is that, the query points are chosen from our fixed grid point. also a new input (sdft) is fed to the network to represent the current sdf of the query point.</p>
<section id="challenges-of-working-with-implicit-data" class="level3" data-number="3.8.1">
<h3 data-number="3.8.1" class="anchored" data-anchor-id="challenges-of-working-with-implicit-data"><span class="header-section-number">3.8.1</span> Challenges of working with Implicit data</h3>
<p>use of implicit data has a great benefit over explicit methods in applications such as reconstruction, classification etc. they have a good binding with NNs and small network size for embedding and encoding of 3d shapes. however in context of deformation, we need to have a global understanding of the shape - or state - to generalize the action ( Force ) effect on the whole body. as the data unit provided to the network is related to one point ( and not the whole shape ), here the network has a partial observation of the state each time.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-baudin2015open" class="csl-entry" role="listitem">
Baudin, M., Dutfoy, A., Iooss, B., and Popelin, A.-L. (2015). Open TURNS: An industrial software for uncertainty quantification in simulation. <em>arXiv preprint arXiv:1501.05242</em>.
</div>
<div id="ref-cressie2015statistics" class="csl-entry" role="listitem">
Cressie, N. (2015). <em>Statistics for spatial data</em>. John Wiley &amp; Sons.
</div>
<div id="ref-ranjan2018generating" class="csl-entry" role="listitem">
Ranjan, A., Bolkart, T., Sanyal, S., and Black, M. J. (2018). Generating 3D faces using convolutional mesh autoencoders. in <em>Proceedings of the european conference on computer vision (ECCV)</em>, 704–720.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./2-StateOfTheArt.html" class="pagination-link" aria-label="State of the art">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">State of the art</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./4-Results.html" class="pagination-link" aria-label="Results">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Results</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>