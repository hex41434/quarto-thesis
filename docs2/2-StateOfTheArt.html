<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Title of the thesis :D - 2&nbsp; State of the art</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./3-Methods.html" rel="next">
<link href="./1-Introduction.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./2-StateOfTheArt.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">State of the art</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./img/tuc.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Title of the thesis :D</a> 
        <div class="sidebar-tools-main">
    <a href="./thesis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Abstract</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-StateOfTheArt.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">State of the art</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-Methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-Results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Results</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-Discussion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Discussion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#fem-simulations-and-applications" id="toc-fem-simulations-and-applications" class="nav-link active" data-scroll-target="#fem-simulations-and-applications"><span class="header-section-number">2.1</span> FEM Simulations and applications</a>
  <ul class="collapse">
  <li><a href="#current-advancements-and-challenges" id="toc-current-advancements-and-challenges" class="nav-link" data-scroll-target="#current-advancements-and-challenges"><span class="header-section-number">2.1.1</span> Current Advancements and Challenges</a></li>
  </ul></li>
  <li><a href="#replacing-fem-with-ai-approaches" id="toc-replacing-fem-with-ai-approaches" class="nav-link" data-scroll-target="#replacing-fem-with-ai-approaches"><span class="header-section-number">2.2</span> Replacing FEM with AI Approaches</a>
  <ul class="collapse">
  <li><a href="#physics-informed-neural-networks-pinn" id="toc-physics-informed-neural-networks-pinn" class="nav-link" data-scroll-target="#physics-informed-neural-networks-pinn"><span class="header-section-number">2.2.1</span> Physics informed Neural Networks (PINN)</a></li>
  <li><a href="#image-based-fems" id="toc-image-based-fems" class="nav-link" data-scroll-target="#image-based-fems"><span class="header-section-number">2.2.2</span> Image based fems …</a></li>
  </ul></li>
  <li><a href="#introduction-to-deep-neural-networks" id="toc-introduction-to-deep-neural-networks" class="nav-link" data-scroll-target="#introduction-to-deep-neural-networks"><span class="header-section-number">2.3</span> Introduction to Deep Neural Networks</a></li>
  <li><a href="#autoencoders" id="toc-autoencoders" class="nav-link" data-scroll-target="#autoencoders"><span class="header-section-number">2.4</span> Autoencoders</a></li>
  <li><a href="#d-deep-learning" id="toc-d-deep-learning" class="nav-link" data-scroll-target="#d-deep-learning"><span class="header-section-number">2.5</span> 3D Deep Learning</a>
  <ul class="collapse">
  <li><a href="#nn-and-explicit-representations-point-clouds" id="toc-nn-and-explicit-representations-point-clouds" class="nav-link" data-scroll-target="#nn-and-explicit-representations-point-clouds"><span class="header-section-number">2.5.1</span> NN and Explicit representations : Point Clouds</a></li>
  <li><a href="#nn-and-explicit-representations-mesh" id="toc-nn-and-explicit-representations-mesh" class="nav-link" data-scroll-target="#nn-and-explicit-representations-mesh"><span class="header-section-number">2.5.2</span> NN and Explicit representations : Mesh</a></li>
  </ul></li>
  <li><a href="#nn-and-imlicit-representations" id="toc-nn-and-imlicit-representations" class="nav-link" data-scroll-target="#nn-and-imlicit-representations"><span class="header-section-number">2.6</span> NN and Imlicit representations</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-sota" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">State of the art</span></span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<section id="fem-simulations-and-applications" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="fem-simulations-and-applications"><span class="header-section-number">2.1</span> FEM Simulations and applications</h2>
<p>The Finite Element Method (FEM) is a powerful computational technique used for solving complex structural, fluid, and thermal problems in engineering and physical sciences. It works by breaking down a large problem into smaller, simpler parts known as finite elements, and then systematically solving these elements to understand the behavior of the entire system.</p>
<p>FEM is a numerical approximation of the continuous solution field <span class="math inline">u</span> of any partial differential equation (PDE) given by Eq. (1) on a given domain <span class="math inline">\Omega</span> can be performed by various methods. Some of the widely used techniques include finite element method [ref1], finite volume method [ref2], particle methods [ref3], and finite cell method [ref4]. In this contribution, we restrict the discussion to Galerkin-based finite element methods.</p>
<p><span class="math display">
\mathcal{L}(u) = 0 \quad \text{on } \Omega
</span></p>
<p><span class="math display">
u = u_d \quad \text{on } \Gamma_D
</span></p>
<p><span class="math display">
\frac{\partial u}{\partial x} = g \quad \text{on } \Gamma_N
</span></p>
<p>Consider the PDE in Eq. (1) defined on a domain <span class="math inline">\Omega</span> together with the boundary conditions given by Eqs. 2 and 3. Here, <span class="math inline">u_d</span> and <span class="math inline">g</span> are the Dirichlet and Neumann boundary conditions on the respective boundaries. A finite element formulation of Eq. (1) on a discretization of the domain with <span class="math inline">m</span> elements and <span class="math inline">n</span> nodes, together with boundary conditions, will result in the system of equations shown by Eq. (4). We assume all the necessary conditions on the test and trial spaces [ref1] are fulfilled.</p>
<p><span class="math display">
\begin{pmatrix}
k_{1,1} &amp; k_{1,2} &amp; \cdots &amp; k_{1,n} \\
k_{2,1} &amp; k_{2,2} &amp; \cdots &amp; k_{2,n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
k_{n,1} &amp; k_{n,2} &amp; \cdots &amp; k_{n,n}
\end{pmatrix}
\begin{pmatrix}
u_1 \\
u_2 \\
\vdots \\
u_n
\end{pmatrix} =
\begin{pmatrix}
F_1 \\
F_2 \\
\vdots \\
F_n
\end{pmatrix}
</span></p>
<p>In Eq. (4), <span class="math inline">\mathbf{K}(u)</span> is the non-linear left hand side matrix, also called the stiffness matrix. <span class="math inline">\mathbf{u}</span> is the discrete solution field, and <span class="math inline">\mathbf{F}</span> is the right hand side vector. The residual of the system of equations in Eq. (4) can be written as</p>
<p><span class="math display">
r(u^h) = K(u^h)u^h - F
</span></p>
<p>To obtain the solution <span class="math inline">u^h</span>, a Newton–Raphson iteration technique can be employed using the linearization of <span class="math inline">r(u^h)</span> and its tangent matrix. This requires the solution of a linear system of equations in every iteration. These iterations are carried out until the residual norm <span class="math inline">\|r(u^h)\|</span> meets the tolerance requirements. For a detailed discussion of the methodology, readers are referred to [ref2]. For this residual-based formulation, in case of a linear operator <span class="math inline">K</span>, it takes only one iteration to converge. For a large number of elements and nodes, among different steps of the finite element methodology, the most computationally demanding step is the solution of the linear system of equations. In an application where computational efficiency is critical, like real time simulations [ref3] and digital twins [ref4], it is imperative that this step be avoided. Techniques suitable for such applications, like model order reduction [ref5, ref6], construct a surrogate model of Eq. (4) to reduce this cost significantly. Techniques involving neural-networks can completely avoid this cost, but will require a significant amount of training and test data, which is typically generated by simulating the underlying finite element problem. In “Finite element method-enhanced neural network for forward problems” section, we discuss an algorithm that combines residual information from a numerical method to train a neural network for linear PDEs. In this case the residual <span class="math inline">r(u^h)</span> becomes</p>
<p><span class="math display">
r(u^h) = Ku^h - F
</span></p>
<p>FEM is widely used in various industries to simulate physical phenomena and optimize designs, significantly reducing production costs by allowing for virtual testing and refinement before physical prototypes are created. The method gained prominence in the 1960s and has since undergone significant advancements, driven by improvements in computer technology and numerical algorithms. Today, FEM is integral to fields such as aerospace, automotive, civil engineering, and biomedical engineering, offering highly accurate predictions and contributing to innovation and efficiency in product development.</p>
<section id="current-advancements-and-challenges" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="current-advancements-and-challenges"><span class="header-section-number">2.1.1</span> Current Advancements and Challenges</h3>
<p>Despite its significant power and advantages, FEM’s primary focus on mesh construction and solving numerous complex partial differential equations (PDEs) can lead to slow performance when dealing with highly intricate problems. This method demands substantial computational power, and each parameter change necessitates a complete re-execution, demonstrating limited flexibility in adapting to changes. Consequently, scientists are actively seeking to improve and potentially replace FEM with more efficient methods.</p>
<p>Due to the challenges associated with traditional FEM, mesh-free methods have consistently garnered attention. These approaches aim to address FEM’s limitations by offering greater flexibility, faster computations, and the ability to handle complex scenarios more efficiently. However, it is important to note that FEM remains comprehensive and applicable for a wide range of problems, including solid and fluid mechanics, among others. Alternative methods are often tailored to specific types of problems and applications, meaning that there is currently no complete replacement for FEM. Instead, these alternative methods are being developed and refined to provide better and more suitable solutions for specific issues​. for instance in <span class="citation" data-cites="zhang2024state">Zhang et al. (<a href="References.html#ref-zhang2024state" role="doc-biblioref">2024</a>)</span> Mesh-free methods for crack problems have been reviewed.</p>
<p>Among the proposed approaches, the Smoothed Particle Hydrodynamics (SPH) [] , the Element-Free Galerkin Method (EFGM) [] and the Material Point Method (MPM) [] have been notable efforts. SPH employs particles to simulate fluid elements and interactions, proving particularly effective for complex fluid dynamics problems. EFGM, on the other hand, utilizes nodes and shape functions to approximate solutions, providing significant flexibility and accuracy for intricate geometries and boundary conditions. Additionally, MPM represents materials as moving points through a computational grid, making it especially suitable for scenarios involving large deformations and complex material behaviors. These mesh-free approaches offer enhanced adaptability and computational efficiency, addressing some of the core limitations of FEM​​.</p>
<p>In this thesis however, our focus on FEM is specifically related to structural analysis and deformation problems. Among the mesh-free methods introduced in this field, the EFGM and the MPM are particularly relevant. These methods provide promising alternatives to traditional FEM by enhancing computational efficiency and adaptability in solving mechanical and deformation-related problems​.</p>
</section>
</section>
<section id="replacing-fem-with-ai-approaches" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="replacing-fem-with-ai-approaches"><span class="header-section-number">2.2</span> Replacing FEM with AI Approaches</h2>
<p>Similar to other fields, the application of AI in FEM has a relatively long history. While a comprehensive solution to completely replace FEM is yet to be found, AI’s advancements in this area are promising.</p>
<p>in <span class="citation" data-cites="zhang2024state">Zhang et al. (<a href="References.html#ref-zhang2024state" role="doc-biblioref">2024</a>)</span> the Mesh-free methods for crack problems have been reviewed.</p>
<p>with examples such as neural networks for stress analysis, which have been proposed for specific applications[].</p>
<p>For deformation problems, solutions often involve simplifying the problem to 2D images and employing image processing techniques[].</p>
<p>Advancements in replacing the Finite Element Method (FEM) are not confined to a few specific techniques. Instead, various solutions have been proposed depending on the nature of the problem, the type of data, and the complexities involved. These diverse approaches reflect the need for tailored solutions to effectively address the unique challenges presented by different FEM applications.</p>
<p>Physics-Informed Neural Networks (PINNs) are another innovative approach, integrating physical laws into the learning process to solve PDEs. In the following sections, we will review some of the most significant works related to our problem:</p>
<section id="physics-informed-neural-networks-pinn" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="physics-informed-neural-networks-pinn"><span class="header-section-number">2.2.1</span> Physics informed Neural Networks (PINN)</h3>
<p>Physics-Informed Neural Networks (PINNs) <span class="citation" data-cites="raissi2019physics">Raissi et al. (<a href="References.html#ref-raissi2019physics" role="doc-biblioref">2019</a>)</span> are a class of neural networks that integrate physical laws described by partial differential equations (PDEs) into the learning process. They leverage the universal approximation capability of neural networks to solve forward and inverse problems governed by PDEs. The core idea of PINNs is to minimize a loss function that includes both the data-driven error and the residuals of the PDEs, thereby ensuring that the learned solution satisfies the underlying physical laws.</p>
<p>In the context of a PDE, such as <span class="math inline">\mathcal{N}(u(x)) = 0</span>, where <span class="math inline">\mathcal{N}</span> is a differential operator and <span class="math inline">u(x)</span> is the solution, the loss function <span class="math inline">\mathcal{L}</span> for a PINN can be expressed as:</p>
<p><span class="math display">
\mathcal{L} = \mathcal{L}_{\text{data}} + \mathcal{L}_{\text{PDE}}
</span></p>
<p>Here, <span class="math inline">\mathcal{L}_{\text{data}}</span> represents the mean squared error (MSE) between the neural network’s predictions <span class="math inline">u_{\theta}(x)</span> and the observed data points <span class="math inline">u_{\text{obs}}(x)</span>:</p>
<p><span class="math display">
\mathcal{L}_{\text{data}} = \frac{1}{N} \sum_{i=1}^{N} \left( u_{\theta}(x_i) - u_{\text{obs}}(x_i) \right)^2
</span></p>
<p>The term <span class="math inline">\mathcal{L}_{\text{PDE}}</span> enforces the PDE constraints by computing the MSE of the residuals at collocation points <span class="math inline">x_c</span>:</p>
<p><span class="math display">
\mathcal{L}_{\text{PDE}} = \frac{1}{M} \sum_{j=1}^{M} \left( \mathcal{N}(u_{\theta}(x_{c_j})) \right)^2
</span></p>
<p>By optimizing the combined loss <span class="math inline">\mathcal{L}</span>, the neural network is trained to produce a solution that fits the observed data while also satisfying the physical constraints imposed by the PDE.</p>
<p>PINNs have been successfully applied to various challenging problems, including fluid dynamics, structural mechanics, and electromagnetic simulations. Their ability to incorporate prior physical knowledge directly into the learning process makes them a powerful tool for modeling complex systems where data is scarce or noisy. Additionally, PINNs can be used for solving inverse problems, where the goal is to infer unknown parameters or functions within the PDEs, by including terms in the loss function that account for the discrepancies between the predicted and observed data, as well as the governing physical laws. This versatility highlights the potential of PINNs in enhancing the accuracy and robustness of simulations in scientific and engineering applications.</p>
</section>
<section id="image-based-fems" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="image-based-fems"><span class="header-section-number">2.2.2</span> Image based fems …</h3>
<p>Despite significant advancements in various fields, a comprehensive model specifically designed to address the problem of 3D shape deformation using AI has not yet been developed (as of the time of writing this thesis). Many existing methods are frequently confined to 2D spaces, with fewer efforts made to extend these solutions to 3D problems. In real-world applications, interacting with 3D data is preferable as it more closely resembles actual conditions, enhancing the realism and accuracy of simulations. Expanding AI applications to 3D FEM simulations can significantly improve their applicability and fidelity in real-world scenarios.</p>
<p>On the other hand, various AI techniques for working with 3D data have been actively pursued in domains such as computer graphics, 3D reconstruction, 3D object classification etc. It is important to note that this is a multidisciplinary issue, requiring collaboration across different scientific fields. Interaction among specialists from various domains is crucial to finding a common ground and proposing more effective solutions. This interdisciplinary cooperation is essential for aligning different areas of expertise to develop more robust and effective methods for 3D shape deformation.</p>
<p>Therefore, in this section, we will explore computer science methods that are similar to our problem and work with 3D data using AI tools. The work presented here is relatively specialized, and as of now, no existing AI method has been found that directly addresses our specific problem. However, we can expect to see an increase in intelligent models tackling this issue in the near future. The continued development of AI in this area holds great promise for improving the accuracy and efficiency of simulations involving 3D shape deformation.</p>
</section>
</section>
<section id="introduction-to-deep-neural-networks" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="introduction-to-deep-neural-networks"><span class="header-section-number">2.3</span> Introduction to Deep Neural Networks</h2>
<p>Deep Neural Networks (DNN) represent a significant evolution in the field of artificial intelligence, particularly in machine learning and pattern recognition. Building on the foundation of traditional neural networks, DeepNNs consist of multiple layers of interconnected neurons, allowing them to model complex patterns and relationships in data more effectively than shallow networks. The advent of DeepNNs has revolutionized numerous fields, including computer vision, natural language processing, and reinforcement learning, marking a pivotal shift in the capabilities of AI systems.</p>
<p>The concept of neural networks dates back to the mid-20th century, with the introduction of the perceptron by Frank Rosenblatt in 1958. However, the journey towards DeepNNs gained momentum only in the 1980s, with the development of the backpropagation algorithm, a critical breakthrough enabling the training of multi-layered networks. Backpropagation, introduced by Rumelhart, Hinton, and Williams in 1986, made it feasible to adjust the weights of neural networks through gradient descent, efficiently minimizing the error between predicted and actual outcomes. This algorithm remains at the heart of training deep networks, enabling them to learn complex functions from data.</p>
<p>Despite these early advances, DeepNNs struggled to gain traction due to computational limitations and the challenge of vanishing gradients, a problem where gradients used to update the weights become increasingly small in deeper layers, hindering effective learning. This issue was addressed by the introduction of more advanced activation functions like ReLU (Rectified Linear Unit), which helped maintain more consistent gradients, and by innovations such as batch normalization and more sophisticated weight initialization techniques.</p>
<p>Key Concepts and Advancements One of the core principles behind the success of DeepNNs is gradient descent, an optimization algorithm used to minimize the loss function. The loss function <span class="math inline">L(\theta)</span> quantifies the error in the network’s predictions, where <span class="math inline">\theta</span> represents the network’s parameters (weights and biases). Gradient descent iteratively adjusts <span class="math inline">\theta</span> in the direction opposite to the gradient of the loss function, formally expressed as:</p>
<p><span class="math display">\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)</span></p>
<p>where <span class="math inline">η</span> is the learning rate, and <span class="math inline">\nabla_\theta L(\theta_t)</span> is the gradient of the loss function with respect to the parameters. However, the success of gradient descent in deep networks relies heavily on effective weight initialization, appropriate activation functions, and strategies to mitigate overfitting and vanishing gradients. The introduction of ReLU activation functions, expressed as <span class="math inline">f(x)=max(0,x)</span>, was instrumental in overcoming the vanishing gradient problem by ensuring that gradients remain significant in deeper layers, thus allowing for better learning and performance.</p>
<p>Additionally, innovations like convolutional layers in CNNs, recurrent layers in RNNs, and attention mechanisms in transformers have expanded the applicability and power of DeepNNs. CNNs have revolutionized image processing, RNNs have enabled effective modeling of sequential data, and transformers have set new benchmarks in natural language processing.</p>
<p>A crucial aspect of neural networks is the dataset. Typically, the model is trained on a large amount of training data, validated on a separate validation set, and then tested on unseen test data to evaluate its performance. The selection and preprocessing of data are of particular importance in this process. Neural networks often operate best when the data is scaled within a specific range, usually between 0 and 1. Therefore, considerable effort is made to ensure that the data is normalized to fall within this optimal range, which significantly contributes to the effectiveness of the model.</p>
<p>The success of these architectures has been further amplified by large-scale datasets and the availability of massive computational resources, leading to groundbreaking achievements such as Google’s AlphaGo, OpenAI’s GPT models, and various state-of-the-art systems in image and speech recognition.</p>
</section>
<section id="autoencoders" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="autoencoders"><span class="header-section-number">2.4</span> Autoencoders</h2>
<p>An autoencoder is a type of neural network designed to learn a compressed, efficient representation of input data. It consists of two main components: an encoder and a decoder. The encoder compresses the input data into a lower-dimensional representation, often referred to as the latent space or bottleneck. The decoder then attempts to reconstruct the original input from this compressed representation. The goal is to minimize the difference between the input and the reconstructed output.</p>
<p>Mathematically, the process can be represented as: <span class="math display">
\mathbf{z} = f_\theta(\mathbf{x})
</span></p>
<p><span class="math display">
\hat{\mathbf{x}} = g_\phi(\mathbf{z})
</span></p>
<p>Here, <span class="math inline">\mathbf{z}</span> is the latent representation of the input <span class="math inline">\mathbf{x}</span>, and <span class="math inline">\hat{\mathbf{x}}</span> is the reconstructed output. The objective of the autoencoder is to minimize the reconstruction loss, commonly measured as the mean squared error (MSE):</p>
<p><span class="math display"> L(\mathbf{x}, \hat{\mathbf{x}}) = \| \mathbf{x} - \hat{\mathbf{x}} \|^2_2 </span></p>
<p>Autoencoders are particularly useful in areas such as dimensionality reduction and reconstruction, where they compress high-dimensional data into a lower-dimensional latent space and then accurately reconstruct the original data. They also excel in denoising by removing noise from corrupted data, and in anomaly detection by identifying outliers based on reconstruction errors. Additionally, autoencoders are valuable in generative modeling, contributing to the creation of new, similar data samples, which is especially beneficial in image synthesis and data augmentation.</p>
</section>
<section id="d-deep-learning" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="d-deep-learning"><span class="header-section-number">2.5</span> 3D Deep Learning</h2>
<p>Three-dimensional (3D) data processing using deep learning has become increasingly important in fields such as computer vision, robotics, and virtual reality. The way 3D data is represented plays a critical role in the design of models, the training process, and the final output. Depending on how the data is structured and represented, 3D deep learning can be categorized into several approaches. The most common representations include point clouds, meshes, and multi-view images, which are considered explicit forms of 3D data. Each of these representations has its unique challenges and benefits, influencing the choice of network architecture and the methods used for training.</p>
<p>In addition to explicit representations, implicit representations have gained significant attention in recent years. Unlike explicit forms, where the 3D data is directly stored and processed, implicit representations model the 3D structure in a more abstract way, such as through occupancy fields or signed distance functions. These newer approaches allow for more flexible and continuous representation of 3D shapes, often leading to better generalization and smoother reconstructions. Due to their efficiency and ability to handle complex geometries, implicit representations are becoming increasingly popular in state-of-the-art 3D deep learning applications.</p>
<section id="nn-and-explicit-representations-point-clouds" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="nn-and-explicit-representations-point-clouds"><span class="header-section-number">2.5.1</span> NN and Explicit representations : Point Clouds</h3>
<section id="pointnet-pointnet" class="level4" data-number="2.5.1.1">
<h4 data-number="2.5.1.1" class="anchored" data-anchor-id="pointnet-pointnet"><span class="header-section-number">2.5.1.1</span> PointNet, PointNet++</h4>
</section>
</section>
<section id="nn-and-explicit-representations-mesh" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="nn-and-explicit-representations-mesh"><span class="header-section-number">2.5.2</span> NN and Explicit representations : Mesh</h3>
<p>Meshes are one of the most widely used explicit representations in 3D deep learning, particularly in fields like computer graphics, medical imaging, and 3D modeling. A mesh is essentially a specific form of graph, composed of vertices (points in 3D space) and edges (connections between vertices), which together form faces that define the surface of a 3D object. This representation is highly expressive, capturing intricate details of an object’s surface geometry and topology. Unlike more regular data structures such as images or point clouds, meshes have a non-Euclidean structure, presenting unique challenges for neural networks that need to process this type of data. To address these challenges, specialized neural network architectures like graph neural networks (GNNs) or convolutional neural networks adapted for non-Euclidean spaces (e.g., mesh convolutional networks) are often used. These networks are designed to leverage the connectivity information inherent in meshes, allowing them to learn and extract meaningful features directly from the mesh’s topology. Therefore, these GNNs are highly effective for data with a graph-like structure, where connections between verices are crucial, such as social network or molecular graphs, where understanding complex relational patterns is crucial. However, when the mesh geometry and the positions of the vertices themselves become important, these networks face significant challenges. Examples of networks that have addressed this issue in geometric deep learning for classification, under specific constraints, include MeshCNN, SplineCNN, and CoMA <span class="citation" data-cites="ranjan2018generating">Ranjan et al. (<a href="References.html#ref-ranjan2018generating" role="doc-biblioref">2018</a>)</span>.</p>
<section id="convoutional-mesh-autoencoder-coma" class="level4" data-number="2.5.2.1">
<h4 data-number="2.5.2.1" class="anchored" data-anchor-id="convoutional-mesh-autoencoder-coma"><span class="header-section-number">2.5.2.1</span> Convoutional Mesh AutoEncoder (CoMA)</h4>
<p>The Convolutional Mesh AutoEncoder (CoMA), introduced in 2017, is a neural network architecture specifically designed to process 3D mesh data.</p>
<p>Unlike traditional autoencoders, which are inherently designed for structured, grid-like data such as images, CoMA is specifically engineered to tackle the complexities of irregular 3D mesh structures. Traditional convolutional neural networks (CNNs) excel in processing data arranged in regular grids, where each pixel or voxel is neatly aligned with its neighbors. However, 3D meshes are fundamentally different—they consist of vertices connected by edges forming irregular polygons, typically triangles, that define the surface of a shape. These irregularities present significant challenges for standard CNNs, which rely on the spatial consistency of grid data to apply convolutional operations effectively.</p>
<p>CoMA overcomes these challenges by extending convolutional operations from regular grids to graph structures, where the mesh vertices and their connections (edges) form a graph. This adaptation is made possible through spectral graph convolutional layers, which operate in the frequency domain, allowing the network to process the mesh’s geometry in a way that respects its inherent irregularity. These layers perform convolutions not in the traditional spatial sense, but by filtering the mesh’s geometric features across its graph-based structure. This allows CoMA to capture both local and global geometric information, which is crucial for accurately representing complex 3D shapes.</p>
<p>The architecture of CoMA is built around a symmetric encoder-decoder design, where both the encoder and decoder consist of four layers. The encoder’s role is to compress the high-dimensional mesh data into a lower-dimensional latent space. This compression is achieved through the spectral graph convolutional layers, which progressively reduce the resolution of the mesh while preserving its most significant geometric features. The latent space effectively captures the high-level, abstract representation of the 3D shape, distilling its most important characteristics into a compact form.</p>
<p>Once the mesh data is compressed into this latent space, the decoder takes over, reconstructing the original high-resolution mesh from the compact representation. The decoder mirrors the encoder’s structure, using upsampling operations and inverse convolutions to progressively restore the mesh’s resolution. This reconstruction process allows CoMA to generate a detailed and accurate representation of the original 3D shape, ensuring that essential geometric properties are preserved throughout the process. The combination of these advanced techniques enables CoMA to learn and manipulate complex 3D shapes efficiently, making it a powerful tool for tasks like shape reconstruction, deformation transfer, and facial expression synthesis.</p>
<p>A key innovation of CoMA is its ability to handle the irregular topology of meshes through spectral graph convolutions, which operate in the frequency domain. By leveraging Chebyshev polynomials and fast localized convolutions, the model efficiently processes mesh data. However, it is important to note that all mesh samples in the dataset must share the same topology, and a model trained on one dataset is not easily extendable to others. Additionally, the input mesh must exhibit properties such as regularity, uniform connectivity, and also a consistent hierarchical structure to support both downsampling and upsampling operations. Without these properties, operations like pooling and unpooling can become problematic, potentially necessitating remeshing to create a dataset suitable for CoMA. Furthermore, the network’s first - and last - layers must have a size of 3 times the number of vertices to represent XYZ positions, which can result in a large model when dealing with high-resolution meshes. Therefore, while CoMA is powerful for tasks like 3D shape reconstruction, facial expression synthesis, and deformation transfer, it does have specific requirements and limitations regarding the structure and properties of the input meshes.</p>
</section>
<section id="splinecnn" class="level4" data-number="2.5.2.2">
<h4 data-number="2.5.2.2" class="anchored" data-anchor-id="splinecnn"><span class="header-section-number">2.5.2.2</span> SplineCNN</h4>
<p><span class="citation" data-cites="fey2018splinecnn">Fey et al. (<a href="References.html#ref-fey2018splinecnn" role="doc-biblioref">2018</a>)</span> SplineCNN is a type of convolutional neural network designed to operate on non-Euclidean domains, such as graphs and meshes, where data is irregular and connectivity information is crucial. Unlike traditional CNNs that use fixed rectangular kernels, SplineCNN employs learnable B-spline kernels that can adapt to the underlying structure of the data, allowing the network to perform convolutions directly on the graph or mesh. This flexibility enables SplineCNN to effectively capture both the local geometric structure and the connectivity patterns, making it well-suited for tasks like 3D shape analysis and graph-based classification.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/Chp1/splineCNN.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<p>SplineCNN has demonstrated its effectiveness by improving state-of-the-art results in several benchmark tasks, including image graph classification, graph node classification, and shape correspondence on meshes. For the task of shape correspondence, SplineCNN was validated on a collection of 3D meshes, solving the challenge of matching each node of a given shape to the corresponding node of a reference shape. However, the dataset used in this experiment had a significant limitation: all meshes shared the same topology, requiring that the mesh sizes and node orders remain consistent across the entire dataset, which is a considerable constraint.</p>
</section>
<section id="meshcnn" class="level4" data-number="2.5.2.3">
<h4 data-number="2.5.2.3" class="anchored" data-anchor-id="meshcnn"><span class="header-section-number">2.5.2.3</span> MeshCNN</h4>
<p><span class="citation" data-cites="hanocka2019meshcnn">Hanocka et al. (<a href="References.html#ref-hanocka2019meshcnn" role="doc-biblioref">2019</a>)</span></p>
<p>MeshCNN is a specialized neural network architecture designed to process 3D mesh data by adapting convolutional operations to the irregular, non-Euclidean structure of meshes. Unlike traditional CNNs that operate on grid-like data, MeshCNN applies convolutions directly on the edges of a mesh, enabling the network to capture the geometric and topological features inherent in 3D shapes. The architecture employs edge-based convolutions and pooling operations to reduce the mesh’s complexity while preserving its essential properties, making it particularly effective for tasks such as 3D shape classification and segmentation. This approach is well-suited for scenarios where understanding the detailed geometry and topology of 3D objects is crucial, such as distinguishing between different types of 3D models or segmenting parts of a 3D object.</p>
<p>However, MeshCNN also introduces certain complexities and limitations. The input dimensionality is defined by the number of features per edge multiplied by the total number of edges, which can lead to increased computational demands, particularly for large meshes. Additionally, the architecture requires a consistent and well-defined mesh structure, often necessitating preprocessing steps like remeshing or simplification to ensure compatibility. This dependence on specific mesh topologies can limit the network’s generalization to different types of meshes, and the computational load may challenge scalability in scenarios with varying mesh structures or limited resources.</p>
</section>
<section id="meshnet" class="level4" data-number="2.5.2.4">
<h4 data-number="2.5.2.4" class="anchored" data-anchor-id="meshnet"><span class="header-section-number">2.5.2.4</span> MeshNet</h4>
<p><span class="citation" data-cites="feng2019meshnet">Feng et al. (<a href="References.html#ref-feng2019meshnet" role="doc-biblioref">2019</a>)</span></p>
</section>
</section>
</section>
<section id="nn-and-imlicit-representations" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="nn-and-imlicit-representations"><span class="header-section-number">2.6</span> NN and Imlicit representations</h2>
<section id="deepsdf" class="level4" data-number="2.6.0.1">
<h4 data-number="2.6.0.1" class="anchored" data-anchor-id="deepsdf"><span class="header-section-number">2.6.0.1</span> DeepSDF</h4>
<p><span class="citation" data-cites="park2019deepsdf">Park et al. (<a href="References.html#ref-park2019deepsdf" role="doc-biblioref">2019</a>)</span></p>
<p>The most direct application of this approach is to train a single deep network for a given target shape as depicted in Fig X Given a target shape, we prepare a set of pairs ( X ) composed of 3D point samples and their SDF values:</p>
<p><span class="math display">
X := \{(x, s) : \text{SDF}(x) = s\}
</span></p>
<p>We train the parameters <span class="math inline">\theta</span> of a multi-layer fully-connected neural network <span class="math inline">f\_\theta</span> on the training set $ S $ to make <span class="math inline">f\_\theta</span> a good approximator of the given SDF in the target domain <span class="math inline">\Omega</span> :</p>
<p><span class="math display">
f_\theta(x) \approx \text{SDF}(x), \forall x \in \Omega
</span></p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-feng2019meshnet" class="csl-entry" role="listitem">
Feng, Y., Feng, Y., You, H., Zhao, X., and Gao, Y. (2019). Meshnet: Mesh neural network for 3d shape representation. in <em>Proceedings of the AAAI conference on artificial intelligence</em>, 8279–8286.
</div>
<div id="ref-fey2018splinecnn" class="csl-entry" role="listitem">
Fey, M., Lenssen, J. E., Weichert, F., and Müller, H. (2018). Splinecnn: Fast geometric deep learning with continuous b-spline kernels. in <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 869–877.
</div>
<div id="ref-hanocka2019meshcnn" class="csl-entry" role="listitem">
Hanocka, R., Hertz, A., Fish, N., Giryes, R., Fleishman, S., and Cohen-Or, D. (2019). Meshcnn: A network with an edge. <em>ACM Transactions on Graphics (ToG)</em> 38, 1–12.
</div>
<div id="ref-park2019deepsdf" class="csl-entry" role="listitem">
Park, J. J., Florence, P., Straub, J., Newcombe, R., and Lovegrove, S. (2019). Deepsdf: Learning continuous signed distance functions for shape representation. in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 165–174.
</div>
<div id="ref-raissi2019physics" class="csl-entry" role="listitem">
Raissi, M., Perdikaris, P., and Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. <em>Journal of Computational physics</em> 378, 686–707.
</div>
<div id="ref-ranjan2018generating" class="csl-entry" role="listitem">
Ranjan, A., Bolkart, T., Sanyal, S., and Black, M. J. (2018). Generating 3D faces using convolutional mesh autoencoders. in <em>Proceedings of the european conference on computer vision (ECCV)</em>, 704–720.
</div>
<div id="ref-zhang2024state" class="csl-entry" role="listitem">
Zhang, M., Abidin, A. R. Z., and Tan, C. S. (2024). State-of-the-art review on meshless methods in the application of crack problems. <em>Theoretical and Applied Fracture Mechanics</em>, 104348.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./1-Introduction.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./3-Methods.html" class="pagination-link" aria-label="Methods">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>